#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨verlå®˜æ–¹åº“å‡½æ•°å°†JSONLè½¬æ¢ä¸ºParquetæ ¼å¼
å¤ç”¨verlç°æœ‰çš„æ•°æ®å¤„ç†é€»è¾‘
ä½¿ç”¨æ–¹æ³•ï¼šlogger
python scripts/jsonl_to_parquet_converter.py \
    --input "/home/jovyan2/query_rl/data/sales_rag/train_val.jsonl" \
    --output "/home/jovyan2/query_rl/data/sales_rag/val.parquet" \
    --validate
"""

import pandas as pd
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def read_jsonl_file(file_path: str) -> List[Dict[str, Any]]:
    """
    è¯»å–JSONLæ–‡ä»¶
    
    Args:
        file_path: JSONLæ–‡ä»¶è·¯å¾„
        
    Returns:
        List[Dict]: æ•°æ®åˆ—è¡¨
    """
    data = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:  # è·³è¿‡ç©ºè¡Œ
                    try:
                        item = json.loads(line)
                        data.append(item)
                    except json.JSONDecodeError as e:
                        logger.warning(f"ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                        continue
        
        logger.info(f"æˆåŠŸè¯»å– {len(data)} æ¡è®°å½•ä» {file_path}")
        return data
        
    except Exception as e:
        logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        raise


def convert_jsonl_to_parquet(jsonl_path: str, parquet_path: str) -> None:
    """
    ä½¿ç”¨pandasç›´æ¥è½¬æ¢JSONLåˆ°Parquetæ ¼å¼
    è¿™æ˜¯verlå®˜æ–¹æ¨èçš„æ ‡å‡†æ–¹å¼
    
    Args:
        jsonl_path: è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„
        parquet_path: è¾“å‡ºParquetæ–‡ä»¶è·¯å¾„
    """
    try:
        # è¯»å–JSONLæ•°æ®
        data = read_jsonl_file(jsonl_path)
        
        if not data:
            logger.warning(f"æ²¡æœ‰æ•°æ®å¯ä»¥è½¬æ¢: {jsonl_path}")
            return
        
        # è½¬æ¢ä¸ºDataFrame - è¿™æ˜¯verlæ ‡å‡†æ ¼å¼
        df = pd.DataFrame(data)
        
        # å¤„ç†reward_modelå­—æ®µï¼Œç¡®ä¿åŒ…å«ground_truth
        if 'reward_model' in df.columns:
            fixed_count = 0
            
            def fix_reward_model(row):
                """ä¿®å¤reward_modelå­—æ®µï¼Œæ·»åŠ ground_truth"""
                reward_model = row.get('reward_model', {})
                parsed_data = row.get('parsed_data', {})
                
                # å¦‚æœreward_modelä¸æ˜¯å­—å…¸ï¼Œåˆå§‹åŒ–ä¸ºå­—å…¸
                if not isinstance(reward_model, dict):
                    reward_model = {}
                
                # å¦‚æœç¼ºå°‘ground_truthï¼Œä»parsed_dataä¸­æå–
                if 'ground_truth' not in reward_model or reward_model.get('ground_truth') == {}:
                    # è§£æparsed_dataï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸ï¼‰
                    if isinstance(parsed_data, str):
                        try:
                            parsed_data = json.loads(parsed_data)
                        except:
                            parsed_data = {}
                    
                    # æ„å»ºground_truth
                    ground_truth = {
                        'context': parsed_data.get('context', ''),
                        'user_profile': parsed_data.get('user_profile', ''),
                        'history_summary': parsed_data.get('history_summary', ''),
                        'original_query': parsed_data.get('current_query', ''),
                    }
                    
                    reward_model['ground_truth'] = ground_truth
                    reward_model['style'] = reward_model.get('style', 'rule')
                    
                    nonlocal fixed_count
                    fixed_count += 1
                
                return reward_model
            
            df['reward_model'] = df.apply(fix_reward_model, axis=1)
            
            if fixed_count > 0:
                logger.info(f"âœ“ ä¸º {fixed_count} æ¡è®°å½•æ·»åŠ äº† ground_truth å­—æ®µ")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path = Path(parquet_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä¸ºParquetæ ¼å¼ - ä½¿ç”¨verlå…¼å®¹çš„è®¾ç½®
        df.to_parquet(
            parquet_path,
            engine='pyarrow',
            compression='snappy',
            index=False  # verlä¸éœ€è¦ç´¢å¼•
        )
        
        logger.info(f"æˆåŠŸè½¬æ¢ {len(data)} æ¡è®°å½•åˆ° {parquet_path}")
        logger.info(f"Parquetæ–‡ä»¶ä¿¡æ¯: è¡Œæ•°={len(df)}, åˆ—æ•°={len(df.columns)}")
        
        # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
        if len(df) > 0:
            logger.info("æ•°æ®é¢„è§ˆ:")
            logger.info(f"åˆ—å: {list(df.columns)}")
            if 'prompt' in df.columns:
                logger.info(f"ç¬¬ä¸€ä¸ªpromptç¤ºä¾‹: {str(df['prompt'].iloc[0])[:100]}...")
        
    except Exception as e:
        logger.error(f"è½¬æ¢å¤±è´¥: {e}")
        raise


def validate_parquet_file(parquet_path: str) -> bool:
    """
    éªŒè¯Parquetæ–‡ä»¶æ˜¯å¦ç¬¦åˆverlè¦æ±‚
    
    Args:
        parquet_path: Parquetæ–‡ä»¶è·¯å¾„
        
    Returns:
        bool: éªŒè¯ç»“æœ
    """
    try:
        # è¯»å–Parquetæ–‡ä»¶
        df = pd.read_parquet(parquet_path)
        
        logger.info(f"éªŒè¯Parquetæ–‡ä»¶: {parquet_path}")
        logger.info(f"æ•°æ®å½¢çŠ¶: {df.shape}")
        logger.info(f"åˆ—å: {list(df.columns)}")
        
        # æ£€æŸ¥verlå¿…éœ€çš„å­—æ®µ
        required_fields = ['prompt', 'data_source']
        missing_fields = [field for field in required_fields if field not in df.columns]
        
        if missing_fields:
            logger.warning(f"ç¼ºå°‘verlå¿…éœ€å­—æ®µ: {missing_fields}")
        else:
            logger.info("âœ“ æ‰€æœ‰verlå¿…éœ€å­—æ®µéƒ½å­˜åœ¨")
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if 'prompt' in df.columns:
            empty_prompts = df['prompt'].isna().sum()
            if empty_prompts > 0:
                logger.warning(f"å‘ç° {empty_prompts} ä¸ªç©ºprompt")
            else:
                logger.info("âœ“ æ‰€æœ‰promptå­—æ®µéƒ½æœ‰æ•ˆ")
        
        # æ£€æŸ¥reward_modelå­—æ®µï¼ˆGRPOéœ€è¦ï¼‰
        if 'reward_model' in df.columns:
            logger.info("âœ“ æ£€æµ‹åˆ°reward_modelå­—æ®µï¼Œç¬¦åˆGRPOè¦æ±‚")
            
            # æ£€æŸ¥ground_truthå­—æ®µ
            first_reward_model = df['reward_model'].iloc[0]
            if isinstance(first_reward_model, dict):
                if 'ground_truth' in first_reward_model:
                    logger.info("âœ“ reward_modelåŒ…å«ground_truthå­—æ®µ")
                    ground_truth = first_reward_model['ground_truth']
                    if isinstance(ground_truth, dict):
                        logger.info(f"  - ground_truthå­—æ®µ: {list(ground_truth.keys())}")
                        required_keys = ['context', 'user_profile', 'history_summary']
                        missing_keys = [k for k in required_keys if k not in ground_truth]
                        if not missing_keys:
                            logger.info("âœ“ ground_truthåŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µ")
                        else:
                            logger.warning(f"âš  ground_truthç¼ºå°‘æ¨èå­—æ®µ: {missing_keys}")
                    else:
                        logger.warning(f"âš  ground_truthä¸æ˜¯å­—å…¸ç±»å‹: {type(ground_truth)}")
                else:
                    logger.error("âœ— reward_modelç¼ºå°‘ground_truthå­—æ®µ")
                    return False
            else:
                logger.warning(f"âš  reward_modelä¸æ˜¯å­—å…¸ç±»å‹: {type(first_reward_model)}")
        
        return True
        
    except Exception as e:
        logger.error(f"éªŒè¯å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='JSONLè½¬Parquetæ ¼å¼è½¬æ¢å™¨ - å…¼å®¹verlæ¡†æ¶',
        epilog='ç¤ºä¾‹: python jsonl_to_parquet_converter.py --input train.jsonl --output train.parquet'
    )
    parser.add_argument('--input', '-i', type=str, required=True, 
                       help='è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', type=str, required=True, 
                       help='è¾“å‡ºParquetæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--validate', '-v', action='store_true', 
                       help='è½¬æ¢åéªŒè¯æ–‡ä»¶æ˜¯å¦ç¬¦åˆverlè¦æ±‚')
    
    args = parser.parse_args()
    
    try:
        logger.info("å¼€å§‹JSONLåˆ°Parquetæ ¼å¼è½¬æ¢...")
        logger.info(f"è¾“å…¥æ–‡ä»¶: {args.input}")
        logger.info(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
        
        # æ‰§è¡Œè½¬æ¢
        convert_jsonl_to_parquet(args.input, args.output)
        
        # éªŒè¯ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if args.validate:
            logger.info("\nå¼€å§‹éªŒè¯Parquetæ–‡ä»¶...")
            is_valid = validate_parquet_file(args.output)
            if is_valid:
                logger.info("âœ“ Parquetæ–‡ä»¶éªŒè¯é€šè¿‡ï¼Œç¬¦åˆverlè¦æ±‚")
            else:
                logger.warning("âœ— Parquetæ–‡ä»¶éªŒè¯å¤±è´¥")
                return 1
        
        logger.info("\nè½¬æ¢å®Œæˆï¼ğŸ‰")
        logger.info("ç”Ÿæˆçš„Parquetæ–‡ä»¶å¯ä»¥ç›´æ¥ç”¨äºverlè®­ç»ƒæ¡†æ¶")
        
    except Exception as e:
        logger.error(f"é”™è¯¯: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())