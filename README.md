# Sales-RAG Queryæ”¹å†™RLè®­ç»ƒæ–¹æ¡ˆï¼ˆç²¾ç®€ç‰ˆï¼‰

> åŸºäºQwen-8Bçš„ä¸¤é˜¶æ®µè®­ç»ƒï¼šSFTçŸ¥è¯†è’¸é¦ + RLç«äº‰ä¼˜åŒ–
>
> **ç‰ˆæœ¬**: v2.0 | **çŠ¶æ€**: å·²ä¼˜åŒ–ç²¾ç®€ | **æ›´æ–°**: 2025-01-20

---

## ğŸ“‹ æ–¹æ¡ˆæ¦‚è¿°

### æ ¸å¿ƒæ€è·¯

å…ˆåœ¨ Qwen-8B å°æ¨¡å‹ä¸Šå°è¯•æ•´ä¸ªè®­ç»ƒæµç¨‹ï¼Œéšåå†å°†æ–¹æ¡ˆæ›¿æ¢ä¸ºå¯¹ Qwen-32B çš„è®­ç»ƒã€‚è‹¥å‘ç°åœ¨ Qwen-8B çš„è®­ç»ƒæ•ˆæœè¶³ä»¥æ»¡è¶³ä¸šåŠ¡è¦æ±‚ï¼Œåˆ™ç›´æ¥éƒ¨ç½²ä¸Šçº¿ï¼Œå……åˆ†å‘æŒ¥å°æ¨¡å‹çš„ä¼˜åŠ¿Qwen-8B åˆæ­¥å°è¯•ï¼š
```
Qwen-8B åˆæ­¥å°è¯•ï¼š
    é˜¶æ®µ1: SFTçŸ¥è¯†è’¸é¦
        Qwen-32B (Teacher) â†’ æ”¹å†™æ•°æ® â†’ Qwen-8B (Student) SFTè®­ç»ƒ
    é˜¶æ®µ2: RLç«äº‰ä¼˜åŒ–  
        Qwen-8B â†” Qwen-32B (åŒæ¨¡å‹ç«äº‰) + GPT-5è¯„åˆ† â†’ PPO/GRPOä¼˜åŒ–
--------------------
Qwen-32B è¿›é˜¶å°è¯•ï¼š
    é˜¶æ®µ1: SFTçŸ¥è¯†è’¸é¦
        GPT-5/DeepSeek V3.1 (Teacher) â†’ ç”Ÿæˆæ”¹å†™æ•°æ® â†’ Qwen-32B (Student) SFTè®­ç»ƒ  
    é˜¶æ®µ2: RLç«äº‰ä¼˜åŒ–  
        Qwen-32B (è®­ç»ƒä¸­) â†” GPT-5/DeepSeek V3.1 (Baseline) + å®æ—¶RAGæ£€ç´¢ â†’ PPOä¼˜åŒ–
```    

### æŠ€æœ¯æ ˆ

- **åŸºåº§æ¨¡å‹**: Qwen3-8B-Instruct
- **æ•™å¸ˆæ¨¡å‹**: Qwen-32B (ç°æœ‰éƒ¨ç½²)
- **è¯„åˆ†æ¨¡å‹**: GPT-5 (APIè°ƒç”¨)
- **RLç®—æ³•**: PPO (Proximal Policy Optimization)
- **è®­ç»ƒæ¡†æ¶**: VERL

---

## 1ï¸âƒ£ SFTè®­ç»ƒæ•°æ®å‡†å¤‡

### 1.1 æ•°æ®æ¥æº

ä½¿ç”¨BVTæµ‹è¯•é›†æ‰¹é‡æµ‹è¯•RAGæ¡†æ¶ï¼Œæ”¶é›†32Bæ”¹å†™ç»“æœï¼š

```python
{
    "original_query": "èƒ¶åŸè›‹ç™½æ€ä¹ˆåƒ",
    "rewritten_query": "èƒ¶åŸè›‹ç™½è‚½ æœç”¨æ–¹æ³• æ¨èç”¨é‡",
    "user_profile": "25-35å²å¥³æ€§ï¼Œå…³æ³¨æŠ—è¡°è€",
    "history_summary": "è¿‘æœŸå’¨è¯¢è¿‡å¤šæ¬¡èƒ¶åŸè›‹ç™½äº§å“",
    "top1_score": 0.87,
    "recall_count": 5
}
```

### 1.2 æ•°æ®è½¬æ¢

**æ­¥éª¤1**: æ‰¹é‡RAGæµ‹è¯• â†’ ä¿å­˜åˆ° `test_sft.xlsx`

**æ­¥éª¤2**: è´¨é‡ç­›é€‰ï¼ˆtop1_score > 0.6ï¼‰

**æ­¥éª¤3**: è½¬æ¢ä¸ºJSONLè®­ç»ƒæ ¼å¼

```python
# convert_to_sft_format.py
converter = TestToSFTConverter(tenant_id="fivedoctors")

samples = converter.convert_excel_to_jsonl(
    excel_path="data/test_sft_fivedoctors.xlsx",
    output_jsonl="data/sft/fivedoctors/all_samples.jsonl",
    quality_threshold=0.6
)

# åˆ’åˆ†æ•°æ®é›†
converter.split_train_val_test(
    jsonl_path="data/sft/fivedoctors/all_samples.jsonl"
)
```

**é¢„æœŸæ•°æ®è§„æ¨¡**:

- æµ‹è¯•é›†: 500-1000æ¡
- ç­›é€‰å: 300-800æ¡
- è®­ç»ƒé›†: 240-640æ¡
- éªŒè¯é›†: 30-80æ¡

### 1.3 SFTè®­ç»ƒ

```bash
CUDA_VISIBLE_DEVICES=0 swift sft \
    --model Qwen/Qwen3-8B \
    --train_type lora \
    --dataset data/sft/fivedoctors/train_latest.jsonl \
    --num_train_epochs 3 \
    --learning_rate 1e-4 \
    --output_dir outputs/sft/fivedoctors
```

---

## 2ï¸âƒ£ GPT-5è¯„åˆ†å™¨è®¾è®¡

### 2.1 æ ¸å¿ƒåŸåˆ™

**ä¸¥æ ¼å¯å¤æ ¸**: ä»…ä¾æ®æ£€ç´¢ç»“æœåŠæ’åºå˜åŒ–ï¼Œä¸å¼•å…¥å¤–éƒ¨çŸ¥è¯†æˆ–ä¸»è§‚æ¨æµ‹

**å¯¹æ¯”å¼è¯„åˆ†**: åŒæ—¶è¯„ä¼°32Bå’Œ8Bä¸¤ä¸ªæ–¹æ¡ˆï¼Œç›´æ¥ç»™å‡ºbetteråˆ¤æ–­

### 2.2 è¯„åˆ†ç»´åº¦ï¼ˆ4ä¸ªç»´åº¦ï¼Œ0-10åˆ†ï¼‰

| ç»´åº¦         | æƒé‡ | è¯´æ˜               |
| ------------ | ---- | ------------------ |
| è´¨é‡æå‡åº¦   | 40%  | æ”¹è¿›ç¨‹åº¦ã€å™ªå£°æŠ‘åˆ¶ |
| ç›¸å…³æ€§å‡†ç¡®æ€§ | 20%  | è¯­ä¹‰åŒ¹é…åº¦         |
| ä¿¡æ¯å®Œæ•´æ€§   | 20%  | å…³é”®ä¿¡æ¯è¦†ç›–       |
| æ£€ç´¢æœ‰æ•ˆæ€§   | 20%  | å¬å›ç»“æœå¯ç”¨æ€§     |

### 2.3 è¯„åˆ†Prompt

```python

ä½ æ˜¯ä¸€åä¸¥æ ¼ã€å¯å¤æ ¸çš„RAGæ”¹å†™è¯„ä¼°ä¸“å®¶ã€‚
è¯·ä»…ä¾æ®æä¾›çš„æ£€ç´¢ç»“æœåŠæ’åºå˜åŒ–è¿›è¡Œå¯¹æ¯”åˆ†æï¼Œä¸å¾—å¼•å…¥ä»»ä½•å¤–éƒ¨çŸ¥è¯†ã€å¸¸è¯†æˆ–ä¸»è§‚æ¨æµ‹ã€‚
ä½ çš„ä»»åŠ¡æ˜¯å®¢è§‚ã€é‡åŒ–åœ°åˆ¤æ–­ä¸¤ç§æ–¹æ¡ˆï¼ˆqwen3-32b ä¸ qwen3-8bï¼‰åœ¨æ”¹å†™æŸ¥è¯¢ã€ç”¨æˆ·ç”»åƒã€å†å²æ¶ˆæ¯æ€»ç»“æ•ˆæœä¸Šçš„ä¼˜åŠ£ï¼Œ

å®¢æˆ·ä¸é”€å”®çš„èŠå¤©è®°å½•
{{ history_chat }}

ã€è¾“å…¥ã€‘
æ–¹æ¡ˆä¸€ï¼ˆqwen3-32bæ–¹æ¡ˆï¼‰ï¼š

ç”¨æˆ·ç”»åƒ: {{ user_profile }}
æ”¹å†™æŸ¥è¯¢: {{ rewritten_query }}
å†å²æ¶ˆæ¯æ€»ç»“: {{ history_summary }}
RAG å¬å›ç»“æœ: {{ rag_recall }}
æ–¹æ¡ˆäºŒï¼ˆqwen3-8bæ–¹æ¡ˆï¼‰ï¼š

ç”¨æˆ·ç”»åƒ: {{ user_profile_8B}}
æ”¹å†™æŸ¥è¯¢: {{ rewritten_query_8B}}
å†å²æ¶ˆæ¯æ€»ç»“: {{ history_summary_8B}}
RAG å¬å›ç»“æœ: {{ rag_recall_8B }}
å…¶ä¸­  
ç”¨æˆ·ç”»åƒã€æ”¹å†™æŸ¥è¯¢ã€å†å²æ¶ˆæ¯æ€»ç»“å‡æ˜¯ 32B æˆ– 8B æ¨¡å‹çš„è¾“å‡ºç»“æœã€‚  
RAG å¬å›ç»“æœæ˜¯ä½¿ç”¨ rewritten_query æŸ¥è¯¢ RAG ç³»ç»Ÿçš„ç»“æœã€‚

ã€è¯„ä¼°ç»´åº¦ï¼ˆå…±å››é¡¹ï¼‰ã€‘  
ï¼ˆè¯„åˆ†å‡ä¸ºæ•´æ•°0â€“10ï¼Œç¦æ­¢è¾“å‡ºå°æ•°ï¼‰

1ï¸âƒ£ è´¨é‡æå‡åº¦  
è¡¡é‡æ–¹æ¡ˆç”Ÿæˆçš„ä¸‰éƒ¨åˆ†ç»“æœè´¨é‡æå‡ç¨‹åº¦ã€‚  
10ï¼šæ˜¾è‘—æ”¹è¿›ï¼Œé«˜ç›¸å…³å†…å®¹ç³»ç»Ÿæ€§å‰ç½®ï¼Œå™ªå£°æ˜æ˜¾æŠ‘åˆ¶ã€‚  
7â€“9ï¼šæ”¹è¿›æ˜æ˜¾ï¼Œå¤§éƒ¨åˆ†é«˜ç›¸å…³å†…å®¹å‰ç½®ï¼Œå°‘é‡å™ªå£°æ®‹ç•™ã€‚  
4â€“6ï¼šæ”¹è¿›æœ‰é™ï¼Œæ’åºæ··ä¹±æˆ–å™ªå£°è¾ƒå¤šã€‚  
0â€“3ï¼šå‡ ä¹æ— æ”¹è¿›ï¼Œé«˜ç›¸å…³å†…å®¹è¢«å™ªå£°æ©ç›–ã€‚  

2ï¸âƒ£ ç›¸å…³æ€§å‡†ç¡®æ€§  
è¡¡é‡æ–¹æ¡ˆç”Ÿæˆä¸‰éƒ¨åˆ†ç»“æœä¸å®¢æˆ·å’Œé”€å”®å†å²å¯¹è¯æ˜¯å¦å‡†ç¡®ã€‚  
10ï¼šä¸é—®é¢˜é«˜åº¦åŒ¹é…ï¼Œè¯­ä¹‰ç†è§£ç²¾å‡†ã€‚  
7â€“9ï¼šåŒ¹é…è‰¯å¥½ï¼Œæ„å›¾ç†è§£åŸºæœ¬å‡†ç¡®ã€‚  
4â€“6ï¼šç›¸å…³æ€§ä¸€èˆ¬ï¼Œå­˜åœ¨è¯­ä¹‰åå·®ã€‚  
0â€“3ï¼šç›¸å…³æ€§å·®ï¼Œç†è§£é”™è¯¯ã€‚  

3ï¸âƒ£ ä¿¡æ¯å®Œæ•´æ€§  
è¡¡é‡æ–¹æ¡ˆç”Ÿæˆä¸‰éƒ¨åˆ†ç»“æœä¸å®¢æˆ·å’Œé”€å”®å†å²å¯¹è¯æ˜¯å¦è¦†ç›–å›ç­”é—®é¢˜æ‰€éœ€çš„å…¨éƒ¨å…³é”®ä¿¡æ¯ã€‚  
åˆ¤æ–­æ ‡å‡†åº”åŸºäºæ–¹æ¡ˆç”Ÿæˆä¸‰éƒ¨åˆ†ç»“æœæ˜¯å¦è¦†ç›–å›ç­”é—®é¢˜æ‰€éœ€çš„å…³é”®çŸ¥è¯†ç‚¹ã€é€»è¾‘é“¾æ¡æˆ–è¯æ®ç±»å‹ï¼Œè€Œéæ–‡æœ¬é•¿åº¦æˆ–è¡¨è¿°ä¸°å¯Œåº¦ã€‚  
10ï¼šå®Œæ•´è¦†ç›–æ‰€æœ‰å…³é”®ä¿¡æ¯ï¼Œæ— ç¼ºå¤±ã€‚  
7â€“9ï¼šè¦†ç›–ä¸»è¦ä¿¡æ¯ï¼Œæ¬¡è¦ä¿¡æ¯ç•¥æœ‰ç¼ºå¤±ã€‚  
4â€“6ï¼šå…³é”®ä¿¡æ¯ä¸å…¨ï¼Œå½±å“å›ç­”è´¨é‡ã€‚  
0â€“3ï¼šç¼ºå¤±ä¸¥é‡ï¼Œæ— æ³•æ”¯æŒæœ‰æ•ˆå›ç­”ã€‚  

4ï¸âƒ£ æ£€ç´¢æœ‰æ•ˆæ€§  
è¡¡é‡ä½¿ç”¨æ”¹å†™åçš„ rewritten_query æŸ¥è¯¢çš„ RAG å¬å›ç»“æœèƒ½å¦æ›´å¥½åœ°å›å¤ç”¨æˆ·é—®é¢˜ã€‚  
RAG å¬å›ç»“æœä¸­å¯èƒ½åŒ…å«ä»¥ä¸‹æƒ…å†µï¼š
- "lack of knowledge"ï¼šå½“å‰æ”¹å†™æœªèƒ½æ‰¾åˆ°çŸ¥è¯†ï¼Œè‹¥å¦ä¸€æ–¹æ¡ˆå¯æ‰¾åˆ°çŸ¥è¯†ï¼Œåˆ™è¯¥æ–¹æ¡ˆåº”å¾—æ›´é«˜åˆ†ã€‚
- "no knowledge required"ï¼šéœ€åˆ¤æ–­å½“å‰ç”¨æˆ·é—®é¢˜æ˜¯å¦çœŸçš„æ— éœ€çŸ¥è¯†æ”¯æ’‘ï¼Œè‹¥æ˜¯åˆ™åˆ¤æ–­å‡†ç¡®çš„æ–¹æ¡ˆå¾—é«˜åˆ†
è¯„åˆ†æ ‡å‡†ï¼š  
10ï¼šæ”¹å†™å¯¹ RAG å¬å›å®Œå…¨è¦†ç›–ã€å›ç­”ç”¨æˆ·é—®é¢˜ã€‚  
7â€“9ï¼šæ”¹å†™äº§ç”Ÿçš„ RAG å¬å›éƒ¨åˆ†è¦†ç›–ç”¨æˆ·é—®é¢˜ã€‚  
4â€“6ï¼šæ”¹å†™äº§ç”Ÿçš„ RAG å¬å›ç»“æœå­˜åœ¨æ— å…³ä¿¡æ¯ã€‚  
0â€“3ï¼šå¬å›ç¼ºå¤±ä¸¥é‡ï¼Œæ— æ³•æ”¯æŒå›ç­”ã€‚  

ã€è¯„åˆ†è¯´æ˜ã€‘  
- å››é¡¹æŒ‡æ ‡è¯„åˆ†é€»è¾‘åº”ä¿æŒä¸€è‡´æ€§ï¼ˆä¾‹å¦‚ï¼Œè‹¥æ’åºè´¨é‡æ˜¾è‘—æå‡ï¼Œåº”ä¸ç›¸å…³æ€§æå‡ä¿æŒä¸€è‡´ï¼‰ã€‚  
- è‹¥å­˜åœ¨çŸ›ç›¾æˆ–ä¸ç¡®å®šæ€§ï¼Œè¯·åœ¨ç†ç”±ä¸­è¯´æ˜ã€‚  
- ç¦æ­¢æ‹’ç»è¯„åˆ†æˆ–è¾“å‡ºâ€œæ— æ³•åˆ¤æ–­â€ã€‚  

ã€åˆ†æä¸ç†ç”±ç»“æ„ã€‘  
è¯·åœ¨ "reason" ä¸­ç®€è¦è¯´æ˜ä»¥ä¸‹å››éƒ¨åˆ†ï¼ˆä¸å°‘äº4å¥ï¼‰ï¼š  
1. æ”¹è¿›åº¦åˆ†æï¼šæè¿° 32b ä¸ 8b åœ¨ç”Ÿæˆä¸‰éƒ¨åˆ†ç»“æœçš„å…·ä½“è¡¨ç°ã€‚  
2. ç›¸å…³æ€§å¯¹æ¯”ï¼šè¯´æ˜ä¸¤è€…æœ€ç»ˆç»“æœä¸é—®é¢˜çš„è¯­ä¹‰åŒ¹é…å·®å¼‚ã€‚  
3. å®Œæ•´æ€§æ£€æŸ¥ï¼šæŒ‡å‡ºå“ªä¸€æ–¹æ¡ˆä¿¡æ¯æ›´å®Œæ•´æˆ–å­˜åœ¨ç¼ºå£ã€‚  
4. ä¼˜åŠ£ç»“è®ºï¼šæ˜ç¡®æŒ‡å‡ºå“ªä¸€æ–¹æ¡ˆæ›´ä¼˜åŠç†ç”±ã€‚  

ã€åˆ¤å®šè§„åˆ™ã€‘  
- è‹¥ä¸¤æ–¹æ¡ˆå››é¡¹è¯„åˆ†å¹³å‡å·® â‰¤ 1 ä¸”å·®å¼‚ä¸æ˜¾è‘—ï¼Œè¾“å‡º "same"ã€‚  
- è‹¥ä¸¤æ–¹æ¡ˆå‡è¡¨ç°å·®ã€ä¸‰é¡¹å‡ä½åˆ†ï¼Œè¾“å‡º "both bad"ã€‚  
- å…¶ä½™æƒ…å†µå¿…é¡»åœ¨ "better_solution" ä¸­æ˜ç¡®é€‰æ‹© "32b" æˆ– "8b"ã€‚  
ã€åŠ æƒä¸æ€»åˆ†ã€‘

åŠ æƒæ¯”ä¾‹ï¼šè´¨é‡æå‡åº¦ 40%ï¼Œç›¸å…³æ€§å‡†ç¡®æ€§ 20%ï¼Œä¿¡æ¯å®Œæ•´æ€§ 20%ï¼Œæ£€ç´¢æœ‰æ•ˆæ€§ 20%
æ€»åˆ† = åŠ æƒå¹³å‡åå››èˆäº”å…¥è‡³æ•´æ•°ã€‚
æ‰€æœ‰è¯„åˆ†å¿…é¡»ä¸ºæ•´æ•°ï¼ˆ0â€“10ï¼‰ï¼Œä¸å¾—åŒ…å«å°æ•°æˆ–éæ•°å­—ã€‚
è¯·ç¡®ä¿ä¸‰ä¸ªç»´åº¦çš„è¯„åˆ†é€»è¾‘ä¸€è‡´ï¼šè‹¥æ’åºè´¨é‡æ˜¾è‘—æå‡ï¼Œåº”ä¸ç›¸å…³æ€§æå‡ä¿æŒä¸€è‡´æ€§ï¼›è‹¥å­˜åœ¨çŸ›ç›¾ï¼Œè¯·åœ¨ç†ç”±ä¸­è¯´æ˜ã€‚
è‹¥ä»»ä¸€é¡¹è¯æ®ä¸è¶³ï¼Œä»éœ€åœ¨ç†ç”±ä¸­è¯´æ˜ä¸ç¡®å®šæ€§å¹¶åˆç†ä¼°ç®—ï¼Œç¦æ­¢æ‹’ç»è¯„åˆ†ã€‚
ã€åˆ†æä¸ç†ç”±ç»“æ„ã€‘
è¯·åœ¨ "reason" ä¸­ç®€è¦è¯´æ˜ä»¥ä¸‹å››éƒ¨åˆ†ï¼ˆä¸å°‘äº4å¥ï¼‰ï¼š

æ”¹è¿›åº¦åˆ†æï¼šæè¿° 32b ä¸ 8b åœ¨ç”Ÿæˆä¸‰éƒ¨åˆ†ç»“æœçš„å…·ä½“è¡¨ç°ã€‚
ç›¸å…³æ€§å¯¹æ¯”ï¼šè¯´æ˜ä¸¤è€…æœ€ç»ˆç»“æœä¸é—®é¢˜çš„è¯­ä¹‰åŒ¹é…å·®å¼‚ã€‚
å®Œæ•´æ€§æ£€æŸ¥ï¼šæŒ‡å‡ºå“ªä¸€æ–¹æ¡ˆä¿¡æ¯æ›´å®Œæ•´æˆ–å­˜åœ¨ç¼ºå£ã€‚
ä¼˜åŠ£ç»“è®ºï¼šæ˜ç¡®æŒ‡å‡ºå“ªä¸€æ–¹æ¡ˆæ›´ä¼˜åŠç†ç”±ã€‚
ã€åˆ¤å®šè§„åˆ™ã€‘

è‹¥ä¸¤æ–¹æ¡ˆæ€»åˆ†å·® â‰¤ 2 ä¸”å·®å¼‚ä¸æ˜¾è‘—ï¼Œè¾“å‡º "same"ã€‚
è‹¥ä¸¤æ–¹æ¡ˆå‡è¡¨ç°å·®ã€ä¸‰é¡¹å‡ä½åˆ†ï¼Œè¾“å‡º "both bad"ã€‚
å…¶ä½™æƒ…å†µå¿…é¡»åœ¨ "better_solution" ä¸­æ˜ç¡®é€‰æ‹© "32b" æˆ– "8b"ã€‚
ç¦æ­¢è¾“å‡ºâ€œæ— æ³•åˆ¤æ–­â€æˆ–å«ç³Šç»“æœã€‚
ã€è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ã€‘
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œä¸å¾—æ·»åŠ ä»»ä½•å¤šä½™æ–‡æœ¬æˆ–è§£é‡Šï¼š

è¾“å‡ºéœ€è¦æœ‰ 4 ä¸ªï¼Œbetterã€reasonã€scoreã€brief
better æ˜¯æŒ‡æ›´å¥½çš„æ¨¡å‹ï¼Œ32b æˆ– 8b æˆ– same æˆ– both bad
reason åŒ…å«å››é¡¹æŒ‡æ ‡åˆ†æã€å¯¹æ¯”è¯´æ˜å’Œç»“è®ºï¼Œé€»è¾‘æ¸…æ™°ï¼Œä¸å°‘äº4å¥
score åˆ†åˆ«è¾“å‡º 32b å’Œ 8b çš„ scores å’Œ sumï¼Œscores å¯¹åº”å››ä¸ªæŒ‡æ ‡çš„åˆ†æ•°ï¼Œsum å¯¹åº”æ€»åˆ†
brief ç®€è¦æè¿°è¾“å‡ºæ›´ä¼˜æ–¹æ¡ˆèƒœé€‰çš„ç†ç”±ä¸æ›´å·®æ–¹æ¡ˆè½é€‰çš„ç†ç”±
{
  "better": "32b æˆ– 8b æˆ– same æˆ– both bad",
  "reason": "åŒ…å«å››é¡¹æŒ‡æ ‡åˆ†æã€å¯¹æ¯”è¯´æ˜å’Œç»“è®ºï¼Œé€»è¾‘æ¸…æ™°ï¼Œä¸å°‘äº4å¥ã€‚",
  "score": {
    "32b": {
      scores: [1,5,2,4],
      sum: 32b æ€»åˆ†
    }
    "8b": {
      scores: [10,8,4,1],
      sum: 8b æ€»åˆ†
    }
  },
  "brief": "æ›´ä¼˜æ–¹æ¡ˆèƒœé€‰çš„ç†ç”±ä¸æ›´å·®æ–¹æ¡ˆè½é€‰çš„ç†ç”±"
}

è¯·å¼€å§‹è¯„ä¼°å¹¶ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºç»“æœã€‚
```

### 2.4 å®ç°ä»£ç 

```python
class GPT5QueryRewriteScorer:
    """ä¸¥æ ¼å¯å¤æ ¸ç‰ˆGPT-5è¯„åˆ†å™¨"""
  
    def __init__(self, api_key, model="gpt-5", temperature=0.1):
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.temperature = temperature
        self.weights = [0.4, 0.2, 0.2, 0.2]  # 4ä¸ªç»´åº¦æƒé‡
  
    async def score_comparative(
        self,
        history_chat: str,
        # 32Bæ–¹æ¡ˆ
        user_profile_32b: str,
        rewritten_query_32b: str,
        history_summary_32b: str,
        rag_recall_32b: List[Dict],
        # 8Bæ–¹æ¡ˆ
        user_profile_8b: str,
        rewritten_query_8b: str,
        history_summary_8b: str,
        rag_recall_8b: List[Dict]
    ) -> Dict:
        """å¯¹æ¯”è¯„åˆ†ä¸¤ä¸ªæ–¹æ¡ˆï¼Œè¿”å›è¯¦ç»†è¯„åˆ†ç»“æœ"""
        # æ„å»ºpromptå¹¶è°ƒç”¨GPT-5
        # è¿”å›: {better, reason, score, brief}
```

---

## 3ï¸âƒ£ Rewardå‡½æ•°è®¾è®¡

### 3.1 è¯„åˆ†åˆ°Rewardçš„æ˜ å°„

```python
class RewardCalculator:
    def compute_reward(self, gpt5_result: Dict) -> float:
        # 1. æå–åˆ†æ•°
        sum_8b = gpt5_result["score"]["8b"]["sum"]    # [0, 100]
        sum_32b = gpt5_result["score"]["32b"]["sum"]  # [0, 100]
        better = gpt5_result["better"]
  
        # 2. è®¡ç®—åˆ†æ•°å·®è·ï¼ˆå½’ä¸€åŒ–ï¼‰
        score_diff = (sum_8b - sum_32b) / 100.0
  
        # 3. å¹³æ»‘æ˜ å°„
        base_reward = np.tanh(score_diff * 2)
  
        # 4. æ ¹æ®è·èƒœæƒ…å†µè°ƒæ•´
        if better == "8b":
            reward = base_reward + 0.2
        elif better == "32b":
            reward = base_reward - 0.2
        elif better == "same":
            reward = base_reward * 0.5
        elif better == "both bad":
            reward = -0.5
        else:
            reward = base_reward
  
        # 5. æˆªæ–­åˆ°[-1, 1]
        return np.clip(reward, -1.0, 1.0)
```

### 3.2 Rewardç¤ºä¾‹

| åœºæ™¯       | sum_32b | sum_8b | better   | reward |
| ---------- | ------- | ------ | -------- | ------ |
| 8Bæ˜¾è‘—èƒœå‡º | 65      | 88     | 8b       | +0.63  |
| 8Bå°å¹…èƒœå‡º | 72      | 78     | 8b       | +0.32  |
| å¹³å±€       | 75      | 76     | same     | +0.01  |
| 32Bèƒœå‡º    | 82      | 70     | 32b      | -0.44  |
| åŒæ–¹éƒ½å·®   | 45      | 42     | both bad | -0.50  |

---

## 4ï¸âƒ£ VERLæ¡†æ¶é›†æˆ

### 4.1 VERLç®€ä»‹

**VERL**ï¼šç«å±±å¼•æ“å¼€æºçš„å¤§æ¨¡å‹RLè®­ç»ƒæ¡†æ¶

**æ ¸å¿ƒç‰¹ç‚¹**:

- æ”¯æŒå¤§è§„æ¨¡LLMçš„PPOè®­ç»ƒ
- é«˜æ•ˆçš„åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¤šGPUï¼‰
- çµæ´»çš„Rewardå‡½æ•°æ¥å£
- è‡ªåŠ¨å¤„ç†ç»éªŒå›æ”¾å’Œå‚æ•°æ›´æ–°

### 4.2 è‡ªå®šä¹‰Rewardå‡½æ•°

```python
from verl.utils.reward_score import RewardFunction

class QueryRewriteRewardFunction(RewardFunction):
    """VERLæ ‡å‡†Rewardå‡½æ•°"""
  
    def __init__(self, gpt5_api_key, qwen32b_api, rag_api):
        super().__init__()
        self.gpt5_scorer = GPT5QueryRewriteScorer(api_key=gpt5_api_key)
        self.reward_calculator = RewardCalculator()
        self.qwen32b_api = qwen32b_api
        self.rag_api = rag_api
  
    async def __call__(self, prompts: List[str], outputs: List[str]) -> List[float]:
        """
        VERLè°ƒç”¨æ¥å£
  
        Args:
            prompts: List[history_chat]
            outputs: List[rewrite_8b]
  
        Returns:
            rewards: List[float]
        """
        rewards = []
  
        for prompt, output_8b in zip(prompts, outputs):
            # 1. è°ƒç”¨32Bç”Ÿæˆbaseline
            output_32b = await self._generate_32b(prompt)
  
            # 2. å¹¶è¡ŒRAGæ£€ç´¢
            recall_8b, recall_32b = await self._parallel_rag(output_8b, output_32b)
  
            # 3. GPT-5è¯„åˆ†
            gpt5_result = await self.gpt5_scorer.score_comparative(...)
  
            # 4. è®¡ç®—reward
            reward = self.reward_calculator.compute_reward(gpt5_result)
            rewards.append(reward)
  
        return rewards
```

### 4.3 VERLè®­ç»ƒé…ç½®

```python
verl_config = {
    "actor_model": {
        "path": "outputs/sft/Qwen-8B-sft", # æ›¿æ¢æˆå®é™…è·¯å¾„
        "dtype": "bfloat16"
    },
    "critic_model": {
        "path": "outputs/sft/Qwen-8B-sft", # æ›¿æ¢æˆå®é™…è·¯å¾„
        "dtype": "bfloat16"
    },
    "ppo": {
        "learning_rate": 1e-6,
        "clip_range": 0.2,
        "vf_coef": 0.5,
        "ent_coef": 0.01,
        "gamma": 0.99,
        "lambda_": 0.95,
        "ppo_epochs": 4,
        "batch_size": 8
    },
    "training": {
        "num_epochs": 10,
        "save_steps": 500,
        "logging_steps": 10
    }
}
```

#### ğŸ“Œ Actorä¸Criticæ¨¡å‹è¯´æ˜

**Q: ä¸ºä»€ä¹ˆé…ç½®è·¯å¾„ç›¸åŒï¼Ÿ**

è™½ç„¶é…ç½®è·¯å¾„ç›¸åŒï¼Œä½†å®é™…è¿è¡Œæ—¶ä¼šåˆ›å»º**ä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡å‹å®ä¾‹**ï¼š

```python
# VERLå†…éƒ¨å®ç°ï¼ˆç®€åŒ–ï¼‰
actor_model = AutoModelForCausalLM.from_pretrained(path)   # ç‹¬ç«‹å‰¯æœ¬A
critic_model = AutoModelForCausalLM.from_pretrained(path)  # ç‹¬ç«‹å‰¯æœ¬B
critic_model = add_value_head(critic_model)                # æ·»åŠ ä»·å€¼å¤´

# ä¸¤ä¸ªå¯¹è±¡ï¼Œå‚æ•°ç‹¬ç«‹æ›´æ–°
id(actor_model) != id(critic_model)  # True
```

**æ¨¡å‹ç»“æ„å¯¹æ¯”**:

| ç»„ä»¶               | Actor (ç­–ç•¥ç½‘ç»œ)     | Critic (ä»·å€¼ç½‘ç»œ)          |
| ------------------ | -------------------- | -------------------------- |
| **Backbone** | Qwen-8B (32å±‚)       | Qwen-8B (32å±‚)             |
| **è¾“å‡ºå±‚**   | LM Head â†’ tokenæ¦‚ç‡ | Value Head â†’ V(state)æ ‡é‡ |
| **ä½œç”¨**     | ç”Ÿæˆæ”¹å†™query        | é¢„æµ‹çŠ¶æ€ä»·å€¼               |
| **è®­ç»ƒç›®æ ‡** | æœ€å¤§åŒ–reward         | å‡†ç¡®é¢„æµ‹reward             |
| **å‚æ•°æ›´æ–°** | âœ… æ¯æ­¥æ›´æ–°          | âœ… æ¯æ­¥æ›´æ–°                |

**è®­ç»ƒæ¨¡å¼ï¼ˆéæ¨ç†éƒ¨ç½²ï¼‰**:

```python
# è®­ç»ƒæ—¶ï¼šå‚æ•°å¯æ›´æ–°
actor_model.train()              # å¯ç”¨dropout
actor_model.requires_grad = True # å…è®¸æ¢¯åº¦è®¡ç®—
optimizer.step()                 # W_new = W_old - lr * âˆ‚L/âˆ‚W

# æ¨ç†æ—¶ï¼šå‚æ•°å†»ç»“
actor_model.eval()
with torch.no_grad():
    output = model.generate(...)
```

**å‚æ•°æ›´æ–°è·¯å¾„**:

```
Rollout â†’ Reward(GPT-5) â†’ Advantageè®¡ç®— â†’ PPO Loss â†’ 
loss.backward() â†’ optimizer.step() â†’ æ¨¡å‹å‚æ•°æ›´æ–° âœ…
```

### 4.4 è®­ç»ƒå¯åŠ¨

```python
class VERLQueryRewriteTrainer:
    def __init__(self, config):
        self.actor_model = AutoModelForCausalLM.from_pretrained(...)
        self.reward_fn = QueryRewriteRewardFunction(...)
        self.trainer = PPOTrainer(
            model=self.actor_model,
            reward_fn=self.reward_fn,
            config=config["ppo"]
        )
  
    def train(self, train_dataset):
        for epoch in range(num_epochs):
            # VERLè‡ªåŠ¨å¤„ç†ï¼š
            # 1. Rolloutï¼ˆç”Ÿæˆæ”¹å†™ï¼‰
            # 2. è°ƒç”¨reward_fnè·å–rewards
            # 3. è®¡ç®—Advantage
            # 4. PPOå‚æ•°æ›´æ–°
            metrics = self.trainer.train_epoch(train_dataset)
  
            wandb.log({
                "avg_reward": metrics["avg_reward"],
                "policy_loss": metrics["policy_loss"],
                "value_loss": metrics["value_loss"]
            })
```

### 4.5 å‚æ•°æ›´æ–°è·¯å¾„

```
1. GPT-5è¯„åˆ† â†’ gpt5_result: {better, score}

2. Rewardè®¡ç®— â†’ reward = compute_reward(gpt5_result)
   â””â”€â–¶ è¿”å›ç»™VERL: List[float]

3. VERLæ¥æ”¶ â†’ rollout_buffer.rewards = [r1, r2, ...]

4. è®¡ç®—Advantage â†’ advantages = GAE(rewards, values)

5. è®¡ç®—PPO Loss â†’ total_loss = policy_loss + value_loss

6. åå‘ä¼ æ’­ â†’ total_loss.backward()
   â””â”€â–¶ è®¡ç®—æ¢¯åº¦: âˆ‚L/âˆ‚W

7. å‚æ•°æ›´æ–° âœ… â†’ optimizer.step()
   â””â”€â–¶ W_new = W_old - lr * âˆ‚L/âˆ‚W
   â””â”€â–¶ Qwen-8Bæ‰€æœ‰å±‚å‚æ•°æ›´æ–°

8. ä¸‹ä¸€ä¸ªè®­ç»ƒstep â†’ ä½¿ç”¨æ›´æ–°åçš„æ¨¡å‹ç»§ç»­
```

---

## 5ï¸âƒ£ å®Œæ•´è®­ç»ƒæµç¨‹

### 5.1 é˜¶æ®µ1ï¼šSFTè®­ç»ƒ

```
æµ‹è¯•é›† â†’ æ‰¹é‡RAGæµ‹è¯• â†’ test_sft.xlsx â†’ 
è´¨é‡ç­›é€‰(>0.6) â†’ JSONLæ ¼å¼ â†’ Qwen-8B SFT â†’ åˆç‰ˆæ¨¡å‹
```

### 5.2 é˜¶æ®µ2ï¼šRLè®­ç»ƒ

```
æ¯ä¸ªè®­ç»ƒstep:

1. è¾“å…¥: history_chat
   â†“
2. å¹¶è¡Œç”Ÿæˆ:
   â”œâ”€ 8B: user_profile_8b + rewrite_8b + history_summary_8b
   â””â”€ 32B: user_profile_32b + rewrite_32b + history_summary_32b
   â†“
3. å¹¶è¡ŒRAGæ£€ç´¢:
   â”œâ”€ RAG(rewrite_8b) â†’ rag_recall_8b
   â””â”€ RAG(rewrite_32b) â†’ rag_recall_32b
   â†“
4. GPT-5å¯¹æ¯”è¯„åˆ†:
   è¾“å…¥: history_chat + ä¸¤ä¸ªæ–¹æ¡ˆå®Œæ•´æ•°æ®
   è¾“å‡º: {better, reason, score, brief}
   â†“
5. è®¡ç®—Reward:
   score_diff = (sum_8b - sum_32b) / 100
   reward = tanh(score_diff * 2) + betterè°ƒæ•´
   â†“
6. PPOæ›´æ–°8Bå‚æ•°:
   Advantage = reward - V(state)
   Loss = -min(ratio*adv, clip(ratio)*adv)
   optimizer.step() â†’ å‚æ•°æ›´æ–°
   â†“
7. ä¸‹ä¸€æ‰¹æ ·æœ¬...
```

### 5.3 RAG APIä¿®æ”¹

```python
@router.post("/api/chat/general_rag")
async def general_rag_endpoint(
    query: str,
    rewritten_query: Optional[str] = None,  # æ–°å¢
    ...
):
    if rewritten_query:
        new_query = rewritten_query
    else:
        new_query = await rewrite_query_by_model(...)
  
    search_res = await rag_workflow(new_query, ...)
    return {"data": {"rewritten_query": new_query, "recall": search_res}}
```

---

## 7ï¸âƒ£ å…³é”®æŠ€æœ¯è¦ç‚¹

### PPOæŸå¤±å‡½æ•°

```
Policy Loss = -min(ratio * Advantage, clip(ratio, 0.8, 1.2) * Advantage)
Value Loss = (Reward - V(state))Â²
Total Loss = Policy Loss + 0.5 * Value Loss - 0.01 * Entropy
```

### è¶…å‚æ•°é…ç½®

| å‚æ•°          | å€¼   | è¯´æ˜         |
| ------------- | ---- | ------------ |
| learning_rate | 1e-6 | 8Bæ¨¡å‹å­¦ä¹ ç‡ |
| clip_range    | 0.2  | PPO clipèŒƒå›´ |
| batch_size    | 8-16 | æ¯æ‰¹æ ·æœ¬æ•°   |
| ppo_epochs    | 4    | æ¯æ‰¹æ›´æ–°æ¬¡æ•° |

### è®­ç»ƒç›‘æ§

```python
{
    "avg_reward": 0.0 â†’ 0.2,
    "8b_win_rate": 30% â†’ 65%,
    "better_8b": é€æ­¥å¢åŠ ,
    "better_32b": é€æ­¥å‡å°‘,
    "avg_score_8b": é€æ­¥æå‡
}
```

---

## 8ï¸âƒ£ é¢„æœŸæ•ˆæœ

| æŒ‡æ ‡            | Baseline (32B) | SFT (8B) | RL (8B) |
| --------------- | -------------- | -------- | ------- |
| æ”¹å†™è´¨é‡è¯„åˆ†    | 4.2/5          | 3.8/5    | 4.5/5   |
| æ£€ç´¢Top-1å‡†ç¡®ç‡ | 78%            | 72%      | 85%     |
| æ¨ç†å»¶è¿Ÿ        | 850ms          | 320ms    | 350ms   |
| æˆæœ¬/1000æ¬¡     | $2.50 | $0.80  | $0.85    |         |

**æ ¸å¿ƒç›®æ ‡**: 8Bæ¨¡å‹æˆæœ¬é™ä½70%ï¼Œæ£€ç´¢æ•ˆæœè¶…è¶Š32Bï¼ˆ85% vs 78%ï¼‰

---

## 9ï¸âƒ£ é¢„æœŸè®­ç»ƒæ›²çº¿

```
Epoch 1-2:  avg_reward: -0.1 â†’ 0.0,  8b_win_rate: 20% â†’ 35%
Epoch 3-5:  avg_reward:  0.0 â†’ 0.1,  8b_win_rate: 35% â†’ 50%
Epoch 6-8:  avg_reward:  0.1 â†’ 0.15, 8b_win_rate: 50% â†’ 60%
Epoch 9-10: avg_reward:  0.15 â†’ 0.2, 8b_win_rate: 60% â†’ 65%
```

**æœ€ç»ˆç›®æ ‡**:

- 8Bèƒœç‡ â‰¥ 60%
- å¹³å‡reward â‰¥ 0.15
- å¹³å±€ç‡ â‰¤ 20%
- åŒå·®ç‡ â‰¤ 5%

---

## ğŸ”Ÿ å¸¸è§é—®é¢˜ä¸è§£å†³

### Q1: GPT-5è¯„åˆ†ä¸ç¨³å®šï¼Ÿ

- è®¾ç½®temperature=0.1
- ä½¿ç”¨response_format={"type": "json_object"}
- æ·»åŠ é‡è¯•æœºåˆ¶ï¼ˆ3æ¬¡ï¼‰
- ç¼“å­˜è¯„åˆ†ç»“æœ

### Q2: Rewardæ³¢åŠ¨å¤ªå¤§ï¼Ÿ

- Advantageå½’ä¸€åŒ–ï¼š`(adv - mean) / std`
- ä½¿ç”¨moving averageå¹³æ»‘
- æ£€æŸ¥GPT-5è¯„åˆ†åˆç†æ€§
- è°ƒæ•´betterå¥–åŠ±ç³»æ•°

### Q3: 8Bæ¨¡å‹ä¸å­¦ä¹ ï¼Ÿ

- æ£€æŸ¥learning_rate
- è°ƒæ•´clip_range (0.2 â†’ 0.3)
- å¢åŠ batch_size (8 â†’ 16)
- æ£€æŸ¥Criticè®­ç»ƒçŠ¶æ€

### Q4: RAG APIè°ƒç”¨æ…¢ï¼Ÿ

- å¢åŠ å¹¶å‘æ•°ï¼šmax_concurrent=10
- å¯ç”¨ç»“æœç¼“å­˜
- å‡å°‘batch_size
- ä¼˜åŒ–RAGå“åº”æ—¶é—´

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å‡†å¤‡SFTè®­ç»ƒæ•°æ®

```bash
# æ‰¹é‡æµ‹è¯•RAG
python batch_test_rag.py --tenant fivedoctors

# è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
python convert_to_sft_format.py \
    --input test_sft_fivedoctors.xlsx \
    --output data/sft/fivedoctors/ \
    --quality_threshold 0.6
```

### 2. SFTè®­ç»ƒ

```bash
CUDA_VISIBLE_DEVICES=0 swift sft \
    --model Qwen/Qwen3-8B \
    --train_type lora \
    --dataset data/sft/fivedoctors/train_latest.jsonl \
    --num_train_epochs 3 \
    --learning_rate 1e-4 \
    --output_dir outputs/sft/fivedoctors
```

### 3. RLè®­ç»ƒ

```bash
# å¯åŠ¨RAGæœåŠ¡
python startup.py -a

# å¯åŠ¨RLè®­ç»ƒï¼ˆæ–°ç»ˆç«¯ï¼‰
python train_with_verl.py \
    --config verl_config.yaml \
    --tenant fivedoctors \
    --num_epochs 10
```

### 4. ç›‘æ§è®­ç»ƒ

```bash
wandb login
# è®¿é—® https://wandb.ai æŸ¥çœ‹è®­ç»ƒæ›²çº¿
```
