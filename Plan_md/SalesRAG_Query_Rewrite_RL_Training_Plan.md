# Sales-RAG Queryæ”¹å†™RLè®­ç»ƒæ–¹æ¡ˆ

> åŸºäºQwen-8Bçš„ä¸¤é˜¶æ®µè®­ç»ƒï¼šSFTçŸ¥è¯†è’¸é¦ + RLç«äº‰ä¼˜åŒ–
>
> ï¼ˆå…ˆåŸºäºQwen-8Bè·‘é€šå…¨æµç¨‹ï¼Œåç»­ä¼šè¿ç§»åˆ°Qwen-32Bçš„RLè®­ç»ƒä¸­ï¼‰

---

## ğŸ“‹ æ–¹æ¡ˆæ¦‚è¿°

### æ ¸å¿ƒæ€è·¯

```
é˜¶æ®µ1: SFTçŸ¥è¯†è’¸é¦
Qwen-32B (Teacher) â†’ æ”¹å†™æ•°æ® â†’ Qwen-8B (Student) SFTè®­ç»ƒ

é˜¶æ®µ2: RLç«äº‰ä¼˜åŒ–  
Qwen-8B â†” Qwen-32B (åŒæ¨¡å‹ç«äº‰) + GPT-5è¯„åˆ† â†’ PPO/GRPOä¼˜åŒ–
```

### æŠ€æœ¯æ ˆ

- **åŸºåº§æ¨¡å‹**: Qwen3-8B-Instruct
- **æ•™å¸ˆæ¨¡å‹**: Qwen-32B (ç°æœ‰éƒ¨ç½²)
- **è¯„åˆ†æ¨¡å‹**: GPT-5/Deepseek v3.1 (APIè°ƒç”¨)
- **RLç®—æ³•**: PPO (Proximal Policy Optimization)
- **è®­ç»ƒæ¡†æ¶**: VERL (å‚è€ƒDeepRetrieval)

---

## 1. è®­ç»ƒæ•°æ®é›†è®¾è®¡

### 1.1 æ•°æ®æ¥æº - BVT æµ‹è¯•é›†ä¸­è·å–çš„RAGæ—¥å¿—

ä½¿ç”¨BVTæµ‹è¯•é›†ï¼ˆæ©™å•¦æˆ–å…¶ä»–å®¢æˆ·ï¼‰ï¼Œæ‰¹é‡æµ‹è¯•åè·å–å½“å‰32Bçš„æ”¹å†™ç»“æœ

ä»sales-ragç³»ç»Ÿçš„æ—¥å¿—ä¸­æå–çœŸå®ç”¨æˆ·queryå’Œ32Bæ”¹å†™ç»“æœï¼š

```python
            "original_query": payload.get("query", "")
            "rewritten_query": response.get("rewritten_query", "")
            "user_profile": response.get("user_profile", "")
            "history_summary": response.get("history_summary", "")
```

### 1.2 æ•°æ®ç»“æ„å®šä¹‰

#### SFTè®­ç»ƒæ•°æ®æ ¼å¼

```json
{
  "dataset_name": "sales_rag_query_rewrite_sft",
  "version": "1.0.0",
  "tenant_id": "fivedoctors",
  "created_at": "2025-01-20T10:00:00Z",
  "total_samples": 15000,
  
  "samples": [
    {
      "sample_id": "fivedr_001",
      "original_query": "èƒ¶åŸè›‹ç™½æ€ä¹ˆåƒ",
      "rewritten_query": "èƒ¶åŸè›‹ç™½è‚½ æœç”¨æ–¹æ³• æ¨èç”¨é‡ é€‚ç”¨äººç¾¤",
  
      "context": {
        "user_profile": "25-35å²å¥³æ€§ï¼Œå…³æ³¨æŠ—è¡°è€å’Œçš®è‚¤å¥åº·",
        "history_summary": "è¿‘æœŸå’¨è¯¢è¿‡å¤šæ¬¡èƒ¶åŸè›‹ç™½äº§å“ï¼Œå…³å¿ƒæ•ˆæœå’Œä½¿ç”¨æ–¹æ³•",
        "history_context": "ç”¨æˆ·: æˆ‘æƒ³äº†è§£ä¿å¥å“\nåŠ©æ‰‹: å¥½çš„ï¼Œæˆ‘æ¥ä¸ºæ‚¨ä»‹ç»...\nç”¨æˆ·: èƒ¶åŸè›‹ç™½æ€ä¹ˆåƒ",
        "thought_unit": "ç”¨æˆ·å¯èƒ½æƒ³äº†è§£å…·ä½“çš„æœç”¨æŒ‡å—"
      },
  
      "metadata": {
        "source": "production_log",
        "timestamp": "2025-01-15T10:23:45Z",
        "model": "Qwen-32B",
        "quality_score": 4.5
      }
    }
  ]
}
```

#### RLè®­ç»ƒæ•°æ®æ ¼å¼

```json
{
  "dataset_name": "sales_rag_query_rewrite_rl",
  "version": "1.0.0",
  "tenant_id": "fivedoctors",
  
  "samples": [
    {
      "sample_id": "fivedr_rl_001",
      "original_query": "å­•å¦‡èƒ½å–å—",
      "context": {
        "user_profile": "å¤‡å­•æœŸå¥³æ€§ï¼Œ28å²",
        "history_summary": "å’¨è¯¢è¿‡èƒ¶åŸè›‹ç™½äº§å“",
        "history_context": "ç”¨æˆ·: èƒ¶åŸè›‹ç™½æ•ˆæœæ€ä¹ˆæ ·\nåŠ©æ‰‹: ...\nç”¨æˆ·: å­•å¦‡èƒ½å–å—"
      },
  
      "candidates": {
        "qwen_8b_rewrite": "èƒ¶åŸè›‹ç™½è‚½ å­•å¦‡ç¦å¿Œ å­•æœŸæœç”¨å®‰å…¨æ€§",
        "qwen_32b_rewrite": "å­•å¦‡ å¤‡å­•æœŸ èƒ¶åŸè›‹ç™½è‚½ æœç”¨ç¦å¿Œ æ³¨æ„äº‹é¡¹"
      },
  
      "retrieval_results": {
        "qwen_8b_results": [
          {
            "content": "å­•å¦‡åŠå¤‡å­•æœŸå¥³æ€§ä¸å»ºè®®æœç”¨èƒ¶åŸè›‹ç™½è‚½...",
            "score": 0.87,
            "reranker_score": 0.92
          }
        ],
        "qwen_32b_results": [
          {
            "content": "å¤‡å­•æœŸé—´å»ºè®®åœç”¨èƒ¶åŸè›‹ç™½è‚½è¡¥å……å‰‚...",
            "score": 0.91,
            "reranker_score": 0.95
          }
        ]
      },
  
      "evaluation": {
        "gpt5_scores": {
          "qwen_8b": {
            "æ”¹å†™è´¨é‡": 4.2,
            "é¢†åŸŸé€‚é…": 4.0,
            "æ„å›¾ä¿æŒ": 4.5,
            "å¯æ£€ç´¢æ€§": 4.3,
            "ç»¼åˆå¾—åˆ†": 4.25,
            "è¯„åˆ†ç†ç”±": "æ”¹å†™è´¨é‡è¾ƒå¥½ï¼Œä¿ç•™äº†åŸæ„"
          },
          "qwen_32b": {
            "æ”¹å†™è´¨é‡": 4.7,
            "é¢†åŸŸé€‚é…": 4.8,
            "æ„å›¾ä¿æŒ": 4.9,
            "å¯æ£€ç´¢æ€§": 4.6,
            "ç»¼åˆå¾—åˆ†": 4.75,
            "è¯„åˆ†ç†ç”±": "æ”¹å†™è´¨é‡ä¼˜ç§€ï¼Œé¢†åŸŸé€‚é…æ€§å¼º"
          }
        },
        "retrieval_metrics": {
          "qwen_8b": {
            "top1_score": 0.87,
            "avg_top3_score": 0.84,
            "recall_at_3": 1.0
          },
          "qwen_32b": {
            "top1_score": 0.91,
            "avg_top3_score": 0.89,
            "recall_at_3": 1.0
          }
        },
        "winner": "qwen_32b",
        "margin": 0.50
      },
  
      "reward": 0.15  # åŸºäºwinner marginè®¡ç®—
    }
  ]
}
```

### 1.3 æ•°æ®æ”¶é›†è„šæœ¬

å®Œæ•´çš„æ•°æ®æ”¶é›†pipelineï¼š

```python
# data_collection_pipeline.py

import asyncio
from pathlib import Path
from datetime import datetime, timedelta
import json
import pandas as pd
from typing import List, Dict

class QueryRewriteDataCollector:
    """Queryæ”¹å†™æ•°æ®æ”¶é›†å™¨"""
  
    def __init__(self, output_dir: str = "data/query_rewrite_training"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
  
    async def collect_all_data(
        self,
        tenant_ids: List[str] = ["fivedoctors", "chengla", "zlkt"],
        days_back: int = 30
    ):
        """æ”¶é›†æ‰€æœ‰è®­ç»ƒæ•°æ®"""
  
        all_data = {}
  
        for tenant_id in tenant_ids:
            print(f"æ”¶é›† {tenant_id} çš„æ•°æ®...")
  
            # 1. æ”¶é›†çº¿ä¸Šæ—¥å¿—æ•°æ®
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days_back)
  
            log_data = extract_query_rewrite_data(
                tenant_id=tenant_id,
                start_date=start_date,
                end_date=end_date,
                min_samples=10000
            )
  
            # 2. æ”¶é›†æµ‹è¯•é›†æ•°æ®
            test_data = extract_from_test_data(tenant_id)
  
            # 3. åˆå¹¶æ•°æ®
            combined_data = log_data + test_data
  
            # 4. æ•°æ®æ¸…æ´—å’Œè´¨é‡è¯„ä¼°
            cleaned_data = self.clean_data(combined_data)
  
            # 5. æ•°æ®å¢å¼º
            augmented_data = self.augment_data(cleaned_data, tenant_id)
  
            all_data[tenant_id] = augmented_data
  
            print(f"  - æ—¥å¿—æ•°æ®: {len(log_data)} æ¡")
            print(f"  - æµ‹è¯•æ•°æ®: {len(test_data)} æ¡")
            print(f"  - æ¸…æ´—å: {len(cleaned_data)} æ¡")
            print(f"  - å¢å¼ºå: {len(augmented_data)} æ¡")
  
        # 6. ä¿å­˜æ•°æ®
        self.save_datasets(all_data)
  
        return all_data
  
    def clean_data(self, data: List[Dict]) -> List[Dict]:
        """æ•°æ®æ¸…æ´—"""
  
        cleaned = []
  
        for item in data:
            # 1. å»é‡
            if self._is_duplicate(item, cleaned):
                continue
  
            # 2. è´¨é‡æ£€æŸ¥
            if not self._quality_check(item):
                continue
  
            # 3. è§„èŒƒåŒ–
            normalized_item = self._normalize(item)
  
            cleaned.append(normalized_item)
  
        return cleaned
  
    def _quality_check(self, item: Dict) -> bool:
        """æ•°æ®è´¨é‡æ£€æŸ¥"""
  
        original = item.get("original_query", "")
        rewritten = item.get("rewritten_query", "")
  
        # åŸºæœ¬æ£€æŸ¥
        if not original or not rewritten:
            return False
  
        # é•¿åº¦æ£€æŸ¥
        if len(original) < 2 or len(original) > 200:
            return False
  
        if len(rewritten) < 2 or len(rewritten) > 500:
            return False
  
        # ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ˆé¿å…æ”¹å†™å‰åå®Œå…¨ä¸€è‡´ï¼‰
        if original.strip() == rewritten.strip():
            return False
  
        # è¿‡åº¦æ”¹å†™æ£€æŸ¥ï¼ˆæ”¹å†™åä¸åº”è¯¥è¿‡é•¿ï¼‰
        if len(rewritten) > len(original) * 5:
            return False
  
        return True
  
    def augment_data(self, data: List[Dict], tenant_id: str) -> List[Dict]:
        """æ•°æ®å¢å¼º"""
  
        augmented = data.copy()
  
        # åŒä¹‰è¯æ›¿æ¢å¢å¼º
        for item in data[:len(data)//3]:  # å¯¹1/3æ•°æ®è¿›è¡Œå¢å¼º
            aug_item = self._synonym_augmentation(item, tenant_id)
            if aug_item:
                augmented.append(aug_item)
  
        return augmented
  
    def _synonym_augmentation(self, item: Dict, tenant_id: str) -> Dict:
        """åŒä¹‰è¯æ›¿æ¢å¢å¼º"""
  
        # é’ˆå¯¹ä¸åŒç§Ÿæˆ·çš„åŒä¹‰è¯åº“
        synonyms = {
            "fivedoctors": {
                "æ€ä¹ˆ": ["å¦‚ä½•", "æ€æ ·"],
                "åƒ": ["æœç”¨", "ä½¿ç”¨"],
                "æ•ˆæœ": ["ä½œç”¨", "åŠŸæ•ˆ"],
            },
            "chengla": {
                "å­¦ä¹ ": ["å¤‡è€ƒ", "å¤ä¹ "],
                "è¯¾ç¨‹": ["è¯¾å ‚", "åŸ¹è®­"],
            }
        }
  
        tenant_synonyms = synonyms.get(tenant_id, {})
  
        original = item["original_query"]
        rewritten = item["rewritten_query"]
  
        # éšæœºæ›¿æ¢
        import random
        for word, syns in tenant_synonyms.items():
            if word in original and random.random() < 0.3:
                syn = random.choice(syns)
                original = original.replace(word, syn, 1)
                rewritten = rewritten.replace(word, syn, 1)
  
        if original != item["original_query"]:
            aug_item = item.copy()
            aug_item["original_query"] = original
            aug_item["rewritten_query"] = rewritten
            aug_item["metadata"]["augmented"] = True
            return aug_item
  
        return None
  
    def save_datasets(self, all_data: Dict[str, List[Dict]]):
        """ä¿å­˜æ•°æ®é›†"""
  
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
  
        for tenant_id, data in all_data.items():
            # åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
            train_size = int(len(data) * 0.8)
            val_size = int(len(data) * 0.1)
  
            train_data = data[:train_size]
            val_data = data[train_size:train_size + val_size]
            test_data = data[train_size + val_size:]
  
            # SFTæ ¼å¼æ•°æ®
            sft_dir = self.output_dir / "sft" / tenant_id
            sft_dir.mkdir(parents=True, exist_ok=True)
  
            self._save_jsonl(train_data, sft_dir / f"train_{timestamp}.jsonl")
            self._save_jsonl(val_data, sft_dir / f"val_{timestamp}.jsonl")
            self._save_jsonl(test_data, sft_dir / f"test_{timestamp}.jsonl")
  
            # åˆ›å»ºç¬¦å·é“¾æ¥åˆ°latest
            for split in ["train", "val", "test"]:
                latest_link = sft_dir / f"{split}_latest.jsonl"
                if latest_link.exists():
                    latest_link.unlink()
                latest_link.symlink_to(f"{split}_{timestamp}.jsonl")
  
            # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            self._generate_stats_report(tenant_id, {
                "train": train_data,
                "val": val_data,
                "test": test_data
            })
  
    def _save_jsonl(self, data: List[Dict], filepath: Path):
        """ä¿å­˜ä¸ºJSONLæ ¼å¼"""
        with open(filepath, "w", encoding="utf-8") as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
  
    def _generate_stats_report(self, tenant_id: str, splits: Dict):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
  
        report = {
            "tenant_id": tenant_id,
            "timestamp": datetime.now().isoformat(),
            "splits": {}
        }
  
        for split_name, split_data in splits.items():
            report["splits"][split_name] = {
                "total_samples": len(split_data),
                "avg_original_length": sum(len(d["original_query"]) for d in split_data) / len(split_data),
                "avg_rewritten_length": sum(len(d["rewritten_query"]) for d in split_data) / len(split_data),
                "has_user_profile": sum(1 for d in split_data if d.get("context", {}).get("user_profile")),
                "has_history": sum(1 for d in split_data if d.get("context", {}).get("history_context")),
            }
  
        report_path = self.output_dir / "sft" / tenant_id / "stats_report.json"
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
  
        print(f"\nç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        print(json.dumps(report, ensure_ascii=False, indent=2))


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    collector = QueryRewriteDataCollector()
  
    data = await collector.collect_all_data(
        tenant_ids=["fivedoctors", "chengla"],
        days_back=60
    )
  
    print("\næ•°æ®æ”¶é›†å®Œæˆï¼")
    print(f"è¾“å‡ºç›®å½•: {collector.output_dir}")

if __name__ == "__main__":
    asyncio.run(main())##
```

## 2. åŸºäºæµ‹è¯•é›†ç”ŸæˆSFTè®­ç»ƒï¼ˆå®æˆ˜æ–¹æ¡ˆï¼‰

#### æ ¸å¿ƒæ€è·¯

åœ¨å®é™…ä¸šåŠ¡ä¸­ï¼Œæˆ‘ä»¬å·²ç»æœ‰ç°æˆçš„æµ‹è¯•é›†å’ŒRAGæ¡†æ¶ï¼Œå¯ä»¥ç›´æ¥åˆ©ç”¨ï¼š

1. **æ‰¹é‡æµ‹è¯•RAGæ¡†æ¶**ï¼šä½¿ç”¨æµ‹è¯•é›†æ‰¹é‡è°ƒç”¨ç°æœ‰RAGç³»ç»Ÿï¼Œæ”¶é›†32Bæ”¹å†™ç»“æœå’Œæ£€ç´¢æ•ˆæœ
2. **ä¿å­˜ä¸ºExcel**ï¼šå°†æµ‹è¯•ç»“æœä¿å­˜åˆ° `test_sft.xlsx`ï¼ŒåŒ…å«å®Œæ•´çš„queryã€æ”¹å†™ã€æ£€ç´¢ç»“æœ
3. **è½¬æ¢ä¸ºè®­ç»ƒæ•°æ®**ï¼šå°†Excelè½¬æ¢ä¸ºSFTè®­ç»ƒæ ¼å¼ï¼ˆJSONLï¼‰
4. **è´¨é‡ç­›é€‰**ï¼šæ ¹æ®æ£€ç´¢æ•ˆæœç­›é€‰é«˜è´¨é‡æ ·æœ¬ç”¨äºè®­ç»ƒ

è¿™ç§æ–¹å¼çš„ä¼˜åŠ¿ï¼š

- âœ… **çœŸå®æ•°æ®**ï¼šæ¥è‡ªå®é™…ä¸šåŠ¡åœºæ™¯çš„æµ‹è¯•é›†
- âœ… **å¿«é€Ÿè·å–**ï¼šæ— éœ€ç­‰å¾…çº¿ä¸Šæ—¥å¿—ç§¯ç´¯
- âœ… **è´¨é‡å¯æ§**ï¼šæµ‹è¯•é›†é€šå¸¸ç»è¿‡äººå·¥å®¡æ ¸
- âœ… **åŒ…å«æ£€ç´¢åé¦ˆ**ï¼šåŒæ—¶è·å¾—æ”¹å†™ç»“æœå’Œæ£€ç´¢æ•ˆæœ

#### æ­¥éª¤1ï¼šæ‰¹é‡æµ‹è¯•RAGæ¡†æ¶

é€šè¿‡BVTæ•°æ®é›†è¿›è¡ŒRAGæ‰¹é‡åŒ–æµ‹è¯•ï¼Œä¿å­˜ä¸‹é¢å‡ ç±»INFOç”¨äºSFTè®­ç»ƒé›†

```python
 "data": {  
	"user_profile": user_profileor"",   
	 "history_summary": history_summaryor"",   
	 "rewritten_query": new_query, 
	 "recall": search_res1    },
```

#### æ­¥éª¤2ï¼šä»test_sft.xlsxè½¬æ¢ä¸ºSFTè®­ç»ƒæ•°æ®

æ„å»ºjsonæ ¼å¼çš„è®­ç»ƒé›†

```python
# convert_test_to_sft.py

import pandas as pd
import json
from typing import List, Dict
from pathlib import Path

class TestToSFTConverter:
    """å°†æµ‹è¯•ç»“æœè½¬æ¢ä¸ºSFTè®­ç»ƒæ•°æ®"""
  
    def __init__(self, tenant_id: str = "fivedoctors"):
        self.tenant_id = tenant_id
  
        # ç³»ç»Ÿpromptï¼ˆä¸å‰é¢å®šä¹‰ä¸€è‡´ï¼‰
        self.system_prompts = {
            "fivedoctors": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿å¥å“çŸ¥è¯†åº“æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶...""",
            "chengla": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•™è‚²åŸ¹è®­çŸ¥è¯†åº“æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶..."""
        }
  
    def convert_excel_to_jsonl(
        self,
        excel_path: str,
        output_jsonl: str,
        quality_threshold: float = 0.6
    ):
        """å°†Excelè½¬æ¢ä¸ºJSONLè®­ç»ƒæ ¼å¼
  
        Args:
            excel_path: test_sft.xlsxè·¯å¾„
            output_jsonl: è¾“å‡ºçš„JSONLæ–‡ä»¶è·¯å¾„
            quality_threshold: è´¨é‡é˜ˆå€¼ï¼ˆåŸºäºtop1_scoreç­›é€‰ï¼‰
        """
  
        # è¯»å–Excel
        df = pd.read_excel(excel_path)
        print(f"ğŸ“š è¯»å–æµ‹è¯•ç»“æœ: {len(df)} æ¡")
  
        # è´¨é‡ç­›é€‰
        # 1. åªä¿ç•™æˆåŠŸçš„æµ‹è¯•
        df = df[df['success'] == True]
  
        # 2. ç­›é€‰æ£€ç´¢æ•ˆæœå¥½çš„æ ·æœ¬ï¼ˆtop1_score > thresholdï¼‰
        df = df[df['top1_score'] >= quality_threshold]
  
        # 3. ç¡®ä¿æ”¹å†™ä¸ä¸ºç©ºä¸”ä¸åŸqueryä¸åŒ
        df = df[
            (df['rewritten_query'].notna()) &
            (df['rewritten_query'] != df['original_query'])
        ]
  
        print(f"âœ… è´¨é‡ç­›é€‰å: {len(df)} æ¡ (ä¿ç•™ç‡: {len(df)/len(df)*100:.1f}%)")
  
        # è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
        training_samples = []
        system_prompt = self.system_prompts.get(self.tenant_id, "")
  
        for _, row in df.iterrows():
            # æ„å»ºç”¨æˆ·è¾“å…¥
            user_content = f"""åŸå§‹æŸ¥è¯¢: {row['original_query']}"""
      
            # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if pd.notna(row.get('user_profile')) and row['user_profile']:
                user_content += f"\n\nç”¨æˆ·ç”»åƒ: {row['user_profile']}"
      
            if pd.notna(row.get('history_summary')) and row['history_summary']:
                user_content += f"\n\nå†å²æ‘˜è¦: {row['history_summary']}"
      
            user_content += "\n\nè¯·æ”¹å†™è¿™ä¸ªæŸ¥è¯¢ï¼Œä½¿å…¶æ›´é€‚åˆçŸ¥è¯†åº“æ£€ç´¢ã€‚"
      
            # æ„å»ºå¯¹è¯
            sample = {
                "messages": [
                    {
                        "role": "system",
                        "content": system_prompt
                    },
                    {
                        "role": "user",
                        "content": user_content
                    },
                    {
                        "role": "assistant",
                        "content": row['rewritten_query']
                    }
                ],
                "metadata": {
                    "source": "test_set",
                    "tenant_id": self.tenant_id,
                    "top1_score": float(row['top1_score']),
                    "recall_count": int(row['recall_count'])
                }
            }
      
            training_samples.append(sample)
  
        # ä¿å­˜ä¸ºJSONL
        output_path = Path(output_jsonl)
        output_path.parent.mkdir(parents=True, exist_ok=True)
  
        with open(output_jsonl, 'w', encoding='utf-8') as f:
            for sample in training_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
  
        print(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜: {output_jsonl}")
        print(f"   æ€»æ ·æœ¬æ•°: {len(training_samples)}")
  
        return training_samples
  
    def split_train_val_test(
        self,
        jsonl_path: str,
        train_ratio: float = 0.8,
        val_ratio: float = 0.1
    ):
        """åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†"""
  
        # è¯»å–æ‰€æœ‰æ ·æœ¬
        samples = []
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                samples.append(json.loads(line))
  
        # æ‰“ä¹±
        import random
        random.shuffle(samples)
  
        # åˆ’åˆ†
        n = len(samples)
        train_size = int(n * train_ratio)
        val_size = int(n * val_ratio)
  
        train_samples = samples[:train_size]
        val_samples = samples[train_size:train_size + val_size]
        test_samples = samples[train_size + val_size:]
  
        # ä¿å­˜
        base_dir = Path(jsonl_path).parent
  
        splits = {
            'train': train_samples,
            'val': val_samples,
            'test': test_samples
        }
  
        for split_name, split_data in splits.items():
            output_file = base_dir / f"{split_name}_latest.jsonl"
            with open(output_file, 'w', encoding='utf-8') as f:
                for sample in split_data:
                    f.write(json.dumps(sample, ensure_ascii=False) + '\n')
            print(f"  - {split_name}: {len(split_data)} æ¡ â†’ {output_file}")
  
        return splits


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    converter = TestToSFTConverter(tenant_id="fivedoctors")
  
    # è½¬æ¢
    samples = converter.convert_excel_to_jsonl(
        excel_path="data/test_sft_fivedoctors.xlsx",
        output_jsonl="data/sft/fivedoctors/all_samples.jsonl",
        quality_threshold=0.6
    )
  
    # åˆ’åˆ†æ•°æ®é›†
    converter.split_train_val_test(
        jsonl_path="data/sft/fivedoctors/all_samples.jsonl"
    )
  
    print("\nâœ¨ SFTæ•°æ®å‡†å¤‡å®Œæˆï¼")
```

#### æ­¥éª¤3ï¼šå¿«é€Ÿå¯åŠ¨SFTè®­ç»ƒ

å‡†å¤‡å¥½æ•°æ®åï¼Œå¯ä»¥ç›´æ¥å¼€å§‹è®­ç»ƒï¼š

åŸºäºms-swiftè®­ç»ƒæ¡†æ¶

https://swift.readthedocs.io/zh-cn/latest/BestPractices/Qwen3%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.html

```bash
# æ˜¾å­˜å ç”¨ï¼š22GB
CUDA_VISIBLE_DEVICES=0 \
swift sft \
    --model Qwen/Qwen3-8B \
    --train_type lora \
    --dataset 'swift/Qwen3-SFT-Mixin#2000' \
              'swift/self-cognition:qwen3#600' \
    --load_from_cache_file true \
    --torch_dtype bfloat16 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --learning_rate 1e-4 \
    --lora_rank 8 \
    --lora_alpha 32 \
    --target_modules all-linear \
    --gradient_accumulation_steps 16 \
    --eval_steps 50 \
    --save_steps 50 \
    --save_total_limit 2 \
    --logging_steps 5 \
    --max_length 2048 \
    --output_dir output \
    --warmup_ratio 0.05 \
    --dataloader_num_workers 4 \
    --model_author swift \
    --model_name swift-robot
```

**é¢„æœŸæ•°æ®è§„æ¨¡**ï¼š

- æµ‹è¯•é›†: 500-1000æ¡
- è´¨é‡ç­›é€‰å: 300-800æ¡ (top1_score > 0.6)
- è®­ç»ƒé›†: 240-640æ¡
- éªŒè¯é›†: 30-80æ¡
- æµ‹è¯•é›†: 30-80æ¡

#### test_sft.xlsx å­—æ®µè¯´æ˜

| å­—æ®µ            | è¯´æ˜                 | ç¤ºä¾‹                           |
| --------------- | -------------------- | ------------------------------ |
| original_query  | åŸå§‹ç”¨æˆ·query        | "èƒ¶åŸè›‹ç™½æ€ä¹ˆåƒ"               |
| user_profile    | ç”¨æˆ·ç”»åƒ             | "25-35å²å¥³æ€§ï¼Œå…³æ³¨æŠ—è¡°è€"      |
| history_summary | å†å²æ‘˜è¦             | "è¿‘æœŸå’¨è¯¢è¿‡å¤šæ¬¡èƒ¶åŸè›‹ç™½äº§å“"   |
| rewritten_query | 32Bæ”¹å†™çš„query       | "èƒ¶åŸè›‹ç™½è‚½ æœç”¨æ–¹æ³• æ¨èç”¨é‡" |
| recall_results  | æ£€ç´¢ç»“æœåˆ—è¡¨ï¼ˆJSONï¼‰ | [...]                          |
| recall_count    | å¬å›æ–‡æ¡£æ•°           | 5                              |
| top1_score      | Top1æ–‡æ¡£rerankeråˆ†æ•° | 0.87                           |
| avg_top3_score  | Top3å¹³å‡åˆ†æ•°         | 0.82                           |
| success         | æµ‹è¯•æ˜¯å¦æˆåŠŸ         | True                           |

---

## 3. RLè®­ç»ƒè¯¦ç»†æ­¥éª¤

### 3.1 GPT-5è¯„åˆ†æ¨¡å‹é…ç½®

#### ä¸ºä»€ä¹ˆä½¿ç”¨GPT-5ä½œä¸ºè¯„åˆ†æ¨¡å‹ï¼Ÿ

ç›¸æ¯”è®­ç»ƒä¸“é—¨çš„è¯„åˆ†æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨GPT-5 APIæœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

**âœ… ä¼˜åŠ¿**

1. **é›¶è®­ç»ƒæˆæœ¬**: æ— éœ€å‡†å¤‡å¤§é‡æ ‡æ³¨æ•°æ®å’ŒGPUèµ„æºè®­ç»ƒè¯„åˆ†æ¨¡å‹
2. **é«˜è´¨é‡è¯„åˆ†**: GPT-5å…·å¤‡å¼ºå¤§çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œè¯„åˆ†æ›´å‡†ç¡®å’Œä¸€è‡´
3. **çµæ´»å¯è°ƒ**: é€šè¿‡promptå·¥ç¨‹å³å¯å¿«é€Ÿè°ƒæ•´è¯„åˆ†æ ‡å‡†ï¼Œæ— éœ€é‡æ–°è®­ç»ƒ
4. **å¿«é€Ÿä¸Šçº¿**: çœå»è¯„åˆ†æ¨¡å‹è®­ç»ƒçš„1-2å¤©æ—¶é—´ï¼ŒåŠ é€Ÿæ•´ä½“è®­ç»ƒæµç¨‹
5. **å¯è§£é‡Šæ€§**: GPT-5å¯ä»¥æä¾›è¯„åˆ†ç†ç”±ï¼Œä¾¿äºç†è§£å’Œè°ƒè¯•

**âš ï¸ æ³¨æ„äº‹é¡¹**

- APIè°ƒç”¨æˆæœ¬: éœ€è¦è€ƒè™‘GPT-5 APIè°ƒç”¨è´¹ç”¨ï¼ˆå¯é€šè¿‡æ‰¹é‡è°ƒç”¨å’Œç¼“å­˜ä¼˜åŒ–ï¼‰
- è°ƒç”¨å»¶è¿Ÿ: RLè®­ç»ƒä¸­éœ€è¦å¤§é‡è¯„åˆ†ï¼Œå»ºè®®ä½¿ç”¨å¼‚æ­¥æ‰¹é‡è°ƒç”¨
- ç¨³å®šæ€§: è®¾ç½®ä½temperatureï¼ˆ0.1ï¼‰ä¿è¯è¯„åˆ†çš„ç¨³å®šæ€§å’Œä¸€è‡´æ€§

#### GPT-5è¯„åˆ†å™¨å®ç°

ä½¿ç”¨GPT-5 APIä½œä¸ºè¯„åˆ†æ¨¡å‹ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„promptè¿›è¡Œæ‰“åˆ†ï¼š

```python
# gpt5_scorer.py

import os
import json
from openai import OpenAI
from typing import Dict, List, Optional
import asyncio
from tenacity import retry, stop_after_attempt, wait_exponential

class GPT5QueryRewriteScorer:
    """åŸºäºGPT-5çš„Queryæ”¹å†™è¯„åˆ†å™¨"""
  
    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "gpt-5",
        temperature: float = 0.1,  # ä½æ¸©åº¦ä¿è¯è¯„åˆ†ç¨³å®šæ€§
        max_tokens: int = 500
    ):
        self.client = OpenAI(api_key=api_key or os.getenv("OPENAI_API_KEY"))
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
  
        # è¯„åˆ†ç»´åº¦å®šä¹‰
        self.dimensions = [
            "æ”¹å†™è´¨é‡",
            "é¢†åŸŸé€‚é…",
            "æ„å›¾ä¿æŒ",
            "å¯æ£€ç´¢æ€§"
        ]
  
    def build_scoring_prompt(
        self,
        original_query: str,
        rewritten_query: str,
        context: Optional[Dict] = None
    ) -> str:
        """æ„å»ºè¯„åˆ†prompt"""
  
        prompt = f"""# Queryæ”¹å†™è´¨é‡è¯„ä¼°ä»»åŠ¡

ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Queryæ”¹å†™è´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œéœ€è¦å¯¹ç”µå•†é¢†åŸŸçš„Queryæ”¹å†™ç»“æœè¿›è¡Œå¤šç»´åº¦è¯„åˆ†ã€‚

## åŸå§‹Query
{original_query}

## æ”¹å†™Query
{rewritten_query}"""

        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if context:
            if context.get("user_profile"):
                prompt += f"\n\n## ç”¨æˆ·ç”»åƒ\n{context['user_profile']}"
            if context.get("history_summary"):
                prompt += f"\n\n## å†å²æ‘˜è¦\n{context['history_summary']}"

        prompt += """

## è¯„åˆ†ç»´åº¦è¯´æ˜

è¯·ä»ä»¥ä¸‹4ä¸ªç»´åº¦å¯¹æ”¹å†™è´¨é‡è¿›è¡Œè¯„åˆ†ï¼ˆæ¯ä¸ªç»´åº¦1-5åˆ†ï¼‰ï¼š

### 1. æ”¹å†™è´¨é‡ (1-5åˆ†)
- 5åˆ†: æ”¹å†™åçš„queryæå¤§æå‡äº†è¡¨è¾¾çš„æ¸…æ™°åº¦å’Œä¸“ä¸šæ€§
- 4åˆ†: æ”¹å†™åçš„queryæ˜æ˜¾ä¼˜äºåŸqueryï¼Œè¡¨è¾¾æ›´æ¸…æ™°
- 3åˆ†: æ”¹å†™åçš„queryæœ‰æ‰€æ”¹è¿›ï¼Œä½†æå‡æœ‰é™
- 2åˆ†: æ”¹å†™åçš„queryä¸åŸqueryå·®å¼‚ä¸å¤§
- 1åˆ†: æ”¹å†™åçš„queryè´¨é‡ä¸‹é™æˆ–åç¦»åŸæ„

### 2. é¢†åŸŸé€‚é… (1-5åˆ†)
- 5åˆ†: å®Œç¾èå…¥é¢†åŸŸæœ¯è¯­å’Œä¸“ä¸šè¡¨è¾¾ï¼Œéå¸¸ç¬¦åˆç”µå•†åœºæ™¯
- 4åˆ†: è¾ƒå¥½åœ°ä½¿ç”¨äº†é¢†åŸŸç›¸å…³è¯æ±‡
- 3åˆ†: åŸºæœ¬ç¬¦åˆé¢†åŸŸç‰¹ç‚¹
- 2åˆ†: é¢†åŸŸé€‚é…ä¸è¶³
- 1åˆ†: å®Œå…¨ä¸ç¬¦åˆé¢†åŸŸç‰¹ç‚¹

### 3. æ„å›¾ä¿æŒ (1-5åˆ†)
- 5åˆ†: å®Œç¾ä¿ç•™äº†ç”¨æˆ·çš„åŸå§‹æ„å›¾ï¼Œä¸”è¡¨è¾¾æ›´æ¸…æ™°
- 4åˆ†: å¾ˆå¥½åœ°ä¿ç•™äº†ç”¨æˆ·æ„å›¾
- 3åˆ†: åŸºæœ¬ä¿ç•™äº†ç”¨æˆ·æ„å›¾
- 2åˆ†: éƒ¨åˆ†åç¦»äº†ç”¨æˆ·æ„å›¾
- 1åˆ†: ä¸¥é‡åç¦»æˆ–å®Œå…¨æ”¹å˜äº†ç”¨æˆ·æ„å›¾

### 4. å¯æ£€ç´¢æ€§ (1-5åˆ†)
- 5åˆ†: æ”¹å†™åçš„queryæå¤§æå‡äº†æ£€ç´¢ç›¸å…³æ–‡æ¡£çš„èƒ½åŠ›
- 4åˆ†: æ”¹å†™åçš„queryæ˜æ˜¾æ›´æ˜“æ£€ç´¢åˆ°ç›¸å…³å†…å®¹
- 3åˆ†: æ”¹å†™åçš„queryæ£€ç´¢æ€§æœ‰æ‰€æå‡
- 2åˆ†: æ”¹å†™åçš„queryæ£€ç´¢æ€§æå‡ä¸æ˜æ˜¾
- 1åˆ†: æ”¹å†™åçš„queryåè€Œé™ä½äº†æ£€ç´¢æ•ˆæœ

## è¾“å‡ºæ ¼å¼è¦æ±‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºè¯„åˆ†ç»“æœï¼š

```json
{
  "æ”¹å†™è´¨é‡": <1-5çš„æ•´æ•°>,
  "é¢†åŸŸé€‚é…": <1-5çš„æ•´æ•°>,
  "æ„å›¾ä¿æŒ": <1-5çš„æ•´æ•°>,
  "å¯æ£€ç´¢æ€§": <1-5çš„æ•´æ•°>,
  "ç»¼åˆå¾—åˆ†": <å››ä¸ªç»´åº¦çš„å¹³å‡åˆ†ï¼Œä¿ç•™2ä½å°æ•°>,
  "è¯„åˆ†ç†ç”±": "<ç®€è¦è¯´æ˜è¯„åˆ†ä¾æ®ï¼Œ50-100å­—>"
}
```

```python
è¯·å¼€å§‹è¯„åˆ†ï¼š"""        return prompt    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    def score_rewrite(
        self,
        original_query: str,
        rewritten_query: str,
        context: Optional[Dict] = None
    ) -> Dict[str, float]:
        """å¯¹æ”¹å†™ç»“æœè¿›è¡Œè¯„åˆ†        Returns:
            {
                "æ”¹å†™è´¨é‡": 4.5,
                "é¢†åŸŸé€‚é…": 4.0,
                "æ„å›¾ä¿æŒ": 5.0,
                "å¯æ£€ç´¢æ€§": 4.5,
                "ç»¼åˆå¾—åˆ†": 4.5,
                "è¯„åˆ†ç†ç”±": "æ”¹å†™æ•ˆæœå¾ˆå¥½..."
            }
        """        prompt = self.build_scoring_prompt(original_query, rewritten_query, context)        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Queryæ”¹å†™è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¿›è¡Œè¯„åˆ†ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                response_format={"type": "json_object"}  # ç¡®ä¿è¿”å›JSONæ ¼å¼
            )            result_text = response.choices[0].message.content
            scores = json.loads(result_text)            # éªŒè¯å’Œå½’ä¸€åŒ–å¾—åˆ†
            scores = self._validate_scores(scores)            return scores        except Exception as e:
            print(f"GPT-5è¯„åˆ†å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ä¸­ç­‰åˆ†æ•°
            return {
                "æ”¹å†™è´¨é‡": 3.0,
                "é¢†åŸŸé€‚é…": 3.0,
                "æ„å›¾ä¿æŒ": 3.0,
                "å¯æ£€ç´¢æ€§": 3.0,
                "ç»¼åˆå¾—åˆ†": 3.0,
                "è¯„åˆ†ç†ç”±": f"è¯„åˆ†å¤±è´¥: {str(e)}"
            }    def _validate_scores(self, scores: Dict) -> Dict:
        """éªŒè¯å’Œå½’ä¸€åŒ–è¯„åˆ†ç»“æœ"""        # ç¡®ä¿æ‰€æœ‰ç»´åº¦éƒ½å­˜åœ¨
        for dim in self.dimensions:
            if dim not in scores:
                scores[dim] = 3.0
            else:
                # ç¡®ä¿åˆ†æ•°åœ¨1-5èŒƒå›´å†…
                scores[dim] = max(1.0, min(5.0, float(scores[dim])))        # é‡æ–°è®¡ç®—ç»¼åˆå¾—åˆ†
        scores["ç»¼åˆå¾—åˆ†"] = sum(scores[dim] for dim in self.dimensions) / len(self.dimensions)
        scores["ç»¼åˆå¾—åˆ†"] = round(scores["ç»¼åˆå¾—åˆ†"], 2)        # ç¡®ä¿æœ‰è¯„åˆ†ç†ç”±
        if "è¯„åˆ†ç†ç”±" not in scores:
            scores["è¯„åˆ†ç†ç”±"] = "åŸºäºå¤šç»´åº¦ç»¼åˆè¯„ä¼°"        return scores    async def batch_score(
        self,
        query_pairs: List[Dict],
        max_concurrent: int = 5
    ) -> List[Dict]:
        """æ‰¹é‡è¯„åˆ†ï¼ˆå¼‚æ­¥ï¼‰        Args:
            query_pairs: [
                {
                    "original_query": "...",
                    "rewritten_query": "...",
                    "context": {...}
                }
            ]
        """        semaphore = asyncio.Semaphore(max_concurrent)        async def score_one(pair):
            async with semaphore:
                # è½¬æ¢ä¸ºåŒæ­¥è°ƒç”¨ï¼ˆåœ¨å®é™…ä½¿ç”¨ä¸­å¯ä»¥ä½¿ç”¨å¼‚æ­¥HTTPåº“ï¼‰
                return await asyncio.to_thread(
                    self.score_rewrite,
                    pair["original_query"],
                    pair["rewritten_query"],
                    pair.get("context")
                )        tasks = [score_one(pair) for pair in query_pairs]
        results = await asyncio.gather(*tasks)  
return results
```

# ä½¿ç”¨ç¤ºä¾‹

if __name__ == "__main__":
    scorer = GPT5QueryRewriteScorer()

    # å•ä¸ªè¯„åˆ†
    result = scorer.score_rewrite(
        original_query="èƒ¶åŸè›‹ç™½æ€ä¹ˆåƒ",
        rewritten_query="èƒ¶åŸè›‹ç™½è‚½ æœç”¨æ–¹æ³• æ¨èç”¨é‡ é€‚ç”¨äººç¾¤",
        context={
            "user_profile": "25-35å²å¥³æ€§ï¼Œå…³æ³¨æŠ—è¡°è€",
            "history_summary": "è¿‘æœŸå’¨è¯¢è¿‡å¤šæ¬¡èƒ¶åŸè›‹ç™½äº§å“"
        }
    )
    print(json.dumps(result, ensure_ascii=False, indent=2))

### 3.2 Rewardå‡½æ•°è®¾è®¡

è¿™æ˜¯RLè®­ç»ƒçš„æ ¸å¿ƒï¼š

```python

# rl_reward_function.py

import numpy as np
from typing import Dict, List, Tuple
from gpt5_scorer import GPT5QueryRewriteScorer

class MultiDimensionalReward:
    """å¤šç»´åº¦å¥–åŠ±å‡½æ•°"""
  
    def __init__(self, gpt5_api_key: str = None):
        # åˆå§‹åŒ–GPT-5è¯„åˆ†å™¨
        self.gpt5_scorer = GPT5QueryRewriteScorer(api_key=gpt5_api_key)
  
        # æƒé‡é…ç½®
        self.weights = {
            "gpt5_score": 0.4,           # GPT-5è¯„åˆ†æƒé‡
            "retrieval_quality": 0.35,   # æ£€ç´¢è´¨é‡æƒé‡
            "relative_improvement": 0.25 # ç›¸å¯¹32Bçš„æå‡æƒé‡
        }
  
    def compute_reward(
        self,
        original_query: str,
        qwen8b_rewrite: str,
        qwen32b_rewrite: str,
        context: Dict,
        retrieval_results: Dict
    ) -> float:
        """
        è®¡ç®—ç»¼åˆå¥–åŠ±
  
        Returns:
            reward: float, èŒƒå›´ [-1, 1]
        """
  
        # 1. GPT-5è¯„åˆ†å¥–åŠ±
        gpt5_reward = self._compute_gpt5_reward(
            original_query,
            qwen8b_rewrite,
            qwen32b_rewrite,
            context
        )
  
        # 2. æ£€ç´¢è´¨é‡å¥–åŠ±
        retrieval_reward = self._compute_retrieval_reward(
            retrieval_results
        )
  
        # 3. ç›¸å¯¹æå‡å¥–åŠ± (8B vs 32B)
        improvement_reward = self._compute_improvement_reward(
            gpt5_reward,
            retrieval_reward,
            baseline="qwen32b"
        )
  
        # åŠ æƒæ±‚å’Œ
        total_reward = (
            self.weights["gpt5_score"] * gpt5_reward +
            self.weights["retrieval_quality"] * retrieval_reward +
            self.weights["relative_improvement"] * improvement_reward
        )
  
        # å½’ä¸€åŒ–åˆ°[-1, 1]
        total_reward = np.clip(total_reward, -1.0, 1.0)
  
        return total_reward
  
    def _compute_gpt5_reward(
        self,
        original: str,
        rewrite_8b: str,
        rewrite_32b: str,
        context: Dict = None
    ) -> float:
        """GPT-5è¯„åˆ†å¥–åŠ±"""
  
        # è¯„åˆ†8Bçš„æ”¹å†™
        score_8b_dict = self.gpt5_scorer.score_rewrite(
            original,
            rewrite_8b,
            context
        )
        score_8b = score_8b_dict["ç»¼åˆå¾—åˆ†"] / 5.0  # å½’ä¸€åŒ–åˆ°[0, 1]
  
        # è¯„åˆ†32Bçš„æ”¹å†™
        score_32b_dict = self.gpt5_scorer.score_rewrite(
            original,
            rewrite_32b,
            context
        )
        score_32b = score_32b_dict["ç»¼åˆå¾—åˆ†"] / 5.0  # å½’ä¸€åŒ–åˆ°[0, 1]
  
        # è®¡ç®—ç›¸å¯¹å¥–åŠ±
        # å¦‚æœ8B > 32Bï¼Œç»™æ­£å¥–åŠ±
        # å¦‚æœ8B < 32Bï¼Œç»™è´Ÿå¥–åŠ±
        # ä½¿ç”¨tanhå‡½æ•°å¹³æ»‘
        diff = score_8b - score_32b
        reward = np.tanh(diff * 2)  # æ”¾å¤§å·®å¼‚
  
        return reward
  
    def _compute_retrieval_reward(self, retrieval_results: Dict) -> float:
        """æ£€ç´¢è´¨é‡å¥–åŠ±
  
        åŸºäºæ£€ç´¢ç»“æœçš„ç›¸å…³æ€§åˆ†æ•°è®¡ç®—
        """
  
        results_8b = retrieval_results.get("qwen_8b_results", [])
        results_32b = retrieval_results.get("qwen_32b_results", [])
  
        # è®¡ç®—8Bå’Œ32Bçš„æ£€ç´¢è´¨é‡
        quality_8b = self._calculate_retrieval_quality(results_8b)
        quality_32b = self._calculate_retrieval_quality(results_32b)
  
        # ç›¸å¯¹å¥–åŠ±
        diff = quality_8b - quality_32b
        reward = np.tanh(diff * 3)  # æ£€ç´¢è´¨é‡å·®å¼‚æ›´é‡è¦
  
        return reward
  
    def _calculate_retrieval_quality(self, results: List[Dict]) -> float:
        """è®¡ç®—æ£€ç´¢è´¨é‡åˆ†æ•°"""
  
        if not results:
            return 0.0
  
        # è€ƒè™‘å¤šä¸ªå› ç´ 
        # 1. Top-1åˆ†æ•°
        top1_score = results[0].get("reranker_score", results[0].get("score", 0))
  
        # 2. Top-3å¹³å‡åˆ†æ•°
        top3_scores = [r.get("reranker_score", r.get("score", 0)) for r in results[:3]]
        avg_top3 = np.mean(top3_scores) if top3_scores else 0
  
        # 3. åˆ†æ•°è¡°å‡ (æ£€æŸ¥ç»“æœçš„è´¨é‡åˆ†å¸ƒ)
        if len(results) >= 2:
            score_gap = results[0].get("reranker_score", 0) - results[1].get("reranker_score", 0)
            gap_reward = np.clip(score_gap, 0, 0.2) * 2  # å½’ä¸€åŒ–åˆ°[0, 0.4]
        else:
            gap_reward = 0
  
        # ç»¼åˆè´¨é‡åˆ†æ•°
        quality = 0.5 * top1_score + 0.3 * avg_top3 + 0.2 * gap_reward
  
        return quality
  
    def _compute_improvement_reward(
        self,
        gpt5_reward: float,
        retrieval_reward: float,
        baseline: str = "qwen32b"
    ) -> float:
        """ç›¸å¯¹æå‡å¥–åŠ±
  
        é¼“åŠ±8Bæ¨¡å‹è¶…è¶Š32B baseline
        """
  
        # ç»¼åˆæ”¹è¿›åº¦
        improvement = (gpt5_reward + retrieval_reward) / 2
  
        # å¦‚æœæœ‰æ˜¾è‘—æå‡ï¼Œç»™é¢å¤–å¥–åŠ±
        if improvement > 0.3:
            bonus = 0.5  # æ˜¾è‘—è¶…è¶Š32B
        elif improvement > 0.1:
            bonus = 0.2  # ç•¥å¾®è¶…è¶Š32B
        elif improvement > -0.1:
            bonus = 0.0  # æŒå¹³
        else:
            bonus = -0.3  # æ˜æ˜¾ä¸å¦‚32Bï¼Œæƒ©ç½š
  
        return bonus


class AdaptiveRewardShaping:
    """è‡ªé€‚åº”å¥–åŠ±å¡‘å½¢
  
    æ ¹æ®è®­ç»ƒè¿›åº¦åŠ¨æ€è°ƒæ•´å¥–åŠ±
    """
  
    def __init__(self):
        self.training_step = 0
        self.reward_history = []
  
    def shape_reward(self, base_reward: float) -> float:
        """å¥–åŠ±å¡‘å½¢"""
  
        self.training_step += 1
        self.reward_history.append(base_reward)
  
        # æ—©æœŸè®­ç»ƒï¼šæ”¾å¤§å¥–åŠ±ï¼Œé¼“åŠ±æ¢ç´¢
        if self.training_step < 1000:
            shaped_reward = base_reward * 1.5
  
        # ä¸­æœŸè®­ç»ƒï¼šæ­£å¸¸å¥–åŠ±
        elif self.training_step < 5000:
            shaped_reward = base_reward
  
        # åæœŸè®­ç»ƒï¼šæ›´ç²¾ç»†çš„å¥–åŠ±
        else:
            # è®¡ç®—æœ€è¿‘100æ­¥çš„å¹³å‡å¥–åŠ±
            recent_avg = np.mean(self.reward_history[-100:])
  
            # å¦‚æœå½“å‰å¥–åŠ±æ˜¾è‘—é«˜äºå¹³å‡ï¼Œæ”¾å¤§å¥–åŠ±
            if base_reward > recent_avg + 0.2:
                shaped_reward = base_reward * 1.3
            else:
                shaped_reward = base_reward
  
        return np.clip(shaped_reward, -1.0, 1.0)
```

#### 3.3 ä»Rewardåˆ°å‚æ•°æ›´æ–°ï¼šPPOç®—æ³•è¯¦è§£

è¿™æ˜¯RLè®­ç»ƒçš„æ ¸å¿ƒï¼è®©æˆ‘è¯¦ç»†è§£é‡ŠGPT-5è¯„åˆ†ç”Ÿæˆçš„rewardå¦‚ä½•æ›´æ–°Qwen-8Bçš„å‚æ•°ã€‚

#### 3.3.1 PPOç®—æ³•åŸç†

**åŸºæœ¬æµç¨‹**ï¼š

```
1. æ”¶é›†è½¨è¿¹(Trajectory)
   - å½“å‰Qwen-8Bç”Ÿæˆæ”¹å†™
   - è°ƒç”¨RAGè·å–æ£€ç´¢ç»“æœ
   - GPT-5è¯„åˆ† â†’ è®¡ç®—reward

2. è®¡ç®—ä¼˜åŠ¿å‡½æ•°(Advantage)
   - ä¼°è®¡çŠ¶æ€ä»·å€¼ V(s)
   - è®¡ç®— Advantage = Reward - V(s)

3. ç­–ç•¥æ¢¯åº¦æ›´æ–°
   - è®¡ç®—ç­–ç•¥æ¯”ç‡ ratio = Ï€_new / Ï€_old
   - è®¡ç®— PPO lossï¼ˆå¸¦clipï¼‰
   - åå‘ä¼ æ’­æ›´æ–°å‚æ•°

4. ä»·å€¼å‡½æ•°æ›´æ–°
   - æ›´æ–° V(s) ä½¿å…¶æ›´å‡†ç¡®ä¼°è®¡æœªæ¥å›æŠ¥
```

#### 3.3.2 è¯¦ç»†æ•°å­¦æ¨å¯¼

**Step 1: æ”¶é›†ç»éªŒ**

å¯¹äºæ¯ä¸ªqueryï¼Œæˆ‘ä»¬æ”¶é›†ä¸€ä¸ªå®Œæ•´çš„trajectoryï¼š

```python
trajectory = {
    "state": original_query,              # çŠ¶æ€ï¼ˆåŸå§‹queryï¼‰
    "action": qwen8b_rewrite,            # åŠ¨ä½œï¼ˆ8Bç”Ÿæˆçš„æ”¹å†™ï¼‰
    "reward": reward_from_gpt5_and_rag,  # å¥–åŠ±ï¼ˆGPT-5è¯„åˆ†+æ£€ç´¢æ•ˆæœï¼‰
    "log_prob": log_prob_of_action,      # å½“å‰ç­–ç•¥ä¸‹åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
}
```

**Step 2: è®¡ç®—ä¼˜åŠ¿å‡½æ•°(Advantage)**

ä¼˜åŠ¿å‡½æ•°å‘Šè¯‰æˆ‘ä»¬ï¼š**è¿™ä¸ªåŠ¨ä½œæ¯”å¹³å‡æ°´å¹³å¥½å¤šå°‘**

```python
# ä»·å€¼å‡½æ•°ä¼°è®¡ï¼šè¿™ä¸ªçŠ¶æ€ä¸‹æœŸæœ›çš„ç´¯ç§¯å›æŠ¥
V(state) = critic_model(state)  # ä½¿ç”¨criticç½‘ç»œä¼°è®¡

# ä¼˜åŠ¿å‡½æ•°ï¼šå®é™…reward - æœŸæœ›reward
Advantage = Reward - V(state)

# å¦‚æœ Advantage > 0ï¼šè¿™ä¸ªåŠ¨ä½œæ¯”æœŸæœ›å¥½ â†’ å¢åŠ è¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡
# å¦‚æœ Advantage < 0ï¼šè¿™ä¸ªåŠ¨ä½œæ¯”æœŸæœ›å·® â†’ é™ä½è¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡
```

**Step 3: è®¡ç®—ç­–ç•¥æ¯”ç‡**

PPOçš„æ ¸å¿ƒï¼šæ¯”è¾ƒæ–°æ—§ç­–ç•¥

```python
# æ—§ç­–ç•¥ï¼šå½“å‰çš„Qwen-8B
log_prob_old = log P_old(qwen8b_rewrite | original_query)

# æ–°ç­–ç•¥ï¼šæ›´æ–°ä¸€æ­¥åçš„Qwen-8B  
log_prob_new = log P_new(qwen8b_rewrite | original_query)

# ç­–ç•¥æ¯”ç‡
ratio = exp(log_prob_new - log_prob_old) = P_new / P_old
```

**Step 4: PPOæŸå¤±å‡½æ•°**

```python
# åŸºç¡€ç­–ç•¥æ¢¯åº¦
surrogate_loss = ratio * Advantage

# PPO clipï¼šé˜²æ­¢æ›´æ–°å¤ªæ¿€è¿›
clipped_ratio = clip(ratio, 1-Îµ, 1+Îµ)  # Îµé€šå¸¸ä¸º0.2
clipped_loss = clipped_ratio * Advantage

# æœ€ç»ˆlossï¼šå–ä¸¤è€…æœ€å°å€¼ï¼ˆä¿å®ˆæ›´æ–°ï¼‰
policy_loss = -min(surrogate_loss, clipped_loss)

# ä¸ºä»€ä¹ˆæ˜¯è´Ÿå·ï¼Ÿå› ä¸ºæˆ‘ä»¬è¦æœ€å¤§åŒ–rewardï¼Œä½†ä¼˜åŒ–å™¨æ˜¯minimize loss
```

**Step 5: ä»·å€¼å‡½æ•°æŸå¤±**

```python
# Criticç½‘ç»œè¦å‡†ç¡®é¢„æµ‹å›æŠ¥
value_loss = (Reward - V(state))^2
```

**Step 6: æ€»æŸå¤±**

```python
total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy_bonus

# entropy_bonus: é¼“åŠ±æ¢ç´¢ï¼Œé¿å…ç­–ç•¥è¿‡æ—©æ”¶æ•›
```

#### 3.3.3 å…·ä½“ç¤ºä¾‹

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªtraining episodeï¼š

```python
# åŸå§‹query
original_query = "èƒ¶åŸè›‹ç™½æ€ä¹ˆåƒ"

# Qwen-8Bç”Ÿæˆæ”¹å†™
qwen8b_rewrite = "èƒ¶åŸè›‹ç™½è‚½ æœç”¨æ–¹æ³• æ¨èç”¨é‡"

# GPT-5è¯„åˆ† + æ£€ç´¢æ•ˆæœ â†’ reward
reward = 0.15  # æ­£å¥–åŠ±ï¼Œè¯´æ˜8Bè¡¨ç°ä¸é”™ï¼ˆè¶…è¿‡32B baselineï¼‰

# å½“å‰ç­–ç•¥ä¸‹ï¼Œè¿™ä¸ªæ”¹å†™çš„logæ¦‚ç‡
log_prob_old = -2.3  # å¯¹æ•°æ¦‚ç‡ï¼ˆè´Ÿæ•°ï¼‰

# --- PPOæ›´æ–°è¿‡ç¨‹ ---

# 1. Criticä¼°è®¡çŠ¶æ€ä»·å€¼
V_state = 0.1  # Criticè®¤ä¸ºè¿™ä¸ªqueryå¹³å‡èƒ½å¾—0.1çš„reward

# 2. è®¡ç®—Advantage
Advantage = reward - V_state = 0.15 - 0.1 = 0.05  # æ­£æ•°ï¼æ¯”æœŸæœ›å¥½

# 3. æ›´æ–°åçš„ç­–ç•¥
log_prob_new = -2.2  # æ›´æ–°åï¼Œè¿™ä¸ªæ”¹å†™çš„æ¦‚ç‡å¢åŠ äº†

# 4. è®¡ç®—ratio
ratio = exp(-2.2 - (-2.3)) = exp(0.1) = 1.105

# 5. PPO loss
surrogate1 = 1.105 * 0.05 = 0.055
clipped_ratio = min(max(1.105, 0.8), 1.2) = 1.105
surrogate2 = 1.105 * 0.05 = 0.055
policy_loss = -min(0.055, 0.055) = -0.055  # è´Ÿæ•° â†’ æ¢¯åº¦ä¸Šå‡ â†’ å¢åŠ æ¦‚ç‡

# 6. åå‘ä¼ æ’­æ›´æ–°å‚æ•°
# ç»“æœï¼šä¸‹æ¬¡é‡åˆ°ç±»ä¼¼queryï¼Œæ›´å¯èƒ½ç”Ÿæˆç±»ä¼¼çš„å¥½æ”¹å†™
```

å¦‚æœrewardæ˜¯è´Ÿæ•°ï¼ˆ8Bè¡¨ç°å·®äº32Bï¼‰ï¼š

```python
reward = -0.2  # è´Ÿå¥–åŠ±
Advantage = -0.2 - 0.1 = -0.3  # è´Ÿæ•°ï¼æ¯”æœŸæœ›å·®

# PPOä¼šé™ä½è¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡
# ä¸‹æ¬¡é‡åˆ°ç±»ä¼¼queryï¼Œä¼šå°è¯•ç”Ÿæˆä¸åŒçš„æ”¹å†™
```

#### 3.3.4 å‚æ•°æ›´æ–°æµç¨‹ï¼ˆä¼ªä»£ç ï¼‰

```python
class PPOTrainer:
    def __init__(self):
        self.actor = Qwen8B_Model()       # ç­–ç•¥ç½‘ç»œï¼ˆç”Ÿæˆæ”¹å†™ï¼‰
        self.critic = Value_Network()     # ä»·å€¼ç½‘ç»œï¼ˆä¼°è®¡V(s)ï¼‰
        self.optimizer_actor = Adam(self.actor.parameters(), lr=1e-6)
        self.optimizer_critic = Adam(self.critic.parameters(), lr=1e-5)
  
    def update(self, trajectories):
        """ä½¿ç”¨æ”¶é›†çš„trajectoriesæ›´æ–°æ¨¡å‹"""
    
        # 1. æå–æ•°æ®
        states = [t["state"] for t in trajectories]
        actions = [t["action"] for t in trajectories]
        rewards = [t["reward"] for t in trajectories]
        old_log_probs = [t["log_prob"] for t in trajectories]
    
        # 2. è®¡ç®—ä¼˜åŠ¿å‡½æ•°
        with torch.no_grad():
            values = self.critic(states)  # V(s)
            advantages = rewards - values  # Advantage
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
    
        # 3. PPOæ›´æ–°ï¼ˆå¤šä¸ªepochï¼‰
        for epoch in range(4):
            # 3.1 å‰å‘ä¼ æ’­
            new_log_probs = self.actor.get_log_prob(states, actions)
            new_values = self.critic(states)
        
            # 3.2 è®¡ç®—æ¯”ç‡
            ratio = torch.exp(new_log_probs - old_log_probs)
        
            # 3.3 è®¡ç®—PPO loss
            surrogate1 = ratio * advantages
            surrogate2 = torch.clamp(ratio, 0.8, 1.2) * advantages
            policy_loss = -torch.min(surrogate1, surrogate2).mean()
        
            # 3.4 è®¡ç®—value loss
            value_loss = 0.5 * (rewards - new_values).pow(2).mean()
        
            # 3.5 è®¡ç®—entropy
            entropy = self.actor.get_entropy(states)
            entropy_loss = -0.01 * entropy.mean()
        
            # 3.6 æ€»æŸå¤±
            total_loss = policy_loss + value_loss + entropy_loss
        
            # 4. åå‘ä¼ æ’­
            self.optimizer_actor.zero_grad()
            self.optimizer_critic.zero_grad()
            total_loss.backward()
        
            # 5. æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
        
            # 6. æ›´æ–°å‚æ•°
            self.optimizer_actor.step()
            self.optimizer_critic.step()
    
        return {
            "policy_loss": policy_loss.item(),
            "value_loss": value_loss.item()
        }
```

#### 3.3.5 å…³é”®è¶…å‚æ•°

| å‚æ•°                 | å€¼   | è¯´æ˜                           |
| -------------------- | ---- | ------------------------------ |
| learning_rate        | 1e-6 | Actorå­¦ä¹ ç‡ï¼ˆ8Bæ¯”32Bæ›´éœ€è°¨æ…ï¼‰ |
| critic_learning_rate | 1e-5 | Criticå­¦ä¹ ç‡                   |
| ppo_epochs           | 4    | æ¯æ‰¹æ•°æ®æ›´æ–°æ¬¡æ•°               |
| clip_range           | 0.2  | PPO clipèŒƒå›´                   |
| batch_size           | 8-16 | æ¯æ‰¹æ ·æœ¬æ•°                     |
| max_grad_norm        | 0.5  | æ¢¯åº¦è£å‰ªé˜ˆå€¼                   |

#### 3.3.6 è®­ç»ƒç›‘æ§è¦ç‚¹

```python
# å¥åº·çš„è®­ç»ƒåº”è¯¥çœ‹åˆ°ï¼š
wandb.log({
    "avg_reward": 0.0 â†’ 0.1 â†’ 0.2,         # é€æ­¥æå‡
    "policy_loss": -0.15 â†’ -0.10,          # é€æ¸å‡å°ï¼ˆç»å¯¹å€¼ï¼‰
    "value_loss": 0.4 â†’ 0.25 â†’ 0.15,       # é€æ¸å‡å°
    "8b_win_rate": 0.3 â†’ 0.5 â†’ 0.65,       # èƒœç‡æå‡
    "clip_fraction": 0.1-0.3                # 10-30%çš„æ ·æœ¬è¢«clipï¼ˆæ­£å¸¸ï¼‰
})

# âš ï¸ å¼‚å¸¸æƒ…å†µï¼š
# - rewardä¸‹é™ï¼šå¯èƒ½å­¦ä¹ ç‡å¤ªå¤§
# - clip_fraction > 0.5ï¼šæ›´æ–°å¤ªæ¿€è¿›ï¼Œé™ä½å­¦ä¹ ç‡
# - value_lossä¸é™ï¼šCriticè®­ç»ƒæœ‰é—®é¢˜
```

### 3.4 RLè®­ç»ƒä¸»æµç¨‹

```python
# train_rl.py

import asyncio
import torch
from typing import Dict, List
from transformers import AutoModelForCausalLM, AutoTokenizer
import wandb

# å¯¼å…¥VERLæ¡†æ¶ (å‚è€ƒDeepRetrieval)
from verl.trainer import PPOTrainer
from verl.utils.reward_score import RewardFunction

class QueryRewriteRLTrainer:
    """Queryæ”¹å†™RLè®­ç»ƒå™¨"""
  
    def __init__(
        self,
        qwen8b_model_path: str,  # SFTè®­ç»ƒåçš„8Bæ¨¡å‹
        qwen32b_api_url: str,     # 32Bæ¨¡å‹API
        tenant_id: str = "fivedoctors"
    ):
        self.tenant_id = tenant_id
  
        # åŠ è½½8Bæ¨¡å‹ (policy model)
        self.policy_model = AutoModelForCausalLM.from_pretrained(
            qwen8b_model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        self.tokenizer = AutoTokenizer.from_pretrained(qwen8b_model_path)
  
        # 32Bæ¨¡å‹API
        self.qwen32b_api = qwen32b_api_url
  
        # å¥–åŠ±å‡½æ•°
        self.reward_function = MultiDimensionalReward()
        self.reward_shaper = AdaptiveRewardShaping()
  
        # åˆå§‹åŒ–wandb
        wandb.init(
            project="sales-rag-query-rewrite",
            name=f"rl_{tenant_id}",
            config={
                "tenant_id": tenant_id,
                "task": "query_rewrite_rl",
                "algorithm": "PPO"
            }
        )
  
    async def generate_training_episode(
        self,
        original_query: str,
        context: Dict
    ) -> Dict:
        """ç”Ÿæˆä¸€ä¸ªè®­ç»ƒepisode
  
        Returns:
            {
                "original_query": str,
                "qwen8b_rewrite": str,
                "qwen32b_rewrite": str,
                "retrieval_results": {...},
                "reward": float
            }
        """
  
        # 1. 8Bæ¨¡å‹ç”Ÿæˆæ”¹å†™
        qwen8b_rewrite = await self._generate_rewrite_8b(
            original_query, context
        )
  
        # 2. 32Bæ¨¡å‹ç”Ÿæˆæ”¹å†™ (ä½œä¸ºbaseline)
        qwen32b_rewrite = await self._generate_rewrite_32b(
            original_query, context
        )
  
        # 3. å¹¶è¡Œæ£€ç´¢ä¸¤ä¸ªæ”¹å†™çš„ç»“æœï¼ˆå®æ—¶è°ƒç”¨RAG APIï¼‰
        retrieval_results = await self._parallel_retrieval(
            rewrite_8b=qwen8b_rewrite,
            rewrite_32b=qwen32b_rewrite,
            original_query=original_query,
            context=context
        )
  
        # 4. è®¡ç®—å¥–åŠ±
        base_reward = self.reward_function.compute_reward(
            original_query=original_query,
            qwen8b_rewrite=qwen8b_rewrite,
            qwen32b_rewrite=qwen32b_rewrite,
            context=context,
            retrieval_results=retrieval_results
        )
  
        # 5. å¥–åŠ±å¡‘å½¢
        shaped_reward = self.reward_shaper.shape_reward(base_reward)
  
        return {
            "original_query": original_query,
            "context": context,
            "qwen8b_rewrite": qwen8b_rewrite,
            "qwen32b_rewrite": qwen32b_rewrite,
            "retrieval_results": retrieval_results,
            "base_reward": base_reward,
            "shaped_reward": shaped_reward
        }
  
    async def _generate_rewrite_8b(
        self,
        query: str,
        context: Dict
    ) -> str:
        """ä½¿ç”¨8Bæ¨¡å‹ç”Ÿæˆæ”¹å†™"""
  
        # æ„å»ºè¾“å…¥
        prompt = self._build_prompt(query, context)
  
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            max_length=1024,
            truncation=True
        ).to(self.policy_model.device)
  
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.policy_model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.7,
                top_p=0.9,
                do_sample=True
            )
  
        rewrite = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
  
        return rewrite.strip()
  
    async def _generate_rewrite_32b(
        self,
        query: str,
        context: Dict
    ) -> str:
        """è°ƒç”¨32Bæ¨¡å‹APIç”Ÿæˆæ”¹å†™"""
  
        import httpx
  
        # è°ƒç”¨ç°æœ‰çš„rewrite_query_by_modelé€»è¾‘
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.post(
                f"{self.qwen32b_api}/api/rewrite_query",
                json={
                    "query": query,
                    "history": context.get("history_context", ""),
                    "thought": context.get("thought_unit", ""),
                    "tenant_id": self.tenant_id,
                    "user_profile": context.get("user_profile", ""),
                    "history_summary": context.get("history_summary", "")
                }
            )
    
            result = response.json()
            return result.get("rewritten_query", query)
  
    async def _parallel_retrieval(
        self,
        rewrite_8b: str,
        rewrite_32b: str,
        original_query: str = None,
        context: Dict = None
    ) -> Dict:
        """å¹¶è¡Œæ£€ç´¢ä¸¤ä¸ªæ”¹å†™çš„ç»“æœ
  
        ğŸ”¥ å®æ—¶è°ƒç”¨RAGæ¡†æ¶APIè¿›è¡Œæ£€ç´¢
        """
  
        import httpx
  
        # å¹¶è¡Œè°ƒç”¨RAG API
        results_8b_task = asyncio.create_task(
            self._call_rag_api(rewrite_8b, original_query, context)
        )
        results_32b_task = asyncio.create_task(
            self._call_rag_api(rewrite_32b, original_query, context)
        )
  
        results_8b = await results_8b_task
        results_32b = await results_32b_task
  
        return {
            "qwen_8b_results": results_8b,
            "qwen_32b_results": results_32b
        }
  
    async def _call_rag_api(
        self,
        rewritten_query: str,
        original_query: str = None,
        context: Dict = None
    ) -> List[Dict]:
        """è°ƒç”¨RAGæ¡†æ¶APIè¿›è¡Œå®æ—¶æ£€ç´¢
  
        ç›´æ¥è°ƒç”¨general_ragè·¯ç”±ï¼Œè·å–çœŸå®çš„æ£€ç´¢ç»“æœ
        """
  
        import httpx
  
        payload = {
            "query": original_query or rewritten_query,  # åŸå§‹query
            "tenant_id": self.tenant_id,
            "kb_name": "default",
            "history": context.get("history_context", "") if context else "",
            "top_k": 5,
            "score_threshold": 0.5,
            # å…³é”®ï¼šç›´æ¥ä¼ å…¥æ”¹å†™åçš„queryç”¨äºæ£€ç´¢
            "rewritten_query": rewritten_query
        }
  
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    "http://localhost:8000/api/chat/general_rag",
                    json=payload
                )
                response.raise_for_status()
                result = response.json()
          
                # æå–æ£€ç´¢ç»“æœ
                data = result.get("data", {})
                recall_results = data.get("recall", [])
          
                return recall_results
  
        except Exception as e:
            print(f"âš ï¸  RAG APIè°ƒç”¨å¤±è´¥: {e}")
            return []
  
    def train_with_ppo(
        self,
        train_data_path: str,
        num_epochs: int = 10,
        batch_size: int = 8,
        learning_rate: float = 1e-6
    ):
        """ä½¿ç”¨PPOç®—æ³•è®­ç»ƒ"""
  
        # PPOè®­ç»ƒé…ç½®
        ppo_config = {
            "learning_rate": learning_rate,
            "batch_size": batch_size,
            "num_epochs": num_epochs,
            "clip_range": 0.2,
            "vf_coef": 0.1,
            "ent_coef": 0.01,
            "gamma": 0.99,
            "lambda_": 0.95,
        }
  
        # åˆ›å»ºPPO Trainer (ä½¿ç”¨VERLæ¡†æ¶)
        from verl.trainer.ppo import PPOTrainer as VERLPPOTrainer
  
        trainer = VERLPPOTrainer(
            model=self.policy_model,
            tokenizer=self.tokenizer,
            reward_fn=self.reward_function,
            config=ppo_config,
            output_dir=f"outputs/rl/{self.tenant_id}"
        )
  
        # åŠ è½½è®­ç»ƒæ•°æ®
        import json
        train_queries = []
        with open(train_data_path, "r", encoding="utf-8") as f:
            for line in f:
                train_queries.append(json.loads(line))
  
        print(f"åŠ è½½äº† {len(train_queries)} æ¡è®­ç»ƒæ•°æ®")
  
        # è®­ç»ƒå¾ªç¯
        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch + 1}/{num_epochs}")
    
            epoch_rewards = []
    
            for batch_idx in range(0, len(train_queries), batch_size):
                batch = train_queries[batch_idx:batch_idx + batch_size]
        
                # ç”Ÿæˆepisodes
                episodes = []
                for item in batch:
                    episode = asyncio.run(
                        self.generate_training_episode(
                            original_query=item["original_query"],
                            context=item.get("context", {})
                        )
                    )
                    episodes.append(episode)
        
                # PPOæ›´æ–°
                metrics = trainer.step(episodes)
        
                # è®°å½•å¥–åŠ±
                batch_reward = np.mean([ep["shaped_reward"] for ep in episodes])
                epoch_rewards.append(batch_reward)
        
                # æ—¥å¿—
                if batch_idx % 10 == 0:
                    print(f"  Batch {batch_idx}/{len(train_queries)}, "
                          f"Avg Reward: {batch_reward:.4f}, "
                          f"Policy Loss: {metrics['policy_loss']:.4f}")
            
                    wandb.log({
                        "epoch": epoch,
                        "batch": batch_idx,
                        "avg_reward": batch_reward,
                        "policy_loss": metrics["policy_loss"],
                        "value_loss": metrics["value_loss"]
                    })
    
            # Epochæ€»ç»“
            avg_epoch_reward = np.mean(epoch_rewards)
            print(f"Epoch {epoch + 1} å¹³å‡å¥–åŠ±: {avg_epoch_reward:.4f}")
    
            # ä¿å­˜checkpoint
            if (epoch + 1) % 2 == 0:
                checkpoint_path = f"outputs/rl/{self.tenant_id}/checkpoint_epoch{epoch + 1}"
                self.policy_model.save_pretrained(checkpoint_path)
                self.tokenizer.save_pretrained(checkpoint_path)
                print(f"Checkpointå·²ä¿å­˜: {checkpoint_path}")
  
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = f"outputs/rl/{self.tenant_id}/final"
        self.policy_model.save_pretrained(final_path)
        self.tokenizer.save_pretrained(final_path)
        print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_path}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    trainer = QueryRewriteRLTrainer(
        qwen8b_model_path="outputs/sft/fivedoctors/final",
        qwen32b_api_url="http://localhost:7861",
        tenant_id="fivedoctors"
    )
  
    trainer.train_with_ppo(
        train_data_path="data/query_rewrite_training/sft/fivedoctors/train_latest.jsonl",
        num_epochs=10,
        batch_size=8,
        learning_rate=1e-6
    )
```

### 3.5 RLè®­ç»ƒå¯åŠ¨è„šæœ¬

```bash
#!/bin/bash
# scripts/train_rl.sh

export CUDA_VISIBLE_DEVICES=0,1,2,3
export WANDB_PROJECT="sales-rag-query-rewrite"

TENANT_ID="fivedoctors"
SFT_MODEL_PATH="outputs/sft/${TENANT_ID}/final"
QWEN32B_API="http://localhost:7861"
OUTPUT_DIR="outputs/rl/${TENANT_ID}"

echo "=========================================="
echo "RL Training - ${TENANT_ID}"
echo "=========================================="

# å¯åŠ¨RLè®­ç»ƒ
python train_rl.py \
    --qwen8b_model_path ${SFT_MODEL_PATH} \
    --qwen32b_api_url ${QWEN32B_API} \
    --tenant_id ${TENANT_ID} \
    --num_epochs 10 \
    --batch_size 8 \
    --learning_rate 1e-6

echo "RLè®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: ${OUTPUT_DIR}/final"
```

---

## 4ï¸âƒ£ å®Œæ•´è®­ç»ƒæµç¨‹æ€»ç»“

```
Step 1: æ•°æ®æ”¶é›† (1å¤©)
â”œâ”€ æ‰¹é‡æµ‹è¯•RAGæ¡†æ¶ï¼Œä¿å­˜åˆ°test_sft.xlsx
â”œâ”€ è´¨é‡ç­›é€‰ï¼ˆtop1_score > 0.6ï¼‰
â””â”€ è½¬æ¢ä¸ºJSONLè®­ç»ƒæ ¼å¼

Step 2: SFTè®­ç»ƒ (2-3å¤©)
â”œâ”€ Qwen-8B SFTè®­ç»ƒï¼ˆå­¦ä¹ 32Bæ”¹å†™èƒ½åŠ›ï¼‰
â””â”€ éªŒè¯å’Œéƒ¨ç½²SFTæ¨¡å‹

Step 3: RLè®­ç»ƒ (5-7å¤©)
â”œâ”€ åŒæ¨¡å‹éƒ¨ç½²ï¼ˆ8B + 32Bï¼‰
â”œâ”€ PPOè®­ç»ƒå¾ªç¯ï¼ˆå®æ—¶RAG + GPT-5è¯„åˆ†ï¼‰
â”œâ”€ ç›‘æ§å¥–åŠ±æ›²çº¿å’Œèƒœç‡
â””â”€ æ¨¡å‹æ€§èƒ½è¯„ä¼°

Step 4: ä¸Šçº¿éªŒè¯ (1-2å¤©)
â”œâ”€ A/Bæµ‹è¯•éƒ¨ç½²
â””â”€ ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§
```

**æ€»æ—¶é—´**: çº¦9-13å¤©

---

## 5ï¸âƒ£ å®æ—¶RLè®­ç»ƒä¸RAGé›†æˆ

### 5.1 æ ¸å¿ƒæ€è·¯

åœ¨RLè®­ç»ƒä¸­ï¼Œ8Bæ¨¡å‹æ¯æ­¥ç”Ÿæˆæ–°çš„æ”¹å†™query â†’ éœ€è¦**å®æ—¶è°ƒç”¨RAG API**è·å–æ£€ç´¢æ•ˆæœ â†’ è®¡ç®—rewardæ›´æ–°æ¨¡å‹ã€‚

**å…³é”®æµç¨‹**ï¼š

```
è®­ç»ƒæ ·æœ¬ â†’ 8B/32Bå¹¶è¡Œç”Ÿæˆæ”¹å†™ â†’ å®æ—¶è°ƒç”¨RAGæ£€ç´¢ â†’ 
æ¯”è¾ƒæ£€ç´¢æ•ˆæœ â†’ è®¡ç®—reward â†’ PPOæ›´æ–°8Bå‚æ•°
```

### 5.2 RAG APIä¿®æ”¹

åœ¨ `general_rag_routes.py` ä¸­æ·»åŠ å‚æ•°æ”¯æŒå¤–éƒ¨æ”¹å†™ï¼š

```python
@router.post("/api/chat/general_rag")
async def general_rag_endpoint(
    query: str,
    tenant_id: str,
    rewritten_query: Optional[str] = None,  # ğŸ†• æ–°å¢å‚æ•°
    ...
):
    # å¦‚æœæä¾›æ”¹å†™queryï¼Œç›´æ¥ä½¿ç”¨
    if rewritten_query:
        new_query = rewritten_query
    else:
        new_query = await rewrite_query_by_model(...)
  
    # åç»­æ£€ç´¢æµç¨‹ä¸å˜
    search_res = await rag_workflow(new_query, ...)
    return {"data": {"rewritten_query": new_query, "recall": search_res}}
```

### 5.3 æ€§èƒ½ä¼˜åŒ–è¦ç‚¹

**1. æ‰¹é‡å¹¶å‘å¤„ç†**

- æ¯æ‰¹32ä¸ªæ ·æœ¬ï¼Œå¹¶è¡Œè°ƒç”¨RAG API
- è®¾ç½® `max_concurrent_requests=10`

**2. ç»“æœç¼“å­˜**

- ç¼“å­˜æ£€ç´¢ç»“æœï¼Œé¢„æœŸå‘½ä¸­ç‡20-30%
- å‡å°‘30%çš„é‡å¤APIè°ƒç”¨

**3. è¶…æ—¶é™çº§**

- 30ç§’è¶…æ—¶é™åˆ¶
- å¤±è´¥æ—¶ä½¿ç”¨ç¼“å­˜æˆ–è·³è¿‡æ ·æœ¬

### 5.4 è®­ç»ƒç›‘æ§

```python
wandb.log({
    "avg_reward": avg_reward,
    "8b_win_rate": wins / total,  # å…³é”®æŒ‡æ ‡
    "8b_avg_top1": avg_8b_top1,
    "32b_avg_top1": avg_32b_top1,
    "rag_api_calls": total_calls,
    "cache_hit_rate": hits / total_calls
})
```

### 5.5 å¿«é€Ÿå¯åŠ¨

```bash
# ç»ˆç«¯1: å¯åŠ¨RAGæœåŠ¡
cd sales-rag && python startup.py -a

# ç»ˆç«¯2: å¯åŠ¨RLè®­ç»ƒ
python train_rl.py \
    --qwen8b_model_path outputs/sft/fivedoctors/final \
    --qwen32b_api_url http://localhost:7861 \
    --rag_api_url http://localhost:8000/api/chat/general_rag \
    --max_concurrent_rag_calls 10
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

| æŒ‡æ ‡            | Baseline (32B)         | SFT (8B) | RL (8B) |
| --------------- | ---------------------- | -------- | ------- |
| æ”¹å†™è´¨é‡è¯„åˆ†    | 4.2/5                  | 3.8/5    | 4.5/5   |
| æ£€ç´¢Top-1å‡†ç¡®ç‡ | 78%                    | 72%      | 85%     |
| æ¨ç†å»¶è¿Ÿ        | 850ms                  | 320ms    | 350ms   |
| æˆæœ¬/1000æ¬¡     | $2.50          | $0.80 | $0.85    |         |

**æ ¸å¿ƒç›®æ ‡**ï¼šé€šè¿‡RLè®­ç»ƒï¼Œ8Bæ¨¡å‹åœ¨ä¿æŒä½æˆæœ¬ï¼ˆé™ä½70%ï¼‰çš„åŒæ—¶ï¼Œæ£€ç´¢æ•ˆæœè¶…è¶Š32B baselineï¼ˆ85% vs 78%ï¼‰ï¼

---

## ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹

1. **GPT-5è¯„åˆ†é©±åŠ¨**ï¼šæ— éœ€ä¸“é—¨è®­ç»ƒè¯„åˆ†æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨GPT-5 APIè¯„ä¼°æ”¹å†™è´¨é‡
2. **å®æ—¶RAGåé¦ˆ**ï¼šRLè®­ç»ƒä¸­å®æ—¶è°ƒç”¨RAGç³»ç»Ÿï¼ŒåŸºäºçœŸå®æ£€ç´¢æ•ˆæœä¼˜åŒ–
3. **PPOç¨³å®šæ›´æ–°**ï¼šé€šè¿‡clipæœºåˆ¶å’Œadvantageå‡½æ•°ï¼Œç¡®ä¿è®­ç»ƒç¨³å®šæ”¶æ•›
4. **åŒæ¨¡å‹ç«äº‰**ï¼š8BæŒç»­ä¸32B baselineç«äº‰ï¼Œè‡ªåŠ¨å­¦ä¹ è¶…è¶Šç­–ç•¥
5. **å¤šç»´åº¦å¥–åŠ±**ï¼šç»¼åˆGPT-5è¯„åˆ†ã€æ£€ç´¢è´¨é‡ã€ç›¸å¯¹æå‡ä¸‰ä¸ªç»´åº¦è®¡ç®—reward
