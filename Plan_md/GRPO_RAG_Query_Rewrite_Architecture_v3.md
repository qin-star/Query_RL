# SalesRAG Queryæ”¹å†™GRPO+GPT-5æ··åˆæ¶æ„è®¾è®¡ v4.0

## ğŸ“‹ æ¦‚è¿°

åŸºäºGRPOç»„å†…ç«äº‰æœºåˆ¶ï¼Œç»“åˆGPT-5è´¨é‡è¯„ä¼°ï¼Œè®¾è®¡**GRPOé€‰æ‹©+GPT-5è¯„åˆ†çš„æ··åˆè®­ç»ƒæ¶æ„**ã€‚æœ¬æ–¹æ¡ˆåˆ©ç”¨GRPOçš„ç»„å†…ä¼˜åŒ–èƒ½åŠ›ç­›é€‰æœ€ä¼˜å€™é€‰ï¼Œå†é€šè¿‡GPT-5è¯„ä¼°ç»å¯¹è´¨é‡ï¼Œå®ç°é«˜æ•ˆã€ç¨³å®šçš„queryæ”¹å†™å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚

### æ ¸å¿ƒåˆ›æ–°ç‚¹

1. **GRPOç»„å†…ç«äº‰**: Actoræ¨¡å‹ç”Ÿæˆ5ä¸ªå€™é€‰æ”¹å†™ï¼Œé€šè¿‡log-probå’Œè¯­ä¹‰è´¨é‡é€‰å‡ºæœ€ä¼˜
2. **GPT-5ä¸»å¥–åŠ±**: å¯¹æœ€ä¼˜å€™é€‰è¿›è¡ŒåŒæ¨¡å‹å¯¹æ¯”è¯„åˆ†ï¼Œæä¾›ç»å¯¹è´¨é‡ä¿¡å·
3. **æ··åˆå¥–åŠ±æœºåˆ¶**: GPT-5ä¸»å¥–åŠ±(80-90%) + GRPOè¾…åŠ©å¥–åŠ±(10-20%)
4. **é«˜æ•ˆè®­ç»ƒ**: ä»…å¯¹ç»„å†…æœ€ä¼˜è¾“å‡ºè¿›è¡Œå¤–éƒ¨è¯„åˆ†ï¼Œå¤§å¹…é™ä½GPT-5è°ƒç”¨æˆæœ¬

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ„è®¾è®¡
æ··åˆGRPOæ¶æ„ v4.0:
â”œâ”€â”€ verl_code/verl/workers/grpo_selector.py                 # GRPOç»„å†…é€‰æ‹©å™¨ï¼ˆæ–°å¢ï¼‰
â”œâ”€â”€ verl_code/verl/workers/gpt5_quality_evaluator.py        # GPT-5è´¨é‡è¯„ä¼°å™¨ï¼ˆé‡æ„ï¼‰
â”œâ”€â”€ verl_code/verl/workers/hybrid_reward_combiner.py        # æ··åˆå¥–åŠ±åˆæˆå™¨ï¼ˆæ–°å¢ï¼‰
â”œâ”€â”€ verl_code/verl/trainer/main_ppo.py                      # ä¸»è®­ç»ƒå…¥å£ï¼ˆä¿®æ”¹ï¼‰
â”œâ”€â”€ verl_code/verl/trainer/config/ppo_trainer.yaml          # æ‰©å±•é…ç½®æ–‡ä»¶
â”œâ”€â”€ scripts/run_grpo_gpt5_hybrid.sh                         # è®­ç»ƒå¯åŠ¨è„šæœ¬
â””â”€â”€ deprecated/v3_dual_model_backup/                        # v3ç‰ˆæœ¬å¤‡ä»½


### 1. ç³»ç»Ÿæ¶æ„å›¾

```mermaid
graph TB
    A[Excelæ•°æ®æ–‡ä»¶] --> B[æå–å†å²ä¼ å‚ä¸Šä¸‹æ–‡]
    B --> C[ç»„åˆquery_rewrite_prompt ç”Ÿæˆå®Œæ•´Prompt]

    C --> D[Actoræ¨¡å‹ Qwen-8B ç”Ÿæˆ5ä¸ªå€™é€‰æ”¹å†™]
    D --> E[GRPOç»„å†…æ‰“åˆ†]
    E --> F[é€‰æ‹©ç»„å†…æœ€ä¼˜è¾“å‡º]

    F --> G[ç”Ÿæˆç»“æ„åŒ–è¾“å‡º]
    G --> H[è°ƒç”¨RAG /chat_8bæ¥å£]
    H --> I[ä¼ å…¥ user_profileã€rewritten_queryã€history_summary]
    I --> J[RAGæ£€ç´¢å¤„ç†]
    J --> K[ç»„åˆ8Bæœ€ç»ˆå“åº”]

    C --> L[å‚è€ƒæ¨¡å‹ Qwen-32B]
    L --> M[è°ƒç”¨RAG /chatæ¥å£]
    M --> N[32Bå†…éƒ¨å®Œæˆæ”¹å†™]
    N --> O[RAGæ£€ç´¢å¹¶ç”Ÿæˆ32Bå®Œæ•´è¾“å‡º]

    K --> P[8Bä¸32Bè¾“å‡ºå¯¹æ¯”]
    O --> P

    P --> Q[GPT-5è¯„åˆ†ç³»ç»Ÿ ç”Ÿæˆä¸»å¥–åŠ±]
    E --> R[GRPOè¾…åŠ©å¥–åŠ±]

    Q --> S[ä¸»å¥–åŠ±]
    R --> T[è¾…åŠ©å¥–åŠ±]

    S --> U[å¥–åŠ±èåˆ]
    T --> U

    U --> V[PPO/GRPOå‚æ•°æ›´æ–°]
    V --> W[è®­ç»ƒå¾ªç¯]
```

### 2. æ ¸å¿ƒç»„ä»¶è¯´æ˜

#### 2.1 GRPOç»„å†…é€‰æ‹©å™¨
- **åŠŸèƒ½**: ä»5ä¸ªå€™é€‰æ”¹å†™ä¸­é€‰æ‹©æœ€ä¼˜è¾“å‡º
- **é€‰æ‹©ä¾æ®**: 
  - log-probï¼ˆç”Ÿæˆæ¦‚ç‡ï¼‰
  - è¯­ä¹‰æµç•…åº¦
  - è¯­æ³•è‡ªç„¶åº¦
  - åˆæ­¥æ£€ç´¢æ•ˆæœï¼ˆå¯é€‰ï¼‰
- **è¾“å‡º**: ç»„å†…æœ€ä¼˜çš„1ä¸ªæ”¹å†™

#### 2.2 GPT-5è´¨é‡è¯„ä¼°å™¨ï¼ˆä¸»å¥–åŠ±ï¼‰
- **è¾“å…¥**: GRPOé€‰å‡ºçš„æœ€ä¼˜æ”¹å†™ + 32Bå‚è€ƒæ”¹å†™
- **è¯„ä¼°ç»´åº¦**: 
  - è´¨é‡æå‡åº¦ï¼ˆ40%ï¼‰
  - ç›¸å…³æ€§å‡†ç¡®æ€§ï¼ˆ20%ï¼‰
  - ä¿¡æ¯å®Œæ•´æ€§ï¼ˆ20%ï¼‰
  - æ£€ç´¢æœ‰æ•ˆæ€§ï¼ˆ20%ï¼‰
- **è¾“å‡º**: ç»å¯¹è´¨é‡åˆ†æ•° [0, 1]
- **æƒé‡**: 0.8-0.9ï¼ˆä¸»å¥–åŠ±ï¼‰

#### 2.3 æ··åˆå¥–åŠ±åˆæˆå™¨
- **ä¸»å¥–åŠ±**: GPT-5è¯„åˆ†ï¼ˆæƒé‡0.8-0.9ï¼‰
- **è¾…åŠ©å¥–åŠ±**: GRPOç»„å†…ä¼˜åŠ¿ï¼ˆæƒé‡0.1-0.2ï¼‰
- **åˆæˆç­–ç•¥**: åŠ æƒæ±‚å’Œ
- **æœ€ç»ˆè¾“å‡º**: ç”¨äºPPOæ›´æ–°çš„å¥–åŠ±ä¿¡å·

#### 2.4 RAGæ¥å£é€‚é…å™¨
- **8Bæ¥å£**: `/rag/chat_8b`
  - è¾“å…¥ï¼š`user_profile`, `rewritten_query`, `history_summary`
  - ç‰¹ç‚¹ï¼šæ¥æ”¶å¤–éƒ¨æ”¹å†™çš„queryï¼Œç›´æ¥è¿›è¡Œæ£€ç´¢
- **32Bæ¥å£**: `/rag/chat`
  - è¾“å…¥ï¼šåŸå§‹ä¸Šä¸‹æ–‡
  - ç‰¹ç‚¹ï¼š32Bæ¨¡å‹å†…éƒ¨å®Œæˆqueryæ”¹å†™ï¼Œç„¶åæ£€ç´¢
- **ä½œç”¨**: ç¡®ä¿8Bå’Œ32Bä½¿ç”¨ç›¸åŒçš„RAGæ£€ç´¢é€»è¾‘è¿›è¡Œå…¬å¹³å¯¹æ¯”

## ğŸ”§ æŠ€æœ¯å®ç°ç»†èŠ‚

### 3.1 GRPOç»„å†…é€‰æ‹©

#### 3.1.1 å€™é€‰ç”Ÿæˆä¸æ‰“åˆ†
```python
def generate_and_score_candidates(
    prompt: str, 
    actor_model, 
    group_size: int = 5
) -> Tuple[List[str], List[float]]:
    """
    ç”Ÿæˆå¤šä¸ªå€™é€‰æ”¹å†™å¹¶è®¡ç®—GRPOåˆ†æ•°
    """
    candidates = []
    log_probs = []
    
    # ç”Ÿæˆå¤šä¸ªå€™é€‰ï¼ˆä¸åŒæ¸©åº¦ä¿è¯å¤šæ ·æ€§ï¼‰
    temperatures = [0.6, 0.7, 0.8, 0.9, 1.0]
    
    for temp in temperatures:
        output = actor_model.generate(
            prompt,
            temperature=temp,
            return_log_prob=True,  # è¿”å›logæ¦‚ç‡
            max_new_tokens=512
        )
        candidates.append(output['text'])
        log_probs.append(output['log_prob'])
    
    # GRPOç»„å†…æ‰“åˆ†ï¼šåŸºäºlog_prob
    grpo_scores = compute_grpo_scores(log_probs)
    
    return candidates, grpo_scores

def compute_grpo_scores(log_probs: List[float]) -> List[float]:
    """
    è®¡ç®—GRPOç»„å†…ç›¸å¯¹åˆ†æ•°
    """
    # ç»„å†…æ ‡å‡†åŒ–
    mean_lp = np.mean(log_probs)
    std_lp = np.std(log_probs)
    
    if std_lp < 1e-8:
        return [0.0] * len(log_probs)
    
    # æ ‡å‡†åŒ–åˆ†æ•°
    grpo_scores = [(lp - mean_lp) / std_lp for lp in log_probs]
    
    return grpo_scores
```

#### 3.1.2 é€‰æ‹©æœ€ä¼˜å€™é€‰
```python
def select_best_candidate(
    candidates: List[str],
    grpo_scores: List[float]
) -> Tuple[str, float]:
    """
    é€‰æ‹©GRPOåˆ†æ•°æœ€é«˜çš„å€™é€‰
    """
    best_idx = np.argmax(grpo_scores)
    best_candidate = candidates[best_idx]
    best_score = grpo_scores[best_idx]
    
    return best_candidate, best_score
```

### 3.2 GPT-5è´¨é‡è¯„ä¼°ï¼ˆä¸»å¥–åŠ±ï¼‰

#### 3.2.1 åŒæ¨¡å‹å¯¹æ¯”è¯„åˆ†
```python
async def evaluate_with_gpt5(
    best_candidate: str,
    reference_output: str,
    rag_results: Dict
) -> float:
    """
    ä½¿ç”¨GPT-5è¯„ä¼°æœ€ä¼˜å€™é€‰çš„è´¨é‡
    
    Returns:
        float: è´¨é‡åˆ†æ•° [0, 1]
    """
    # æ„å»ºè¯„ä¼°prompt
    eval_prompt = build_comparison_prompt(
        actor_output=best_candidate,
        reference_output=reference_output,
        rag_results=rag_results
    )
    
    # è°ƒç”¨GPT-5è¯„åˆ†
    gpt5_result = await gpt5_rater.rate(eval_prompt)
    
    # æå–è´¨é‡åˆ†æ•°
    quality_score = extract_quality_score(gpt5_result)
    
    return quality_score

def extract_quality_score(gpt5_result: Dict) -> float:
    """
    ä»GPT-5ç»“æœä¸­æå–è´¨é‡åˆ†æ•°
    
    Returns:
        float: [0, 1]èŒƒå›´çš„åˆ†æ•°
            - 0.9-1.0: 8Bæ˜æ˜¾ä¼˜äº32B
            - 0.7-0.9: 8Bç•¥ä¼˜äº32B
            - 0.5-0.7: ç›¸å½“
            - 0.3-0.5: 32Bç•¥ä¼˜äº8B
            - 0.0-0.3: 32Bæ˜æ˜¾ä¼˜äº8B
    """
    better = gpt5_result.get('better', 'same')
    scores_8b = gpt5_result.get('score', {}).get('8b', {}).get('sum', 20)
    scores_32b = gpt5_result.get('score', {}).get('32b', {}).get('sum', 20)
    
    # è®¡ç®—ç›¸å¯¹è´¨é‡
    if better == '8b':
        # 8Bæ›´å¥½ï¼Œæ˜ å°„åˆ°[0.7, 1.0]
        quality = 0.7 + (scores_8b / 40.0) * 0.3
    elif better == '32b':
        # 32Bæ›´å¥½ï¼Œæ˜ å°„åˆ°[0.0, 0.5]
        quality = 0.5 - (scores_32b / 40.0) * 0.5
    else:
        # ç›¸å½“ï¼Œæ˜ å°„åˆ°[0.5, 0.7]
        quality = 0.5 + (scores_8b / 40.0) * 0.2
    
    return np.clip(quality, 0.0, 1.0)
```

### 3.3 æ··åˆå¥–åŠ±åˆæˆ

#### 3.3.1 å¥–åŠ±èåˆ
```python
def compute_final_reward(
    gpt5_quality: float,
    grpo_advantage: float,
    gpt5_weight: float = 0.85,
    grpo_weight: float = 0.15
) -> float:
    """
    åˆæˆæœ€ç»ˆè®­ç»ƒå¥–åŠ±
    
    Args:
        gpt5_quality: GPT-5è´¨é‡è¯„åˆ† [0, 1]
        grpo_advantage: GRPOç»„å†…ä¼˜åŠ¿ (æ ‡å‡†åŒ–å)
        gpt5_weight: GPT-5æƒé‡ (æ¨è0.8-0.9)
        grpo_weight: GRPOæƒé‡ (æ¨è0.1-0.2)
    
    Returns:
        float: æœ€ç»ˆå¥–åŠ±ä¿¡å·
    """
    # å°†GPT-5åˆ†æ•°æ˜ å°„åˆ°[-1, 1]èŒƒå›´ï¼ˆä»¥0.5ä¸ºä¸­å¿ƒï¼‰
    gpt5_reward = (gpt5_quality - 0.5) * 2.0
    
    # åŠ æƒèåˆ
    final_reward = gpt5_weight * gpt5_reward + grpo_weight * grpo_advantage
    
    return final_reward
```

### 3.4 å®Œæ•´è®­ç»ƒæµç¨‹

#### 3.4.1 å•æ­¥è®­ç»ƒæµç¨‹
```python
class HybridGRPOGPT5Trainer:
    async def train_step(self, batch: Dict) -> Dict:
        """
        å•æ­¥è®­ç»ƒæµç¨‹
        
        æµç¨‹:
        1. Actorç”Ÿæˆ5ä¸ªå€™é€‰æ”¹å†™
        2. GRPOç»„å†…æ‰“åˆ†ï¼Œé€‰æ‹©æœ€ä¼˜
        3. RAGæ£€ç´¢
        4. GPT-5è¯„ä¼°æœ€ä¼˜å€™é€‰
        5. åˆæˆæ··åˆå¥–åŠ±
        6. PPOå‚æ•°æ›´æ–°
        """
        all_rewards = []
        all_best_candidates = []
        
        for prompt_data in batch:
            # æ­¥éª¤1: ç”Ÿæˆå€™é€‰å¹¶GRPOæ‰“åˆ†
            candidates, grpo_scores = generate_and_score_candidates(
                prompt=prompt_data['prompt'],
                actor_model=self.actor_model,
                group_size=5
            )
            
            # æ­¥éª¤2: é€‰æ‹©æœ€ä¼˜å€™é€‰
            best_candidate, grpo_advantage = select_best_candidate(
                candidates, grpo_scores
            )
            
            # æ­¥éª¤3: RAGæ£€ç´¢
            rag_results = await self.rag_retriever.retrieve(
                query=best_candidate
            )
            
            # æ­¥éª¤4: è·å–32Bå‚è€ƒè¾“å‡º
            reference_output = await self.get_reference_output(
                prompt=prompt_data['prompt']
            )
            
            # æ­¥éª¤5: GPT-5è¯„ä¼°
            gpt5_quality = await evaluate_with_gpt5(
                best_candidate=best_candidate,
                reference_output=reference_output,
                rag_results=rag_results
            )
            
            # æ­¥éª¤6: åˆæˆæœ€ç»ˆå¥–åŠ±
            final_reward = compute_final_reward(
                gpt5_quality=gpt5_quality,
                grpo_advantage=grpo_advantage,
                gpt5_weight=0.85,
                grpo_weight=0.15
            )
            
            all_rewards.append(final_reward)
            all_best_candidates.append(best_candidate)
        
        # æ­¥éª¤7: PPOå‚æ•°æ›´æ–°
        loss = self.ppo_update(
            candidates=all_best_candidates,
            rewards=all_rewards
        )
        
        return {
            'rewards': all_rewards,
            'gpt5_quality': [r['gpt5'] for r in all_rewards],
            'grpo_advantages': [r['grpo'] for r in all_rewards],
            'loss': loss
        }
```

#### 3.4.2 æ‰¹é‡å¹¶è¡Œå¤„ç†
```python
async def batch_train_step(self, batch: List[Dict]) -> Dict:
    """
    æ‰¹é‡å¹¶è¡Œå¤„ç†ï¼Œæé«˜æ•ˆç‡
    """
    # å¹¶è¡Œç”Ÿæˆæ‰€æœ‰å€™é€‰
    generation_tasks = [
        generate_and_score_candidates(item['prompt'], self.actor_model)
        for item in batch
    ]
    all_candidates_scores = await asyncio.gather(*generation_tasks)
    
    # é€‰æ‹©æœ€ä¼˜å€™é€‰
    best_candidates = [
        select_best_candidate(cands, scores)
        for cands, scores in all_candidates_scores
    ]
    
    # å¹¶è¡ŒRAGæ£€ç´¢
    rag_tasks = [
        self.rag_retriever.retrieve(cand)
        for cand, _ in best_candidates
    ]
    rag_results = await asyncio.gather(*rag_tasks)
    
    # å¹¶è¡ŒGPT-5è¯„ä¼°ï¼ˆæ‰¹é‡è°ƒç”¨é™ä½æˆæœ¬ï¼‰
    gpt5_tasks = [
        evaluate_with_gpt5(cand, ref, rag)
        for (cand, _), ref, rag in zip(
            best_candidates,
            [item['reference'] for item in batch],
            rag_results
        )
    ]
    gpt5_qualities = await asyncio.gather(*gpt5_tasks)
    
    # åˆæˆå¥–åŠ±
    final_rewards = [
        compute_final_reward(gpt5_q, grpo_adv)
        for gpt5_q, (_, grpo_adv) in zip(gpt5_qualities, best_candidates)
    ]
    
    return final_rewards
```

### 3.3 å…¼å®¹æ€§ä¿è¯

#### 3.3.1 å®˜æ–¹GRPOæ¥å£å…¼å®¹
```python
# ä¿æŒä¸å®˜æ–¹verlçš„GRPOæ¥å£å…¼å®¹
def compute_grpo_outcome_advantage(
    token_level_rewards: torch.Tensor,
    response_mask: torch.Tensor,
    index: torch.Tensor,
    norm_adv_by_std_in_grpo: bool = True
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    ä¸å®˜æ–¹å®ç°å®Œå…¨å…¼å®¹çš„GRPOä¼˜åŠ¿è®¡ç®—
    """
    # ä½¿ç”¨å®˜æ–¹å®ç°ä½œä¸ºåŸºç¡€
    advantages, returns = official_grpo_implementation(
        token_level_rewards, response_mask, index, norm_adv_by_std_in_grpo
    )
    
    # æ·»åŠ è¾…åŠ©å¥–åŠ±ä¿¡å·
    if hasattr(self, 'auxiliary_rewards'):
        advantages = self.apply_auxiliary_rewards(advantages)
    
    return advantages, returns
```

#### 3.5.2 é…ç½®æ–‡ä»¶
```yaml
# GRPO+GPT-5æ··åˆè®­ç»ƒé…ç½®
algorithm:
  adv_estimator: grpo
  norm_adv_by_std_in_grpo: true
  use_kl_in_reward: false
  
  # æ··åˆè®­ç»ƒå‚æ•°
  hybrid_grpo_gpt5:
    enable: true
    
    # GRPOç»„å†…é€‰æ‹©
    group_size: 5  # æ¯ä¸ªpromptç”Ÿæˆ5ä¸ªå€™é€‰
    selection_method: "max_log_prob"  # é€‰æ‹©æ–¹æ³•ï¼šmax_log_prob, weighted_sample
    
    # GPT-5è¯„ä¼°
    gpt5_weight: 0.85  # GPT-5ä¸»å¥–åŠ±æƒé‡ (0.8-0.9)
    grpo_weight: 0.15  # GRPOè¾…åŠ©å¥–åŠ±æƒé‡ (0.1-0.2)
    
    # è¯„ä¼°é…ç½®
    gpt5_model: "GPT-5"
    gpt5_batch_size: 8  # GPT-5æ‰¹é‡è¯„ä¼°å¤§å°
    gpt5_timeout: 10.0  # GPT-5è°ƒç”¨è¶…æ—¶(ç§’)
    
    # RAGæ£€ç´¢
    rag_endpoint: "http://localhost:8000/chat_8b"
    reference_endpoint: "http://localhost:8000/chat"  # 32Bå‚è€ƒæ¨¡å‹
```

---

## ğŸ”¨ ä»£ç ä¿®æ”¹æŒ‡å—

### ä¿®æ”¹1: åˆ›å»ºGRPOç»„å†…é€‰æ‹©å™¨

**æ–°å»ºæ–‡ä»¶**: `verl_code/verl/workers/grpo_selector.py`

```python
"""
GRPOç»„å†…é€‰æ‹©å™¨
åŠŸèƒ½ï¼šä»å¤šä¸ªå€™é€‰æ”¹å†™ä¸­é€‰æ‹©æœ€ä¼˜è¾“å‡º
"""

import numpy as np
from typing import List, Tuple, Dict
import torch
import logging

logger = logging.getLogger(__name__)


class GRPOSelector:
    """GRPOç»„å†…é€‰æ‹©å™¨"""
    
    def __init__(self, selection_method: str = "max_log_prob"):
        """
        Args:
            selection_method: é€‰æ‹©æ–¹æ³•
                - "max_log_prob": é€‰æ‹©log_probæœ€é«˜çš„
                - "weighted_sample": æŒ‰log_probåŠ æƒé‡‡æ ·
        """
        self.selection_method = selection_method
    
    def select_best(
        self,
        candidates: List[str],
        log_probs: List[float]
    ) -> Tuple[str, float, int]:
        """
        ä»å€™é€‰ä¸­é€‰æ‹©æœ€ä¼˜
        
        Args:
            candidates: å€™é€‰æ”¹å†™åˆ—è¡¨
            log_probs: å¯¹åº”çš„logæ¦‚ç‡
            
        Returns:
            (best_candidate, grpo_advantage, best_idx)
        """
        if not candidates or not log_probs:
            raise ValueError("å€™é€‰åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        if len(candidates) != len(log_probs):
            raise ValueError("å€™é€‰æ•°é‡ä¸log_probæ•°é‡ä¸åŒ¹é…")
        
        # è®¡ç®—GRPOç»„å†…ç›¸å¯¹åˆ†æ•°
        grpo_scores = self._compute_grpo_scores(log_probs)
        
        # é€‰æ‹©æœ€ä¼˜
        if self.selection_method == "max_log_prob":
            best_idx = np.argmax(grpo_scores)
        elif self.selection_method == "weighted_sample":
            # æŒ‰åˆ†æ•°åŠ æƒé‡‡æ ·
            weights = np.exp(grpo_scores - np.max(grpo_scores))
            weights = weights / np.sum(weights)
            best_idx = np.random.choice(len(candidates), p=weights)
        else:
            raise ValueError(f"æœªçŸ¥çš„é€‰æ‹©æ–¹æ³•: {self.selection_method}")
        
        best_candidate = candidates[best_idx]
        grpo_advantage = grpo_scores[best_idx]
        
        logger.info(f"GRPOé€‰æ‹©: å€™é€‰{best_idx}, ä¼˜åŠ¿={grpo_advantage:.3f}")
        
        return best_candidate, grpo_advantage, best_idx
    
    def _compute_grpo_scores(self, log_probs: List[float]) -> np.ndarray:
        """
        è®¡ç®—GRPOç»„å†…ç›¸å¯¹åˆ†æ•°ï¼ˆæ ‡å‡†åŒ–ï¼‰
        
        Args:
            log_probs: logæ¦‚ç‡åˆ—è¡¨
            
        Returns:
            æ ‡å‡†åŒ–åçš„GRPOåˆ†æ•°
        """
        log_probs_array = np.array(log_probs, dtype=np.float32)
        
        # ç»„å†…æ ‡å‡†åŒ–
        mean_lp = np.mean(log_probs_array)
        std_lp = np.std(log_probs_array)
        
        if std_lp < 1e-8:
            logger.warning("ç»„å†…log_probæ ‡å‡†å·®è¿‡å°ï¼Œè¿”å›é›¶åˆ†æ•°")
            return np.zeros_like(log_probs_array)
        
        # æ ‡å‡†åŒ–: (x - mean) / std
        grpo_scores = (log_probs_array - mean_lp) / std_lp
        
        return grpo_scores
```

---

### ä¿®æ”¹2: åˆ›å»ºRAGæ¥å£é€‚é…å™¨

**æ–°å»ºæ–‡ä»¶**: `verl_code/verl/workers/rag_adapter.py`

```python
"""
RAGæ¥å£é€‚é…å™¨
åŠŸèƒ½ï¼šé€‚é…8Bå’Œ32Bæ¨¡å‹çš„ä¸åŒRAGè°ƒç”¨æ–¹å¼
"""

import asyncio
from typing import Dict, Tuple
import logging
from src.core.rag_chater import RagChater

logger = logging.getLogger(__name__)


class RAGAdapter:
    """RAGæ¥å£é€‚é…å™¨"""
    
    def __init__(
        self,
        tenant_id: str,
        contact_id: str,
        account_id: str,
        message_id: str
    ):
        """
        åˆå§‹åŒ–RAGé€‚é…å™¨
        
        Args:
            tenant_id: ç§Ÿæˆ·ID
            contact_id: è”ç³»äººID
            account_id: è´¦æˆ·ID
            message_id: æ¶ˆæ¯ID
        """
        self.rag_chater = RagChater(
            tenant_id=tenant_id,
            contact_id=contact_id,
            account_id=account_id,
            message_id=message_id
        )
    
    async def call_8b_rag(
        self,
        rewritten_query: str,
        user_profile: str,
        history_summary: str,
        contexts: list[dict] = None,
        thought_unit: str = "",
        score_threshold: float = 0.95
    ) -> Dict:
        """
        è°ƒç”¨8Bæ¨¡å‹çš„RAGæ¥å£
        
        Args:
            rewritten_query: 8Bæ¨¡å‹æ”¹å†™åçš„query
            user_profile: ç”¨æˆ·ç”»åƒ
            history_summary: å†å²æ‘˜è¦
            contexts: ä¸Šä¸‹æ–‡åˆ—è¡¨
            thought_unit: æ€è€ƒå•å…ƒ
            score_threshold: åˆ†æ•°é˜ˆå€¼
            
        Returns:
            dict: RAGå“åº”ç»“æœ
                {
                    'response': [...],  # RAGæ£€ç´¢ç»“æœ
                    'status': 'success',
                    'cost_time': 0.5,
                    'rewritten_query': '...'
                }
        """
        try:
            response_data, status, request_body, cost_time = await self.rag_chater.chat_8b(
                contexts=contexts,
                thought_unit=thought_unit,
                score_threshold=score_threshold,
                user_profile=user_profile,
                history_summary=history_summary,
                rewritten_query=rewritten_query
            )
            
            logger.info(f"8B RAGè°ƒç”¨æˆåŠŸ: query={rewritten_query[:50]}, cost={cost_time:.2f}s")
            
            return {
                'response': response_data,
                'status': status,
                'cost_time': cost_time,
                'rewritten_query': rewritten_query,
                'request_body': request_body
            }
            
        except Exception as e:
            logger.error(f"8B RAGè°ƒç”¨å¤±è´¥: {e}")
            return {
                'response': [],
                'status': 'error',
                'cost_time': 0.0,
                'rewritten_query': rewritten_query,
                'error': str(e)
            }
    
    async def call_32b_rag(
        self,
        contexts: list[dict] = None,
        context: str = "",
        thought_unit: str = "",
        score_threshold: float = 0.95
    ) -> Dict:
        """
        è°ƒç”¨32Bæ¨¡å‹çš„RAGæ¥å£ï¼ˆ32Bå†…éƒ¨å®Œæˆqueryæ”¹å†™ï¼‰
        
        Args:
            contexts: ä¸Šä¸‹æ–‡åˆ—è¡¨
            context: å•ä¸ªä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
            thought_unit: æ€è€ƒå•å…ƒ
            score_threshold: åˆ†æ•°é˜ˆå€¼
            
        Returns:
            dict: RAGå“åº”ç»“æœ
                {
                    'response': [...],  # RAGæ£€ç´¢ç»“æœ
                    'status': 'success',
                    'cost_time': 0.5
                }
        """
        try:
            response_data, status, request_body, cost_time = await self.rag_chater.chat(
                contexts=contexts,
                context=context,
                thought_unit=thought_unit,
                score_threshold=score_threshold
            )
            
            logger.info(f"32B RAGè°ƒç”¨æˆåŠŸ: cost={cost_time:.2f}s")
            
            return {
                'response': response_data,
                'status': status,
                'cost_time': cost_time,
                'request_body': request_body
            }
            
        except Exception as e:
            logger.error(f"32B RAGè°ƒç”¨å¤±è´¥: {e}")
            return {
                'response': [],
                'status': 'error',
                'cost_time': 0.0,
                'error': str(e)
            }
    
    async def parallel_call_rag(
        self,
        rewritten_query_8b: str,
        user_profile: str,
        history_summary: str,
        contexts: list[dict] = None,
        thought_unit: str = "",
        score_threshold: float = 0.95
    ) -> Tuple[Dict, Dict]:
        """
        å¹¶è¡Œè°ƒç”¨8Bå’Œ32Bçš„RAGæ¥å£
        
        Args:
            rewritten_query_8b: 8Bæ¨¡å‹æ”¹å†™çš„query
            user_profile: ç”¨æˆ·ç”»åƒ
            history_summary: å†å²æ‘˜è¦
            contexts: ä¸Šä¸‹æ–‡åˆ—è¡¨
            thought_unit: æ€è€ƒå•å…ƒ
            score_threshold: åˆ†æ•°é˜ˆå€¼
            
        Returns:
            tuple: (8Bç»“æœ, 32Bç»“æœ)
        """
        # å¹¶è¡Œè°ƒç”¨
        results = await asyncio.gather(
            self.call_8b_rag(
                rewritten_query=rewritten_query_8b,
                user_profile=user_profile,
                history_summary=history_summary,
                contexts=contexts,
                thought_unit=thought_unit,
                score_threshold=score_threshold
            ),
            self.call_32b_rag(
                contexts=contexts,
                thought_unit=thought_unit,
                score_threshold=score_threshold
            ),
            return_exceptions=True
        )
        
        result_8b, result_32b = results
        
        logger.info(
            f"å¹¶è¡ŒRAGè°ƒç”¨å®Œæˆ: "
            f"8BçŠ¶æ€={result_8b.get('status')}, "
            f"32BçŠ¶æ€={result_32b.get('status')}"
        )
        
        return result_8b, result_32b
```

---

### ä¿®æ”¹3: é‡æ„GPT-5è´¨é‡è¯„ä¼°å™¨

**ä¿®æ”¹æ–‡ä»¶**: `verl_code/verl/workers/gpt5_dual_model_rater.py`

**å…³é”®ä¿®æ”¹ç‚¹**:

1. **ç®€åŒ–è¯„åˆ†é€»è¾‘**ï¼ˆç¬¬117-183è¡Œï¼‰:

```python
# åŸä»£ç ï¼ˆå¤æ‚ä¸”ä¸ç¨³å®šï¼‰
def _standardize_scores(self, result: dict) -> dict:
    better_model = result.get('better', 'same')
    
    if better_model == '8b':
        quality_improvement = scores_8b[0] / 10.0
    elif better_model == '32b':
        quality_improvement = max(0.1, scores_8b[0] / 10.0 - 0.2)
    # ...

# ä¿®æ”¹ä¸ºï¼ˆæ¸…æ™°çš„æ˜ å°„ï¼‰
def extract_quality_score(self, result: dict) -> float:
    """
    ä»GPT-5ç»“æœæå–è´¨é‡åˆ†æ•°
    
    Returns:
        float: [0, 1]èŒƒå›´çš„è´¨é‡åˆ†æ•°
            - 0.9-1.0: 8Bæ˜æ˜¾ä¼˜äº32B
            - 0.7-0.9: 8Bç•¥ä¼˜äº32B
            - 0.5-0.7: ç›¸å½“
            - 0.3-0.5: 32Bç•¥ä¼˜äº8B
            - 0.0-0.3: 32Bæ˜æ˜¾ä¼˜äº8B
    """
    better = result.get('better', 'same')
    scores_8b = result.get('score', {}).get('8b', {}).get('sum', 20)
    scores_32b = result.get('score', {}).get('32b', {}).get('sum', 20)
    
    # æ¸…æ™°çš„åˆ†æ®µæ˜ å°„
    if better == '8b':
        # 8Bæ›´å¥½ï¼Œæ˜ å°„åˆ°[0.7, 1.0]
        # scores_8bèŒƒå›´[0, 40]ï¼Œå½’ä¸€åŒ–åæ˜ å°„
        quality = 0.7 + (scores_8b / 40.0) * 0.3
    elif better == '32b':
        # 32Bæ›´å¥½ï¼Œæ˜ å°„åˆ°[0.0, 0.5]
        quality = 0.5 - (scores_32b / 40.0) * 0.5
    else:
        # ç›¸å½“ï¼Œæ˜ å°„åˆ°[0.5, 0.7]
        quality = 0.5 + (scores_8b / 40.0) * 0.2
    
    return np.clip(quality, 0.0, 1.0)
```

2. **ä¿®æ”¹rateæ–¹æ³•è¿”å›å€¼**ï¼ˆç¬¬70-115è¡Œï¼‰:

```python
# åŸä»£ç è¿”å›å¤æ‚å­—å…¸
async def rate(self) -> dict:
    # ...
    return {
        "quality_improvement": 0.5,
        "relevance_accuracy": 0.5,
        # ... å¤šä¸ªå­—æ®µ
    }

# ä¿®æ”¹ä¸ºè¿”å›å•ä¸€è´¨é‡åˆ†æ•°
async def rate(self) -> float:
    """
    æ‰§è¡ŒåŒæ¨¡å‹å¯¹æ¯”è¯„åˆ†
    
    Returns:
        float: è´¨é‡åˆ†æ•° [0, 1]
    """
    eval_payload = self._prepare_eval_payload()
    prompt = g_sa_prompt_manager.render_prompt(
        prompt_name="auto_eval",
        **eval_payload
    )
    
    try:
        raw_response = await asyncio.wait_for(
            self.llm.ainvoke(prompt),
            timeout=10.0
        )
        content = raw_response.content
        result = SafeParser.parse_json_to_dict(content)
        
        # æå–å•ä¸€è´¨é‡åˆ†æ•°
        quality_score = self.extract_quality_score(result)
        
        return quality_score
        
    except Exception as e:
        logger.error(f"GPT-5è¯„åˆ†å¤±è´¥: {e}")
        return 0.5  # å¤±è´¥æ—¶è¿”å›ä¸­æ€§åˆ†æ•°
```

---

### ä¿®æ”¹3: åˆ›å»ºæ··åˆå¥–åŠ±åˆæˆå™¨

**æ–°å»ºæ–‡ä»¶**: `verl_code/verl/workers/hybrid_reward_combiner.py`

```python
"""
æ··åˆå¥–åŠ±åˆæˆå™¨
åŠŸèƒ½ï¼šåˆæˆGPT-5ä¸»å¥–åŠ±å’ŒGRPOè¾…åŠ©å¥–åŠ±
"""

import numpy as np
from typing import List, Dict
import logging

logger = logging.getLogger(__name__)


class HybridRewardCombiner:
    """æ··åˆå¥–åŠ±åˆæˆå™¨"""
    
    def __init__(
        self,
        gpt5_weight: float = 0.85,
        grpo_weight: float = 0.15
    ):
        """
        Args:
            gpt5_weight: GPT-5ä¸»å¥–åŠ±æƒé‡ (æ¨è0.8-0.9)
            grpo_weight: GRPOè¾…åŠ©å¥–åŠ±æƒé‡ (æ¨è0.1-0.2)
        """
        if abs(gpt5_weight + grpo_weight - 1.0) > 1e-6:
            raise ValueError("æƒé‡ä¹‹å’Œå¿…é¡»ä¸º1.0")
        
        self.gpt5_weight = gpt5_weight
        self.grpo_weight = grpo_weight
        
        logger.info(f"æ··åˆå¥–åŠ±åˆæˆå™¨åˆå§‹åŒ–: GPT-5={gpt5_weight}, GRPO={grpo_weight}")
    
    def combine(
        self,
        gpt5_quality: float,
        grpo_advantage: float
    ) -> float:
        """
        åˆæˆæœ€ç»ˆå¥–åŠ±
        
        Args:
            gpt5_quality: GPT-5è´¨é‡è¯„åˆ† [0, 1]
            grpo_advantage: GRPOç»„å†…ä¼˜åŠ¿ (æ ‡å‡†åŒ–åï¼Œé›¶å‡å€¼)
            
        Returns:
            float: æœ€ç»ˆå¥–åŠ±ä¿¡å·
        """
        # å°†GPT-5åˆ†æ•°æ˜ å°„åˆ°[-1, 1]èŒƒå›´ï¼ˆä»¥0.5ä¸ºä¸­å¿ƒï¼‰
        gpt5_reward = (gpt5_quality - 0.5) * 2.0
        
        # åŠ æƒèåˆ
        final_reward = (
            self.gpt5_weight * gpt5_reward +
            self.grpo_weight * grpo_advantage
        )
        
        logger.debug(
            f"å¥–åŠ±åˆæˆ: GPT5={gpt5_reward:.3f}, "
            f"GRPO={grpo_advantage:.3f}, "
            f"æœ€ç»ˆ={final_reward:.3f}"
        )
        
        return final_reward
    
    def batch_combine(
        self,
        gpt5_qualities: List[float],
        grpo_advantages: List[float]
    ) -> List[float]:
        """
        æ‰¹é‡åˆæˆå¥–åŠ±
        
        Args:
            gpt5_qualities: GPT-5è´¨é‡è¯„åˆ†åˆ—è¡¨
            grpo_advantages: GRPOä¼˜åŠ¿åˆ—è¡¨
            
        Returns:
            æœ€ç»ˆå¥–åŠ±åˆ—è¡¨
        """
        if len(gpt5_qualities) != len(grpo_advantages):
            raise ValueError("GPT-5å’ŒGRPOå¥–åŠ±æ•°é‡ä¸åŒ¹é…")
        
        final_rewards = [
            self.combine(gpt5_q, grpo_adv)
            for gpt5_q, grpo_adv in zip(gpt5_qualities, grpo_advantages)
        ]
        
        # ç»Ÿè®¡ä¿¡æ¯
        logger.info(
            f"æ‰¹é‡å¥–åŠ±åˆæˆå®Œæˆ: "
            f"æ•°é‡={len(final_rewards)}, "
            f"å‡å€¼={np.mean(final_rewards):.3f}, "
            f"æ ‡å‡†å·®={np.std(final_rewards):.3f}"
        )
        
        return final_rewards
```

---

### ä¿®æ”¹4: ä¿®æ”¹ä¸»å¥–åŠ±è®¡ç®—å™¨

**ä¿®æ”¹æ–‡ä»¶**: `verl_code/verl/workers/hybrid_grpo_reward_calculator.py`

**å…³é”®ä¿®æ”¹**:

1. **åˆ é™¤å†—ä½™çš„åŒæ¨¡å‹å¯¹æ¯”é€»è¾‘**ï¼ˆç¬¬226-271è¡Œï¼‰:

```python
# åˆ é™¤è¿™ä¸ªæ–¹æ³•ï¼ˆå·²ç”±gpt5_dual_model_rater.pyå¤„ç†ï¼‰
def _compute_gpt5_auxiliary_rewards(...):
    # åˆ é™¤æ•´ä¸ªæ–¹æ³•
    pass
```

2. **ç®€åŒ–compute_group_rewardsæ–¹æ³•**ï¼ˆç¬¬123-167è¡Œï¼‰:

```python
# åŸä»£ç ï¼ˆå¤æ‚çš„æ··åˆé€»è¾‘ï¼‰
async def compute_group_rewards(
    self,
    group_samples: List[Dict],
    gpt5_scores: Optional[List[float]] = None
) -> List[float]:
    # å¤æ‚çš„GRPO+GPT5æ··åˆé€»è¾‘
    grpo_rewards = self._compute_grpo_rewards(group_samples)
    auxiliary_rewards = self._compute_gpt5_auxiliary_rewards(...)
    mixed_rewards = ...
    return mixed_rewards

# ä¿®æ”¹ä¸ºï¼ˆä»…å¤„ç†GRPOé€‰æ‹©ï¼‰
def compute_grpo_selection(
    self,
    candidates: List[str],
    log_probs: List[float]
) -> Tuple[str, float]:
    """
    GRPOç»„å†…é€‰æ‹©æœ€ä¼˜å€™é€‰
    
    Args:
        candidates: å€™é€‰æ”¹å†™åˆ—è¡¨
        log_probs: å¯¹åº”çš„logæ¦‚ç‡
        
    Returns:
        (best_candidate, grpo_advantage)
    """
    from .grpo_selector import GRPOSelector
    
    selector = GRPOSelector(selection_method="max_log_prob")
    best_candidate, grpo_advantage, best_idx = selector.select_best(
        candidates, log_probs
    )
    
    logger.info(f"GRPOé€‰æ‹©å®Œæˆ: å€™é€‰{best_idx}, ä¼˜åŠ¿={grpo_advantage:.3f}")
    
    return best_candidate, grpo_advantage
```

---

### ä¿®æ”¹5: ä¿®æ”¹è®­ç»ƒä¸»æµç¨‹

**ä¿®æ”¹æ–‡ä»¶**: `verl_code/verl/trainer/main_ppo.py`

åœ¨TaskRunnerç±»ä¸­æ·»åŠ æ··åˆè®­ç»ƒé€»è¾‘ï¼ˆå…·ä½“ä½ç½®éœ€è¦æŸ¥çœ‹å®Œæ•´æ–‡ä»¶ï¼Œé€šå¸¸åœ¨è®­ç»ƒå¾ªç¯éƒ¨åˆ†ï¼‰:

```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ 
async def hybrid_grpo_gpt5_train_step(self, batch):
    """
    GRPO+GPT-5æ··åˆè®­ç»ƒæ­¥éª¤ï¼ˆå®Œæ•´ç‰ˆï¼ŒåŒ…å«RAGé€‚é…ï¼‰
    """
    from verl.workers.grpo_selector import GRPOSelector
    from verl.workers.gpt5_dual_model_rater import GPT5DualModelRater
    from verl.workers.hybrid_reward_combiner import HybridRewardCombiner
    from verl.workers.rag_adapter import RAGAdapter
    
    # åˆå§‹åŒ–ç»„ä»¶
    selector = GRPOSelector()
    reward_combiner = HybridRewardCombiner(
        gpt5_weight=self.config.algorithm.hybrid_grpo_gpt5.gpt5_weight,
        grpo_weight=self.config.algorithm.hybrid_grpo_gpt5.grpo_weight
    )
    
    all_rewards = []
    all_best_candidates = []
    
    for prompt_data in batch:
        # æ­¥éª¤1: æå–å†å²ä¸Šä¸‹æ–‡
        contexts = prompt_data.get('contexts', [])
        user_profile = prompt_data.get('user_profile', '')
        history_summary = prompt_data.get('history_summary', '')
        
        # æ­¥éª¤2: ç”Ÿæˆå¤šä¸ªå€™é€‰ï¼ˆgroup_size=5ï¼‰
        candidates, log_probs = await self.actor_model.generate_multiple(
            prompt=prompt_data['prompt'],
            n=self.config.algorithm.hybrid_grpo_gpt5.group_size,
            return_log_probs=True
        )
        
        # æ­¥éª¤3: GRPOé€‰æ‹©æœ€ä¼˜
        best_candidate, grpo_advantage = selector.select_best(
            candidates, log_probs
        )
        
        # æ­¥éª¤4: è§£æ8Bæ¨¡å‹çš„ç»“æ„åŒ–è¾“å‡º
        # å‡è®¾best_candidateæ˜¯JSONæ ¼å¼: {"rewritten_query": "...", ...}
        try:
            candidate_dict = json.loads(best_candidate)
            rewritten_query = candidate_dict.get('rewritten_query', best_candidate)
        except:
            rewritten_query = best_candidate
        
        # æ­¥éª¤5: åˆå§‹åŒ–RAGé€‚é…å™¨
        rag_adapter = RAGAdapter(
            tenant_id=prompt_data.get('tenant_id', 'default'),
            contact_id=prompt_data.get('contact_id', 'default'),
            account_id=prompt_data.get('account_id', 'default'),
            message_id=prompt_data.get('message_id', 'default')
        )
        
        # æ­¥éª¤6: å¹¶è¡Œè°ƒç”¨8Bå’Œ32Bçš„RAGæ¥å£
        result_8b, result_32b = await rag_adapter.parallel_call_rag(
            rewritten_query_8b=rewritten_query,
            user_profile=user_profile,
            history_summary=history_summary,
            contexts=contexts,
            thought_unit=prompt_data.get('thought_unit', ''),
            score_threshold=0.95
        )
        
        # æ­¥éª¤7: æ£€æŸ¥RAGè°ƒç”¨æ˜¯å¦æˆåŠŸ
        if result_8b['status'] != 'success' or result_32b['status'] != 'success':
            logger.warning(f"RAGè°ƒç”¨å¤±è´¥ï¼Œè·³è¿‡è¯¥æ ·æœ¬")
            continue
        
        # æ­¥éª¤8: GPT-5è¯„ä¼°
        rater = GPT5DualModelRater(
            llm="GPT-5",
            actor_response={
                "rewritten_query": rewritten_query,
                "rag_results": result_8b['response']
            },
            reference_response={
                "rewritten_query": "32Bå†…éƒ¨æ”¹å†™",  # 32Bä¸æš´éœ²æ”¹å†™ç»“æœ
                "rag_results": result_32b['response']
            }
        )
        gpt5_quality = await rater.rate()
        
        # æ­¥éª¤9: åˆæˆæœ€ç»ˆå¥–åŠ±
        final_reward = reward_combiner.combine(gpt5_quality, grpo_advantage)
        
        all_rewards.append(final_reward)
        all_best_candidates.append(best_candidate)
        
        logger.info(
            f"æ ·æœ¬å¤„ç†å®Œæˆ: "
            f"GRPOä¼˜åŠ¿={grpo_advantage:.3f}, "
            f"GPT5è´¨é‡={gpt5_quality:.3f}, "
            f"æœ€ç»ˆå¥–åŠ±={final_reward:.3f}"
        )
    
    # æ­¥éª¤10: PPOæ›´æ–°
    if all_rewards:
        loss = self.ppo_update(all_best_candidates, all_rewards)
        
        return {
            'rewards': all_rewards,
            'loss': loss,
            'num_samples': len(all_rewards)
        }
    else:
        logger.warning("æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ï¼Œè·³è¿‡æœ¬æ‰¹æ¬¡æ›´æ–°")
        return {
            'rewards': [],
            'loss': 0.0,
            'num_samples': 0
        }
```

---

### ä¿®æ”¹6: æ›´æ–°é…ç½®æ–‡ä»¶

**ä¿®æ”¹æ–‡ä»¶**: `verl_code/verl/trainer/config/ppo_trainer.yaml`

åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ æ··åˆè®­ç»ƒé…ç½®:

```yaml
# åœ¨algorithméƒ¨åˆ†æ·»åŠ 
algorithm:
  adv_estimator: grpo
  norm_adv_by_std_in_grpo: true
  use_kl_in_reward: false
  
  # GRPO+GPT-5æ··åˆè®­ç»ƒé…ç½®
  hybrid_grpo_gpt5:
    enable: true
    
    # GRPOç»„å†…é€‰æ‹©
    group_size: 5
    selection_method: "max_log_prob"
    
    # å¥–åŠ±æƒé‡
    gpt5_weight: 0.85
    grpo_weight: 0.15
    
    # GPT-5é…ç½®
    gpt5_model: "GPT-5"
    gpt5_batch_size: 8
    gpt5_timeout: 10.0
    
    # RAGé…ç½®
    rag_endpoint: "http://localhost:8000/chat_8b"
    reference_endpoint: "http://localhost:8000/chat"
```

---

### ä¿®æ”¹7: æ›´æ–°è®­ç»ƒè„šæœ¬

**ä¿®æ”¹æ–‡ä»¶**: `scripts/run_hybrid_grpo_query_RL.sh`

```bash
#!/bin/bash
# GRPO+GPT-5æ··åˆè®­ç»ƒè„šæœ¬ v4.0

set -x

export CUDA_VISIBLE_DEVICES=6,7

python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    algorithm.norm_adv_by_std_in_grpo=true \
    algorithm.use_kl_in_reward=false \
    +algorithm.hybrid_grpo_gpt5.enable=true \
    +algorithm.hybrid_grpo_gpt5.group_size=5 \
    +algorithm.hybrid_grpo_gpt5.gpt5_weight=0.85 \
    +algorithm.hybrid_grpo_gpt5.grpo_weight=0.15 \
    +algorithm.hybrid_grpo_gpt5.selection_method=max_log_prob \
    data.train_files=/home/jovyan2/query_rl/data/sales_rag/train.parquet \
    data.val_files=/home/jovyan2/query_rl/data/sales_rag/val.parquet \
    data.train_batch_size=8 \
    actor_rollout_ref.model.path=/home/jovyan2/query_rl/model/Qwen3-8B \
    actor_rollout_ref.rollout.n=5 \
    actor_rollout_ref.rollout.temperature=0.7 \
    trainer.project_name='sales_rag_grpo_gpt5_v4' \
    trainer.experiment_name='qwen3_8b_grpo_gpt5_hybrid' \
    trainer.n_gpus_per_node=2 \
    trainer.total_epochs=20 \
    +seed=42
```

---

## ğŸ“Š ä¿®æ”¹æ€»ç»“

### æ–°å¢æ–‡ä»¶
1. `verl_code/verl/workers/grpo_selector.py` - GRPOç»„å†…é€‰æ‹©å™¨
2. `verl_code/verl/workers/hybrid_reward_combiner.py` - æ··åˆå¥–åŠ±åˆæˆå™¨
3. `verl_code/verl/workers/rag_adapter.py` - RAGæ¥å£é€‚é…å™¨ï¼ˆ**æ–°å¢**ï¼‰

### ä¿®æ”¹æ–‡ä»¶
1. `verl_code/verl/workers/gpt5_dual_model_rater.py` - ç®€åŒ–è¯„åˆ†é€»è¾‘
2. `verl_code/verl/workers/hybrid_grpo_reward_calculator.py` - åˆ é™¤å†—ä½™é€»è¾‘
3. `verl_code/verl/trainer/main_ppo.py` - æ·»åŠ æ··åˆè®­ç»ƒæµç¨‹ï¼ˆå«RAGé€‚é…ï¼‰
4. `verl_code/verl/trainer/config/ppo_trainer.yaml` - æ·»åŠ é…ç½®
5. `scripts/run_hybrid_grpo_query_RL.sh` - æ›´æ–°å¯åŠ¨è„šæœ¬

### æ ¸å¿ƒæ”¹è¿›
1. **æ¸…æ™°çš„èŒè´£åˆ†ç¦»**: GRPOè´Ÿè´£é€‰æ‹©ï¼ŒGPT-5è´Ÿè´£è¯„ä¼°ï¼ŒRAGè´Ÿè´£æ£€ç´¢
2. **ç®€åŒ–çš„å¥–åŠ±è®¡ç®—**: å»é™¤å†—ä½™çš„ç»„å†…æ ‡å‡†åŒ–
3. **é«˜æ•ˆçš„è®­ç»ƒæµç¨‹**: ä»…å¯¹æœ€ä¼˜å€™é€‰è¿›è¡Œå¤–éƒ¨è¯„åˆ†
4. **å¯é…ç½®çš„æƒé‡**: æ”¯æŒçµæ´»è°ƒæ•´GPT-5å’ŒGRPOçš„æƒé‡æ¯”ä¾‹
5. **RAGæ¥å£é€‚é…**: 8Bå’Œ32Bä½¿ç”¨ä¸åŒæ¥å£ï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”ï¼ˆ**æ–°å¢**ï¼‰

### RAGæ¥å£è¯´æ˜

#### 8Bæ¨¡å‹æµç¨‹
```
1. Actorç”Ÿæˆç»“æ„åŒ–è¾“å‡º: {"rewritten_query": "...", "user_profile": "...", ...}
2. è°ƒç”¨ /rag/chat_8b æ¥å£
3. ä¼ å…¥å‚æ•°:
   - rewritten_query: 8Bæ”¹å†™çš„query
   - user_profile: ç”¨æˆ·ç”»åƒ
   - history_summary: å†å²æ‘˜è¦
   - contexts: å¯¹è¯ä¸Šä¸‹æ–‡
4. RAGç›´æ¥ä½¿ç”¨rewritten_queryè¿›è¡Œæ£€ç´¢
5. è¿”å›æ£€ç´¢ç»“æœ
```

#### 32Bæ¨¡å‹æµç¨‹
```
1. è°ƒç”¨ /rag/chat æ¥å£
2. ä¼ å…¥å‚æ•°:
   - contexts: å¯¹è¯ä¸Šä¸‹æ–‡
   - thought_unit: æ€è€ƒå•å…ƒ
3. 32Bæ¨¡å‹å†…éƒ¨å®Œæˆqueryæ”¹å†™
4. RAGä½¿ç”¨32Bæ”¹å†™çš„queryè¿›è¡Œæ£€ç´¢
5. è¿”å›æ£€ç´¢ç»“æœ
```

#### å…³é”®å·®å¼‚
- **8B**: å¤–éƒ¨æ”¹å†™ + æ˜¾å¼ä¼ å‚
- **32B**: å†…éƒ¨æ”¹å†™ + éšå¼å¤„ç†
- **ç›®çš„**: ç¡®ä¿ä¸¤ä¸ªæ¨¡å‹ä½¿ç”¨ç›¸åŒçš„RAGæ£€ç´¢é€»è¾‘ï¼Œä»…æ”¹å†™è´¨é‡ä¸åŒ

### æ•°æ®æµç¤ºä¾‹

```python
# è¾“å…¥æ•°æ®æ ¼å¼
batch_data = {
    'prompt': '...',  # queryæ”¹å†™prompt
    'contexts': [...],  # å¯¹è¯å†å²
    'user_profile': '...',  # ç”¨æˆ·ç”»åƒ
    'history_summary': '...',  # å†å²æ‘˜è¦
    'tenant_id': '...',
    'contact_id': '...',
    'account_id': '...',
    'message_id': '...'
}

# 8Bè¾“å‡ºæ ¼å¼
actor_output = {
    'rewritten_query': 'å¸®æˆ‘æ‰¾ä¸€ä¸‹æœ€è¿‘çš„ä¿ƒé”€æ´»åŠ¨',
    'user_profile': 'ä»·æ ¼æ•æ„Ÿå‹ç”¨æˆ·',
    'history_summary': 'æœ€è¿‘å’¨è¯¢è¿‡äº§å“ä»·æ ¼'
}

# RAGè°ƒç”¨
result_8b = await rag_adapter.call_8b_rag(
    rewritten_query=actor_output['rewritten_query'],
    user_profile=actor_output['user_profile'],
    history_summary=actor_output['history_summary'],
    contexts=batch_data['contexts']
)

result_32b = await rag_adapter.call_32b_rag(
    contexts=batch_data['contexts']
)

# GPT-5å¯¹æ¯”è¯„åˆ†
gpt5_score = await gpt5_rater.rate(
    actor_response={'rewritten_query': actor_output['rewritten_query'], 'rag_results': result_8b},
    reference_response={'rag_results': result_32b}
)
```

