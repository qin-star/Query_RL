# Qwen-32B Queryæ”¹å†™RLè®­ç»ƒæ–¹æ¡ˆ

> åŸºäºGPT-5çŸ¥è¯†è’¸é¦ + RLç«äº‰ä¼˜åŒ–ï¼Œè®­ç»ƒé«˜æ€§èƒ½çš„Qwen-32B Queryæ”¹å†™æ¨¡å‹
>
> ï¼ˆæœ¬æ–¹æ¡ˆæ˜¯åŸºäº8Bæ¨¡å‹è°ƒé€šçš„åŸºç¡€ä¸Šçš„å‡çº§ç‰ˆæœ¬ï¼‰

---

## ğŸ“‹ æ–¹æ¡ˆæ¦‚è¿°

### æ ¸å¿ƒæ€è·¯

```
é˜¶æ®µ1: SFTçŸ¥è¯†è’¸é¦
GPT-5/DeepSeek V3.1 (Teacher) â†’ ç”Ÿæˆæ”¹å†™æ•°æ® â†’ Qwen-32B (Student) SFTè®­ç»ƒ

é˜¶æ®µ2: RLç«äº‰ä¼˜åŒ–  
Qwen-32B (è®­ç»ƒä¸­) â†” GPT-5/DeepSeek V3.1 (Baseline) + å®æ—¶RAGæ£€ç´¢ â†’ PPOä¼˜åŒ–
```

### ä¸ºä»€ä¹ˆé€‰æ‹©GPT-5æˆ–å…¶ä»–æ›´é«˜çº§çš„æ¨¡å‹ä½œä¸ºTeacherï¼Ÿ

**ç›¸æ¯”ä½¿ç”¨ç°æœ‰çš„Qwen-32Bä½œä¸ºTeacherï¼š**

| å¯¹æ¯”é¡¹   | ç°æœ‰Qwen-32B    | GPT-5/DeepSeek V3.1     |
| -------- | --------------- | ----------------------- |
| æ”¹å†™è´¨é‡ | è¾ƒå¥½ï¼Œä½†æœ‰å±€é™  | é¡¶å°–æ°´å¹³                |
| é¢†åŸŸé€‚é… | éœ€è¦promptè°ƒä¼˜  | å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›å¼º        |
| å¤©èŠ±æ¿   | åªèƒ½å­¦åˆ°32Bæ°´å¹³ | å¯ä»¥é€¼è¿‘ç”šè‡³è¶…è¶ŠTeacher |
| æˆæœ¬     | APIè°ƒç”¨æˆæœ¬ä½   | APIè°ƒç”¨æˆæœ¬è¾ƒé«˜         |
| è®­ç»ƒæ•ˆæœ | æå‡æœ‰é™        | æ˜¾è‘—æå‡                |

**æ¨èæ–¹æ¡ˆ**ï¼šä½¿ç”¨**GPT-5**ä½œä¸ºTeacherï¼ˆDeepSeek V3.1ä½œä¸ºå¤‡é€‰ï¼‰

### æŠ€æœ¯æ ˆ

- **åŸºåº§æ¨¡å‹**: Qwen3-32B-Instruct
- **æ•™å¸ˆæ¨¡å‹**: GPT-5 (APIè°ƒç”¨)
- **è¯„åˆ†æ¨¡å‹**: GPT-5 (APIè°ƒç”¨ï¼Œç”¨äºRLå¥–åŠ±è®¡ç®—)
- **RLç®—æ³•**: PPO (Proximal Policy Optimization)
- **è®­ç»ƒæ¡†æ¶**: ms-swift + VERL

---

## 1ï¸âƒ£ æ•°æ®å‡†å¤‡ï¼ˆåŸºäºæµ‹è¯•é›†ï¼‰

### 1.1 æ ¸å¿ƒæµç¨‹

å…ˆæ¢æˆé«˜é˜¶æ¨¡å‹è¿›è¡ŒQueryæ”¹å†™å’Œç”¨æˆ·ç”»åƒç­‰ä¿¡æ¯çš„æå‰ï¼Œæ„å»ºdatasheetç”¨äºQwen-32Bçš„SFT

```
æµ‹è¯•é›†Excel â†’ æ‰¹é‡è°ƒç”¨GPT-5æ”¹å†™ â†’ ä¿å­˜ç»“æœåˆ°test_sft.xlsx â†’ 
è´¨é‡ç­›é€‰(åŸºäºæ£€ç´¢æ•ˆæœ) â†’ è½¬æ¢ä¸ºJSONLè®­ç»ƒæ ¼å¼ â†’ SFTè®­ç»ƒ
```

### 1.2 æ‰¹é‡è°ƒç”¨GPT-5ç”Ÿæˆè®­ç»ƒæ•°æ®

```python
# generate_training_data.py

import asyncio
import pandas as pd
from openai import AsyncOpenAI
from typing import List, Dict
import json

class GPT5DataGenerator:
    """ä½¿ç”¨GPT-5ç”ŸæˆQueryæ”¹å†™è®­ç»ƒæ•°æ®"""
  
    def __init__(
        self,
        api_key: str,
        tenant_id: str = "fivedoctors",
        model: str = "gpt-5"
    ):
        self.client = AsyncOpenAI(api_key=api_key)
        self.tenant_id = tenant_id
        self.model = model
  
        # é¢†åŸŸä¸“ç”¨çš„ç³»ç»Ÿprompt
        self.system_prompts = {
            "fivedoctors": """ä½ æ˜¯ä¿å¥å“é¢†åŸŸçš„ä¸“ä¸šQueryæ”¹å†™ä¸“å®¶ã€‚
æ”¹å†™è¦æ±‚ï¼š
1. æå–æ ¸å¿ƒäº§å“å…³é”®è¯ï¼ˆèƒ¶åŸè›‹ç™½è‚½ã€è™¾é’ç´ ç­‰ï¼‰
2. æ˜ç¡®æŸ¥è¯¢æ„å›¾ï¼ˆåŠŸæ•ˆã€ç”¨æ³•ã€ç¦å¿Œã€é€‚ç”¨äººç¾¤ç­‰ï¼‰
3. è¡¥å……ä¸“ä¸šæœ¯è¯­ï¼Œæå‡æ£€ç´¢ç²¾åº¦
4. ä¿æŒç®€æ´ï¼Œé¿å…å†—ä½™
5. å®Œæ•´ä¿ç•™ç”¨æˆ·åŸå§‹æ„å›¾""",

            "chengla": """ä½ æ˜¯æ•™è‚²åŸ¹è®­é¢†åŸŸçš„ä¸“ä¸šQueryæ”¹å†™ä¸“å®¶ã€‚
æ”¹å†™è¦æ±‚ï¼š
1. è¯†åˆ«è¯¾ç¨‹ç±»å‹å’Œç§‘ç›®
2. æ˜ç¡®å­¦ä¹ é˜¶æ®µå’Œéœ€æ±‚
3. æå–å…³é”®çŸ¥è¯†ç‚¹
4. ä¿æŒæ•™è‚²é¢†åŸŸä¸“ä¸šæ€§"""
        }
  
    async def rewrite_query(
        self,
        query: str,
        user_profile: str = "",
        history_summary: str = ""
    ) -> str:
        """è°ƒç”¨GPT-5æ”¹å†™å•ä¸ªquery"""
  
        # æ„å»ºç”¨æˆ·prompt
        user_content = f"åŸå§‹æŸ¥è¯¢: {query}"
  
        if user_profile:
            user_content += f"\nç”¨æˆ·ç”»åƒ: {user_profile}"
  
        if history_summary:
            user_content += f"\nå†å²æ‘˜è¦: {history_summary}"
  
        user_content += "\n\nè¯·æ”¹å†™è¿™ä¸ªæŸ¥è¯¢ï¼Œä½¿å…¶æ›´é€‚åˆçŸ¥è¯†åº“æ£€ç´¢ã€‚åªè¿”å›æ”¹å†™åçš„æŸ¥è¯¢ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"
  
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": self.system_prompts.get(self.tenant_id, "")
                    },
                    {
                        "role": "user",
                        "content": user_content
                    }
                ],
                temperature=0.3,  # è¾ƒä½æ¸©åº¦ä¿è¯ç¨³å®šæ€§
                max_tokens=200
            )
      
            return response.choices[0].message.content.strip()
  
        except Exception as e:
            print(f"GPT-5è°ƒç”¨å¤±è´¥: {e}")
            return query  # å¤±è´¥æ—¶è¿”å›åŸquery
  
    async def batch_generate_from_excel(
        self,
        excel_path: str,
        output_path: str = "data/gpt5_rewrites.xlsx",
        query_column: str = "é—®é¢˜",
        max_concurrent: int = 10
    ) -> pd.DataFrame:
        """ä»Excelæ‰¹é‡ç”Ÿæˆæ”¹å†™æ•°æ®"""
  
        # è¯»å–æµ‹è¯•é›†
        df = pd.read_excel(excel_path)
        print(f"ğŸ“š è¯»å–æµ‹è¯•é›†: {len(df)} æ¡")
  
        # å‡†å¤‡ä»»åŠ¡
        semaphore = asyncio.Semaphore(max_concurrent)
  
        async def rewrite_with_limit(row):
            async with semaphore:
                query = row[query_column]
                user_profile = row.get("ç”¨æˆ·ç”»åƒ", "")
                history_summary = row.get("å†å²æ‘˜è¦", "")
          
                rewritten = await self.rewrite_query(
                    query, user_profile, history_summary
                )
          
                return {
                    "original_query": query,
                    "rewritten_query": rewritten,
                    "user_profile": user_profile,
                    "history_summary": history_summary
                }
  
        # æ‰¹é‡æ‰§è¡Œ
        print("ğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆ...")
        tasks = [rewrite_with_limit(row) for _, row in df.iterrows()]
        results = await asyncio.gather(*tasks)
  
        # ä¿å­˜ç»“æœ
        result_df = pd.DataFrame(results)
        result_df.to_excel(output_path, index=False)
  
        print(f"âœ… ç”Ÿæˆå®Œæˆ: {output_path}")
        print(f"   æˆåŠŸç”Ÿæˆ: {len(results)} æ¡")
  
        return result_df


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    generator = GPT5DataGenerator(
        api_key="your-api-key",
        tenant_id="fivedoctors"
    )
  
    # æ‰¹é‡ç”Ÿæˆ
    result_df = await generator.batch_generate_from_excel(
        excel_path="sales-rag/Test-jq-only/Test_data/å¥³åšå£«æµ‹è¯•é›†.xlsx",
        output_path="data/gpt5_rewrites_fivedoctors.xlsx"
    )

if __name__ == "__main__":
    asyncio.run(main())
```

### 1.3 è°ƒç”¨RAGéªŒè¯æ£€ç´¢æ•ˆæœ

ç”Ÿæˆæ”¹å†™åï¼Œéœ€è¦éªŒè¯æ£€ç´¢æ•ˆæœï¼Œç­›é€‰é«˜è´¨é‡æ ·æœ¬ï¼š

```python
# validate_with_rag.py

import asyncio
import pandas as pd
import httpx
from typing import List, Dict

async def validate_rewrites_with_rag(
    input_excel: str,
    output_excel: str,
    rag_api_url: str = "http://localhost:8000/api/chat/general_rag",
    tenant_id: str = "fivedoctors"
):
    """è°ƒç”¨RAGéªŒè¯æ”¹å†™æ•ˆæœ"""
  
    df = pd.read_excel(input_excel)
    results = []
  
    async with httpx.AsyncClient(timeout=30.0) as client:
        for _, row in df.iterrows():
            payload = {
                "query": row["original_query"],
                "rewritten_query": row["rewritten_query"],
                "tenant_id": tenant_id,
                "kb_name": "default",
                "top_k": 5,
                "score_threshold": 0.5
            }
      
            try:
                response = await client.post(rag_api_url, json=payload)
                result = response.json()
          
                data = result.get("data", {})
                recall = data.get("recall", [])
          
                # è®¡ç®—æ£€ç´¢è´¨é‡æŒ‡æ ‡
                top1_score = recall[0].get("reranker_score", 0) if recall else 0
                recall_count = len(recall)
          
                results.append({
                    **row.to_dict(),
                    "recall_count": recall_count,
                    "top1_score": top1_score,
                    "success": True
                })
      
            except Exception as e:
                print(f"âŒ æ£€ç´¢å¤±è´¥: {row['original_query']} - {e}")
                results.append({
                    **row.to_dict(),
                    "success": False
                })
  
    # ä¿å­˜éªŒè¯ç»“æœ
    result_df = pd.DataFrame(results)
    result_df.to_excel(output_excel, index=False)
  
    # ç»Ÿè®¡
    success_df = result_df[result_df["success"] == True]
    print(f"\nâœ… éªŒè¯å®Œæˆ:")
    print(f"   æˆåŠŸç‡: {len(success_df)}/{len(df)} = {len(success_df)/len(df)*100:.1f}%")
    print(f"   å¹³å‡Top1åˆ†æ•°: {success_df['top1_score'].mean():.3f}")
  
    return result_df
```

### 1.4 è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼

```python
# convert_to_training_format.py

import pandas as pd
import json
from pathlib import Path

def convert_to_sft_format(
    excel_path: str,
    output_dir: str,
    quality_threshold: float = 0.6
):
    """è½¬æ¢ä¸ºSFTè®­ç»ƒæ ¼å¼"""
  
    df = pd.read_excel(excel_path)
  
    # è´¨é‡ç­›é€‰
    df = df[
        (df["success"] == True) &
        (df["top1_score"] >= quality_threshold) &
        (df["rewritten_query"].notna())
    ]
  
    print(f"âœ… è´¨é‡ç­›é€‰: {len(df)} æ¡æ ·æœ¬ (threshold={quality_threshold})")
  
    # è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼
    samples = []
    for _, row in df.iterrows():
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¿å¥å“é¢†åŸŸçš„Queryæ”¹å†™ä¸“å®¶..."
            },
            {
                "role": "user",
                "content": f"åŸå§‹æŸ¥è¯¢: {row['original_query']}\n\nè¯·æ”¹å†™ã€‚"
            },
            {
                "role": "assistant",
                "content": row["rewritten_query"]
            }
        ]
  
        samples.append({
            "messages": messages,
            "metadata": {
                "top1_score": float(row["top1_score"]),
                "source": "gpt5_teacher"
            }
        })
  
    # åˆ’åˆ†æ•°æ®é›†
    import random
    random.shuffle(samples)
  
    n = len(samples)
    train_size = int(n * 0.8)
    val_size = int(n * 0.1)
  
    splits = {
        "train": samples[:train_size],
        "val": samples[train_size:train_size + val_size],
        "test": samples[train_size + val_size:]
    }
  
    # ä¿å­˜
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
  
    for split_name, split_data in splits.items():
        file_path = output_path / f"{split_name}.jsonl"
        with open(file_path, "w", encoding="utf-8") as f:
            for sample in split_data:
                f.write(json.dumps(sample, ensure_ascii=False) + "\n")
        print(f"   {split_name}: {len(split_data)} æ¡ â†’ {file_path}")
```

### 1.5 å¿«é€Ÿæ‰§è¡Œæµç¨‹

```bash
# æ­¥éª¤1: ä½¿ç”¨GPT-5æ‰¹é‡ç”Ÿæˆæ”¹å†™ï¼ˆçº¦30åˆ†é’Ÿï¼Œè§†APIé€Ÿåº¦ï¼‰
python generate_training_data.py

# æ­¥éª¤2: è°ƒç”¨RAGéªŒè¯æ£€ç´¢æ•ˆæœï¼ˆçº¦20åˆ†é’Ÿï¼‰
python validate_with_rag.py

# æ­¥éª¤3: è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼ï¼ˆ1åˆ†é’Ÿï¼‰
python convert_to_training_format.py

# æ•°æ®å‡†å¤‡å®Œæˆï¼è¿›å…¥SFTè®­ç»ƒé˜¶æ®µ
```

**é¢„æœŸæ•°æ®è§„æ¨¡**ï¼š

- åŸå§‹æµ‹è¯•é›†: 500-1000æ¡
- GPT-5ç”ŸæˆæˆåŠŸ: 480-980æ¡
- æ£€ç´¢éªŒè¯é€šè¿‡: 400-900æ¡
- è´¨é‡ç­›é€‰å(top1_score>0.6): 300-700æ¡
- æœ€ç»ˆè®­ç»ƒé›†: 240-560æ¡

---

## 2ï¸âƒ£ SFTè®­ç»ƒï¼ˆåŸºäºms-swiftï¼‰

### 2.1 ä¸ºä»€ä¹ˆä½¿ç”¨ms-swiftï¼Ÿ

- âœ… æ”¯æŒQwenç³»åˆ—å¼€ç®±å³ç”¨
- âœ… è‡ªåŠ¨é…ç½®LoRAã€DeepSpeed
- âœ… è®­ç»ƒç®€å•ï¼Œä¸€æ¡å‘½ä»¤æå®š
- âœ… æ”¯æŒå¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ

### 2.2 è®­ç»ƒè„šæœ¬

```bash
# train_sft_qwen32b.sh

# ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0,1,2,3  # 4å¼ GPU
export NPROC_PER_NODE=4

# æ¨¡å‹å’Œæ•°æ®è·¯å¾„
MODEL_PATH="Qwen/Qwen2.5-32B-Instruct"
DATASET_PATH="data/sft_fivedoctors/train.jsonl"
OUTPUT_DIR="output/sft_qwen32b_fivedoctors"

# LoRAè®­ç»ƒï¼ˆæ¨èï¼Œæ˜¾å­˜å ç”¨ä½ï¼‰
swift sft \
    --model ${MODEL_PATH} \
    --train_type lora \
    --dataset ${DATASET_PATH} \
    --torch_dtype bfloat16 \
    --num_train_epochs 3 \
    --per_device_train_batch_size 2 \
    --per_device_eval_batch_size 2 \
    --learning_rate 5e-5 \
    --lora_rank 16 \
    --lora_alpha 32 \
    --target_modules all-linear \
    --gradient_accumulation_steps 8 \
    --eval_steps 100 \
    --save_steps 100 \
    --save_total_limit 3 \
    --logging_steps 10 \
    --max_length 2048 \
    --output_dir ${OUTPUT_DIR} \
    --warmup_ratio 0.05 \
    --dataloader_num_workers 4

echo "âœ… SFTè®­ç»ƒå®Œæˆï¼"
echo "æ¨¡å‹ä¿å­˜åœ¨: ${OUTPUT_DIR}"
```

### 2.3 èµ„æºéœ€æ±‚

**Qwen-32B LoRAè®­ç»ƒ**ï¼š

- GPU: 4x A100 (80GB) æˆ– 4x H100
- æ˜¾å­˜: æ¯å¡çº¦50-60GB
- è®­ç»ƒæ—¶é—´: 2-3å¤©ï¼ˆ300-500æ¡æ ·æœ¬ï¼Œ3 epochsï¼‰

**å…¨é‡å¾®è°ƒ**ï¼ˆä¸æ¨èï¼‰ï¼š

- GPU: 8x A100 (80GB)
- æ˜¾å­˜: æ¯å¡çº¦70GB
- è®­ç»ƒæ—¶é—´: 4-5å¤©

---

## 3ï¸âƒ£ RLè®­ç»ƒï¼ˆQwen-32B vs GPT-5ï¼‰

### 3.1 æ ¸å¿ƒæ€è·¯

```
æ¯ä¸ªè®­ç»ƒæ­¥éª¤ï¼š
1. è¾“å…¥åŸå§‹query
2. Qwen-32Bç”Ÿæˆæ”¹å†™ â†’ å®æ—¶è°ƒç”¨RAGæ£€ç´¢ â†’ è·å¾—æ£€ç´¢ç»“æœA
3. GPT-5ç”Ÿæˆæ”¹å†™ â†’ å®æ—¶è°ƒç”¨RAGæ£€ç´¢ â†’ è·å¾—æ£€ç´¢ç»“æœB
4. æ¯”è¾ƒAå’ŒBï¼š
   - GPT-5è¯„åˆ†ï¼šè¯„ä¼°æ”¹å†™è´¨é‡
   - æ£€ç´¢æ•ˆæœï¼šæ¯”è¾ƒreranker_score
5. è®¡ç®—å¥–åŠ±ï¼š
   - å¦‚æœ32Bæ£€ç´¢æ•ˆæœ > GPT-5ï¼šæ­£å¥–åŠ±ï¼ˆé¼“åŠ±ï¼‰
   - å¦‚æœ32Bæ£€ç´¢æ•ˆæœ < GPT-5ï¼šè´Ÿå¥–åŠ±ï¼ˆæƒ©ç½šï¼‰
6. PPOæ›´æ–°Qwen-32Bå‚æ•°
```

### 3.2 GPT-5è¯„åˆ†æ¨¡å‹

```python
# gpt5_scorer.pyï¼ˆä¸ä¹‹å‰ç›¸åŒï¼‰

from openai import OpenAI

class GPT5Scorer:
    """GPT-5è¯„åˆ†å™¨"""
  
    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)
  
    def score_rewrite(
        self,
        original: str,
        rewritten: str,
        context: dict = None
    ) -> dict:
        """è¯„åˆ†æ”¹å†™è´¨é‡"""
  
        # ï¼ˆè¯¦ç»†å®ç°è§å‰æ–‡ï¼‰
        # è¿”å›ï¼š{"ç»¼åˆå¾—åˆ†": 4.5, "è¯„åˆ†ç†ç”±": "..."}
        ...
```

### 3.3 Rewardå‡½æ•°è®¾è®¡

```python
# reward_function.py

import numpy as np
from gpt5_scorer import GPT5Scorer

class RewardFunction:
    """å¤šç»´åº¦å¥–åŠ±å‡½æ•°"""
  
    def __init__(self, gpt5_api_key: str):
        self.gpt5_scorer = GPT5Scorer(api_key=gpt5_api_key)
  
        # å¥–åŠ±æƒé‡
        self.weights = {
            "gpt5_score": 0.35,         # GPT-5æ”¹å†™è´¨é‡è¯„åˆ†
            "retrieval_quality": 0.40,  # æ£€ç´¢æ•ˆæœï¼ˆreranker_scoreï¼‰
            "relative_win": 0.25        # ç›¸å¯¹GPT-5çš„èƒœç‡
        }
  
    def compute_reward(
        self,
        original_query: str,
        qwen32b_rewrite: str,
        gpt5_rewrite: str,
        context: dict,
        qwen32b_retrieval: list,
        gpt5_retrieval: list
    ) -> float:
        """è®¡ç®—ç»¼åˆå¥–åŠ±"""
  
        # 1. GPT-5è¯„åˆ†å¥–åŠ±
        gpt5_score_32b = self.gpt5_scorer.score_rewrite(
            original_query, qwen32b_rewrite, context
        )["ç»¼åˆå¾—åˆ†"] / 5.0  # å½’ä¸€åŒ–åˆ°[0,1]
  
        gpt5_score_gpt5 = self.gpt5_scorer.score_rewrite(
            original_query, gpt5_rewrite, context
        )["ç»¼åˆå¾—åˆ†"] / 5.0
  
        score_diff = gpt5_score_32b - gpt5_score_gpt5
        score_reward = np.tanh(score_diff * 3)  # ç¼©æ”¾åˆ°[-1,1]
  
        # 2. æ£€ç´¢è´¨é‡å¥–åŠ±
        retrieval_32b = self._calc_retrieval_quality(qwen32b_retrieval)
        retrieval_gpt5 = self._calc_retrieval_quality(gpt5_retrieval)
  
        retrieval_diff = retrieval_32b - retrieval_gpt5
        retrieval_reward = np.tanh(retrieval_diff * 3)
  
        # 3. èƒœç‡å¥–åŠ±ï¼ˆæ˜¯å¦è¶…è¶ŠGPT-5ï¼‰
        if retrieval_32b > retrieval_gpt5:
            win_reward = 0.5  # 32Bèµ¢äº†
        elif retrieval_32b < retrieval_gpt5:
            win_reward = -0.3  # 32Bè¾“äº†
        else:
            win_reward = 0.0  # å¹³å±€
  
        # åŠ æƒæ±‚å’Œ
        total_reward = (
            self.weights["gpt5_score"] * score_reward +
            self.weights["retrieval_quality"] * retrieval_reward +
            self.weights["relative_win"] * win_reward
        )
  
        return np.clip(total_reward, -1.0, 1.0)
  
    def _calc_retrieval_quality(self, results: list) -> float:
        """è®¡ç®—æ£€ç´¢è´¨é‡"""
        if not results:
            return 0.0
  
        # Top1åˆ†æ•°
        top1 = results[0].get("reranker_score", 0) if results else 0
  
        # Top3å¹³å‡åˆ†æ•°
        top3_scores = [r.get("reranker_score", 0) for r in results[:3]]
        avg_top3 = np.mean(top3_scores) if top3_scores else 0
  
        # ç»¼åˆè´¨é‡åˆ†æ•°
        return 0.6 * top1 + 0.4 * avg_top3
```

### 3.4 RLè®­ç»ƒä¸»æµç¨‹

```python
# rl_trainer.py

import asyncio
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import httpx
from reward_function import RewardFunction

class Qwen32BRLTrainer:
    """Qwen-32B RLè®­ç»ƒå™¨"""
  
    def __init__(
        self,
        qwen32b_sft_path: str,  # SFTè®­ç»ƒåçš„æ¨¡å‹
        gpt5_api_key: str,
        rag_api_url: str = "http://localhost:8000/api/chat/general_rag",
        tenant_id: str = "fivedoctors"
    ):
        self.tenant_id = tenant_id
        self.rag_api_url = rag_api_url
  
        # åŠ è½½Qwen-32Bæ¨¡å‹ï¼ˆPolicy Modelï¼‰
        self.policy_model = AutoModelForCausalLM.from_pretrained(
            qwen32b_sft_path,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        self.tokenizer = AutoTokenizer.from_pretrained(qwen32b_sft_path)
  
        # GPT-5 APIï¼ˆç”¨äºç”Ÿæˆbaselineæ”¹å†™å’Œè¯„åˆ†ï¼‰
        from openai import AsyncOpenAI
        self.gpt5_client = AsyncOpenAI(api_key=gpt5_api_key)
  
        # å¥–åŠ±å‡½æ•°
        self.reward_fn = RewardFunction(gpt5_api_key=gpt5_api_key)
  
    async def generate_training_episode(
        self,
        original_query: str,
        context: dict
    ) -> dict:
        """ç”Ÿæˆä¸€ä¸ªè®­ç»ƒepisode"""
  
        # 1. Qwen-32Bç”Ÿæˆæ”¹å†™
        qwen32b_rewrite = await self._generate_qwen32b(original_query, context)
  
        # 2. GPT-5ç”Ÿæˆæ”¹å†™ï¼ˆbaselineï¼‰
        gpt5_rewrite = await self._generate_gpt5(original_query, context)
  
        # 3. å¹¶è¡Œè°ƒç”¨RAGæ£€ç´¢
        qwen32b_results, gpt5_results = await asyncio.gather(
            self._call_rag(qwen32b_rewrite, original_query),
            self._call_rag(gpt5_rewrite, original_query)
        )
  
        # 4. è®¡ç®—å¥–åŠ±
        reward = self.reward_fn.compute_reward(
            original_query=original_query,
            qwen32b_rewrite=qwen32b_rewrite,
            gpt5_rewrite=gpt5_rewrite,
            context=context,
            qwen32b_retrieval=qwen32b_results,
            gpt5_retrieval=gpt5_results
        )
  
        return {
            "original_query": original_query,
            "qwen32b_rewrite": qwen32b_rewrite,
            "gpt5_rewrite": gpt5_rewrite,
            "reward": reward,
            "qwen32b_top1": qwen32b_results[0].get("reranker_score", 0) if qwen32b_results else 0,
            "gpt5_top1": gpt5_results[0].get("reranker_score", 0) if gpt5_results else 0
        }
  
    async def _generate_qwen32b(self, query: str, context: dict) -> str:
        """Qwen-32Bç”Ÿæˆæ”¹å†™"""
  
        prompt = f"åŸå§‹æŸ¥è¯¢: {query}\n\nè¯·æ”¹å†™ã€‚"
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.policy_model.device)
  
        with torch.no_grad():
            outputs = self.policy_model.generate(
                **inputs,
                max_new_tokens=150,
                temperature=0.7,
                top_p=0.9
            )
  
        rewrite = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
  
        return rewrite.strip()
  
    async def _generate_gpt5(self, query: str, context: dict) -> str:
        """GPT-5ç”Ÿæˆæ”¹å†™ï¼ˆbaselineï¼‰"""
  
        response = await self.gpt5_client.chat.completions.create(
            model="gpt-5",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯Queryæ”¹å†™ä¸“å®¶..."},
                {"role": "user", "content": f"åŸå§‹æŸ¥è¯¢: {query}\n\nè¯·æ”¹å†™ã€‚"}
            ],
            temperature=0.3,
            max_tokens=150
        )
  
        return response.choices[0].message.content.strip()
  
    async def _call_rag(self, rewritten_query: str, original_query: str) -> list:
        """è°ƒç”¨RAG APIè·å–æ£€ç´¢ç»“æœ"""
  
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                self.rag_api_url,
                json={
                    "query": original_query,
                    "rewritten_query": rewritten_query,
                    "tenant_id": self.tenant_id,
                    "kb_name": "default",
                    "top_k": 5
                }
            )
      
            result = response.json()
            return result.get("data", {}).get("recall", [])
```

### 3.5 RAG APIä¿®æ”¹

éœ€è¦åœ¨ `general_rag_routes.py`æ·»åŠ å‚æ•°æ”¯æŒå¤–éƒ¨ä¼ å…¥æ”¹å†™ï¼š

```python
# sales-rag/libs/chatchat-server/chatchat/server/api_server/general_rag_routes.py

@router.post("/api/chat/general_rag")
async def general_rag_endpoint(
    query: str,
    tenant_id: str,
    rewritten_query: Optional[str] = None,  # ğŸ†• æ–°å¢å‚æ•°
    ...
):
    # å¦‚æœæä¾›äº†æ”¹å†™queryï¼Œç›´æ¥ä½¿ç”¨
    if rewritten_query:
        new_query = rewritten_query
    else:
        # ä½¿ç”¨åŸæœ‰æ”¹å†™é€»è¾‘
        new_query = await rewrite_query_by_model(...)
  
    # åç»­æ£€ç´¢æµç¨‹ä¸å˜
    ...
```

### 3.6 ä»Rewardåˆ°å‚æ•°æ›´æ–°ï¼šPPOç®—æ³•è¯¦è§£

è¿™æ˜¯RLè®­ç»ƒçš„æ ¸å¿ƒï¼è®©æˆ‘è¯¦ç»†è§£é‡ŠGPT-5è¯„åˆ†ç”Ÿæˆçš„rewardå¦‚ä½•æ›´æ–°Qwen-32Bçš„å‚æ•°ã€‚

#### 3.6.1 PPOç®—æ³•åŸç†

**åŸºæœ¬æµç¨‹**ï¼š

```
1. æ”¶é›†è½¨è¿¹(Trajectory)
   - å½“å‰Qwen-32Bç”Ÿæˆæ”¹å†™
   - è°ƒç”¨RAGè·å–æ£€ç´¢ç»“æœ
   - GPT-5è¯„åˆ† â†’ è®¡ç®—reward

2. è®¡ç®—ä¼˜åŠ¿å‡½æ•°(Advantage)
   - ä¼°è®¡çŠ¶æ€ä»·å€¼ V(s)
   - è®¡ç®— Advantage = Reward - V(s)

3. ç­–ç•¥æ¢¯åº¦æ›´æ–°
   - è®¡ç®—ç­–ç•¥æ¯”ç‡ ratio = Ï€_new / Ï€_old
   - è®¡ç®— PPO lossï¼ˆå¸¦clipï¼‰
   - åå‘ä¼ æ’­æ›´æ–°å‚æ•°

4. ä»·å€¼å‡½æ•°æ›´æ–°
   - æ›´æ–° V(s) ä½¿å…¶æ›´å‡†ç¡®ä¼°è®¡æœªæ¥å›æŠ¥
```

#### 3.6.2 è¯¦ç»†æ•°å­¦æ¨å¯¼

**Step 1: æ”¶é›†ç»éªŒ**

å¯¹äºæ¯ä¸ªqueryï¼Œæˆ‘ä»¬æ”¶é›†ä¸€ä¸ªå®Œæ•´çš„trajectoryï¼š

```python
trajectory = {
    "state": original_query,              # çŠ¶æ€ï¼ˆåŸå§‹queryï¼‰
    "action": qwen32b_rewrite,           # åŠ¨ä½œï¼ˆ32Bç”Ÿæˆçš„æ”¹å†™ï¼‰
    "reward": reward_from_gpt5_and_rag,  # å¥–åŠ±ï¼ˆGPT-5è¯„åˆ†+æ£€ç´¢æ•ˆæœï¼‰
    "log_prob": log_prob_of_action,      # å½“å‰ç­–ç•¥ä¸‹åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
}
```

**Step 2: è®¡ç®—ä¼˜åŠ¿å‡½æ•°(Advantage)**

ä¼˜åŠ¿å‡½æ•°å‘Šè¯‰æˆ‘ä»¬ï¼š**è¿™ä¸ªåŠ¨ä½œæ¯”å¹³å‡æ°´å¹³å¥½å¤šå°‘**

```python
# ä»·å€¼å‡½æ•°ä¼°è®¡ï¼šè¿™ä¸ªçŠ¶æ€ä¸‹æœŸæœ›çš„ç´¯ç§¯å›æŠ¥
V(state) = critic_model(state)  # ä½¿ç”¨criticç½‘ç»œä¼°è®¡

# ä¼˜åŠ¿å‡½æ•°ï¼šå®é™…reward - æœŸæœ›reward
Advantage = Reward - V(state)

# å¦‚æœ Advantage > 0ï¼šè¿™ä¸ªåŠ¨ä½œæ¯”æœŸæœ›å¥½ â†’ å¢åŠ è¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡
# å¦‚æœ Advantage < 0ï¼šè¿™ä¸ªåŠ¨ä½œæ¯”æœŸæœ›å·® â†’ é™ä½è¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡
```

**Step 3: è®¡ç®—ç­–ç•¥æ¯”ç‡**

PPOçš„æ ¸å¿ƒï¼šæ¯”è¾ƒæ–°æ—§ç­–ç•¥

```python
# æ—§ç­–ç•¥ï¼šå½“å‰çš„Qwen-32B
log_prob_old = log P_old(qwen32b_rewrite | original_query)

# æ–°ç­–ç•¥ï¼šæ›´æ–°ä¸€æ­¥åçš„Qwen-32B  
log_prob_new = log P_new(qwen32b_rewrite | original_query)

# ç­–ç•¥æ¯”ç‡
ratio = exp(log_prob_new - log_prob_old) = P_new / P_old
```

**Step 4: PPOæŸå¤±å‡½æ•°**

```python
# åŸºç¡€ç­–ç•¥æ¢¯åº¦
surrogate_loss = ratio * Advantage

# PPO clipï¼šé˜²æ­¢æ›´æ–°å¤ªæ¿€è¿›
clipped_ratio = clip(ratio, 1-Îµ, 1+Îµ)  # Îµé€šå¸¸ä¸º0.2
clipped_loss = clipped_ratio * Advantage

# æœ€ç»ˆlossï¼šå–ä¸¤è€…æœ€å°å€¼ï¼ˆä¿å®ˆæ›´æ–°ï¼‰
policy_loss = -min(surrogate_loss, clipped_loss)

# ä¸ºä»€ä¹ˆæ˜¯è´Ÿå·ï¼Ÿå› ä¸ºæˆ‘ä»¬è¦æœ€å¤§åŒ–rewardï¼Œä½†ä¼˜åŒ–å™¨æ˜¯minimize loss
```

**Step 5: ä»·å€¼å‡½æ•°æŸå¤±**

```python
# Criticç½‘ç»œè¦å‡†ç¡®é¢„æµ‹å›æŠ¥
value_loss = (Reward - V(state))^2
```

**Step 6: æ€»æŸå¤±**

```python
total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy_bonus

# entropy_bonus: é¼“åŠ±æ¢ç´¢ï¼Œé¿å…ç­–ç•¥è¿‡æ—©æ”¶æ•›
```

#### 3.6.3 å‚æ•°æ›´æ–°æµç¨‹

```python
# ä¼ªä»£ç ï¼šå®Œæ•´çš„PPOæ›´æ–°æ­¥éª¤

class PPOTrainer:
    def __init__(self):
        self.actor = Qwen32B_Model()      # ç­–ç•¥ç½‘ç»œï¼ˆç”Ÿæˆæ”¹å†™ï¼‰
        self.critic = Value_Network()     # ä»·å€¼ç½‘ç»œï¼ˆä¼°è®¡V(s)ï¼‰
        self.optimizer_actor = Adam(self.actor.parameters(), lr=1e-5)
        self.optimizer_critic = Adam(self.critic.parameters(), lr=1e-4)
  
    def update(self, trajectories):
        """ä½¿ç”¨æ”¶é›†çš„trajectoriesæ›´æ–°æ¨¡å‹"""
    
        # 1. æå–æ•°æ®
        states = [t["state"] for t in trajectories]
        actions = [t["action"] for t in trajectories]
        rewards = [t["reward"] for t in trajectories]
        old_log_probs = [t["log_prob"] for t in trajectories]
    
        # 2. è®¡ç®—ä¼˜åŠ¿å‡½æ•°
        with torch.no_grad():
            values = self.critic(states)  # V(s)
            advantages = rewards - values  # Advantage
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)  # å½’ä¸€åŒ–
    
        # 3. PPOæ›´æ–°ï¼ˆå¤šä¸ªepochï¼‰
        for epoch in range(4):  # PPOé€šå¸¸æ›´æ–°4æ¬¡
            # 3.1 å‰å‘ä¼ æ’­
            new_log_probs = self.actor.get_log_prob(states, actions)
            new_values = self.critic(states)
        
            # 3.2 è®¡ç®—æ¯”ç‡
            ratio = torch.exp(new_log_probs - old_log_probs)
        
            # 3.3 è®¡ç®—PPO loss
            surrogate1 = ratio * advantages
            surrogate2 = torch.clamp(ratio, 0.8, 1.2) * advantages
            policy_loss = -torch.min(surrogate1, surrogate2).mean()
        
            # 3.4 è®¡ç®—value loss
            value_loss = 0.5 * (rewards - new_values).pow(2).mean()
        
            # 3.5 è®¡ç®—entropyï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
            entropy = self.actor.get_entropy(states)
            entropy_loss = -0.01 * entropy.mean()
        
            # 3.6 æ€»æŸå¤±
            total_loss = policy_loss + value_loss + entropy_loss
        
            # 4. åå‘ä¼ æ’­
            self.optimizer_actor.zero_grad()
            self.optimizer_critic.zero_grad()
            total_loss.backward()
        
            # 5. æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
        
            # 6. æ›´æ–°å‚æ•°
            self.optimizer_actor.step()
            self.optimizer_critic.step()
        
        return {
            "policy_loss": policy_loss.item(),
            "value_loss": value_loss.item(),
            "entropy": entropy.mean().item()
        }
```

#### 3.6.4 å…·ä½“ç¤ºä¾‹

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªtraining episodeï¼š

```python
# åŸå§‹query
original_query = "èƒ¶åŸè›‹ç™½æ€ä¹ˆåƒ"

# Qwen-32Bç”Ÿæˆæ”¹å†™
qwen32b_rewrite = "èƒ¶åŸè›‹ç™½è‚½ æœç”¨æ–¹æ³• æ¨èç”¨é‡"

# GPT-5è¯„åˆ† + æ£€ç´¢æ•ˆæœ â†’ reward
reward = 0.65  # æ­£å¥–åŠ±ï¼Œè¯´æ˜32Bè¡¨ç°ä¸é”™

# å½“å‰ç­–ç•¥ä¸‹ï¼Œè¿™ä¸ªæ”¹å†™çš„logæ¦‚ç‡
log_prob_old = -2.3  # å¯¹æ•°æ¦‚ç‡ï¼ˆè´Ÿæ•°ï¼‰

# --- PPOæ›´æ–°è¿‡ç¨‹ ---

# 1. Criticä¼°è®¡çŠ¶æ€ä»·å€¼
V_state = 0.5  # Criticè®¤ä¸ºè¿™ä¸ªqueryå¹³å‡èƒ½å¾—0.5çš„reward

# 2. è®¡ç®—Advantage
Advantage = reward - V_state = 0.65 - 0.5 = 0.15  # æ­£æ•°ï¼æ¯”æœŸæœ›å¥½

# 3. æ›´æ–°åçš„ç­–ç•¥
log_prob_new = -2.1  # æ›´æ–°åï¼Œè¿™ä¸ªæ”¹å†™çš„æ¦‚ç‡å¢åŠ äº†

# 4. è®¡ç®—ratio
ratio = exp(-2.1 - (-2.3)) = exp(0.2) = 1.22

# 5. PPO loss
surrogate1 = 1.22 * 0.15 = 0.183
clipped_ratio = min(max(1.22, 0.8), 1.2) = 1.2
surrogate2 = 1.2 * 0.15 = 0.18
policy_loss = -min(0.183, 0.18) = -0.18  # è´Ÿæ•° â†’ æ¢¯åº¦ä¸Šå‡ â†’ å¢åŠ æ¦‚ç‡

# 6. åå‘ä¼ æ’­æ›´æ–°å‚æ•°
# ç»“æœï¼šä¸‹æ¬¡é‡åˆ°ç±»ä¼¼queryï¼Œæ›´å¯èƒ½ç”Ÿæˆç±»ä¼¼çš„å¥½æ”¹å†™
```

å¦‚æœrewardæ˜¯è´Ÿæ•°ï¼ˆ32Bè¡¨ç°å·®ï¼‰ï¼š

```python
reward = -0.3  # è´Ÿå¥–åŠ±
Advantage = -0.3 - 0.5 = -0.8  # è´Ÿæ•°ï¼æ¯”æœŸæœ›å·®

# PPOä¼šé™ä½è¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡
# ä¸‹æ¬¡é‡åˆ°ç±»ä¼¼queryï¼Œä¼šå°è¯•ç”Ÿæˆä¸åŒçš„æ”¹å†™
```

#### 3.6.5 VERLæ¡†æ¶å®ç°

åœ¨å®é™…ä»£ç ä¸­ï¼ŒVERLæ¡†æ¶å¸®æˆ‘ä»¬å¤„ç†äº†å¤§éƒ¨åˆ†ç»†èŠ‚ï¼š

```python
# ä½¿ç”¨VERLçš„PPO Trainer

from verl.trainer.ppo import PPOTrainer
from verl.utils.reward_score import RewardFunction

# 1. é…ç½®PPOå‚æ•°
ppo_config = {
    "ppo_epochs": 4,           # æ¯æ‰¹æ•°æ®æ›´æ–°4æ¬¡
    "clip_range": 0.2,         # clipèŒƒå›´ [0.8, 1.2]
    "value_loss_coef": 0.5,    # value lossæƒé‡
    "entropy_coef": 0.01,      # entropyæƒé‡
    "max_grad_norm": 0.5,      # æ¢¯åº¦è£å‰ªé˜ˆå€¼
    "learning_rate": 1e-5,     # å­¦ä¹ ç‡
    "gamma": 0.99,             # æœªæ¥å¥–åŠ±æŠ˜æ‰£å› å­
    "gae_lambda": 0.95         # GAEä¼˜åŠ¿ä¼°è®¡å‚æ•°
}

# 2. åˆ›å»ºtrainer
trainer = PPOTrainer(
    actor_model=qwen32b_sft_model,      # å¾…è®­ç»ƒçš„32Bæ¨¡å‹
    critic_model=None,                   # VERLè‡ªåŠ¨åˆ›å»ºcritic
    reward_fn=MultiDimensionalReward(),  # æˆ‘ä»¬çš„rewardå‡½æ•°
    config=ppo_config
)

# 3. è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    # æ”¶é›†ç»éªŒï¼ˆæ‰¹é‡ç”Ÿæˆæ”¹å†™+è·å–rewardï¼‰
    trajectories = await collect_trajectories(
        num_samples=256,
        actor_model=trainer.actor
    )
  
    # PPOæ›´æ–°ï¼ˆVERLè‡ªåŠ¨å¤„ç†æ‰€æœ‰ç»†èŠ‚ï¼‰
    metrics = trainer.update(trajectories)
  
    # ç›‘æ§
    print(f"Policy Loss: {metrics['policy_loss']:.4f}")
    print(f"Value Loss: {metrics['value_loss']:.4f}")
    print(f"Avg Reward: {metrics['avg_reward']:.4f}")
```

#### 3.6.6 å…³é”®è¶…å‚æ•°

| å‚æ•°                  | å€¼    | è¯´æ˜                       |
| --------------------- | ----- | -------------------------- |
| learning_rate         | 1e-5  | Actorå­¦ä¹ ç‡ï¼ˆå°å¿ƒè°ƒæ•´ï¼ï¼‰  |
| critic_learning_rate  | 1e-4  | Criticå­¦ä¹ ç‡ï¼ˆé€šå¸¸>actorï¼‰ |
| ppo_epochs            | 4     | æ¯æ‰¹æ•°æ®æ›´æ–°æ¬¡æ•°           |
| clip_range            | 0.2   | PPO clipèŒƒå›´               |
| batch_size            | 32-64 | æ¯æ‰¹æ ·æœ¬æ•°                 |
| gradient_accumulation | 4-8   | æ¢¯åº¦ç´¯ç§¯æ­¥æ•°               |
| max_grad_norm         | 0.5   | æ¢¯åº¦è£å‰ªé˜ˆå€¼               |

#### 3.6.7 è®­ç»ƒç›‘æ§è¦ç‚¹

```python
# å¥åº·çš„è®­ç»ƒåº”è¯¥çœ‹åˆ°ï¼š
wandb.log({
    "avg_reward": 0.3 â†’ 0.5 â†’ 0.65,      # é€æ­¥æå‡
    "policy_loss": -0.2 â†’ -0.15,         # é€æ¸å‡å°ï¼ˆç»å¯¹å€¼ï¼‰
    "value_loss": 0.5 â†’ 0.3 â†’ 0.15,      # é€æ¸å‡å°
    "qwen32b_win_rate": 0.3 â†’ 0.5 â†’ 0.7,  # èƒœç‡æå‡
    "clip_fraction": 0.1-0.3              # 10-30%çš„æ ·æœ¬è¢«clipï¼ˆæ­£å¸¸ï¼‰
})

# âš ï¸ å¼‚å¸¸æƒ…å†µï¼š
# - rewardä¸‹é™ï¼šå¯èƒ½å­¦ä¹ ç‡å¤ªå¤§
# - clip_fraction > 0.5ï¼šæ›´æ–°å¤ªæ¿€è¿›ï¼Œé™ä½å­¦ä¹ ç‡
# - value_lossä¸é™ï¼šCriticè®­ç»ƒæœ‰é—®é¢˜
```

### 3.7 è®­ç»ƒå¯åŠ¨

```bash
# 1. å¯åŠ¨RAGæœåŠ¡ï¼ˆä¸€ä¸ªç»ˆç«¯ï¼‰
cd sales-rag
python startup.py -a

# 2. å¯åŠ¨RLè®­ç»ƒï¼ˆå¦ä¸€ä¸ªç»ˆç«¯ï¼‰
cd code
python -m verl.trainer.main_ppo \
    --config config/qwen32b_rl_config.yaml \
    --model_path output/sft_qwen32b_fivedoctors \
    --rag_api_url http://localhost:8000/api/chat/general_rag \
    --gpt5_api_key sk-xxx \
    --max_concurrent_rag_calls 10 \
    --num_epochs 10

# è®­ç»ƒç›‘æ§
wandb login
# åœ¨ https://wandb.ai æŸ¥çœ‹è®­ç»ƒæ›²çº¿
```

---

## 4ï¸âƒ£ å…³é”®ä¼˜åŒ–ç­–ç•¥

### 4.1 æˆæœ¬æ§åˆ¶

**GPT-5 APIè°ƒç”¨æˆæœ¬ä¼˜åŒ–**ï¼š

1. **ç¼“å­˜æœºåˆ¶**ï¼šç›¸åŒqueryç¼“å­˜æ”¹å†™ç»“æœ
2. **é‡‡æ ·ç­–ç•¥**ï¼šä¸æ˜¯æ¯ä¸ªæ ·æœ¬éƒ½è°ƒç”¨GPT-5è¯„åˆ†ï¼ˆé‡‡æ ·30%ï¼‰
3. **æ‰¹é‡è°ƒç”¨**ï¼šä½¿ç”¨batch APIï¼ˆå¦‚æœæ”¯æŒï¼‰

**é¢„æœŸæˆæœ¬**ï¼š

- SFTæ•°æ®ç”Ÿæˆï¼š500æ¡ Ã— $0.01 = $5
- RLè®­ç»ƒï¼ˆ10 epochsï¼Œ1000æ¡ï¼‰ï¼š30%é‡‡æ · = 3000æ¬¡è°ƒç”¨ Ã— $0.01 = $30
- **æ€»è®¡**ï¼šçº¦$35-50

### 4.2 è®­ç»ƒæ•ˆç‡

**å¹¶å‘æ§åˆ¶**ï¼š

- RAG APIå¹¶å‘ï¼š10-20ä¸ªè¯·æ±‚
- GPT-5 APIå¹¶å‘ï¼š5-10ä¸ªè¯·æ±‚ï¼ˆé¿å…é™æµï¼‰

**ç¼“å­˜ç­–ç•¥**ï¼š

- æ£€ç´¢ç»“æœç¼“å­˜ï¼šå‘½ä¸­ç‡30-40%
- GPT-5æ”¹å†™ç¼“å­˜ï¼šå‘½ä¸­ç‡50-60%ï¼ˆbaselineç¨³å®šï¼‰

### 4.3 ç›‘æ§æŒ‡æ ‡

```python
# WandBç›‘æ§
wandb.log({
    # æ ¸å¿ƒæŒ‡æ ‡
    "avg_reward": avg_reward,
    "qwen32b_win_rate": wins / total,  # 32Bèƒœç‡
    "qwen32b_avg_top1": avg_32b_top1,
    "gpt5_avg_top1": avg_gpt5_top1,
  
    # APIè°ƒç”¨
    "rag_api_calls": total_calls,
    "gpt5_api_calls": gpt5_calls,
    "cache_hit_rate": cache_hits / total_calls,
  
    # è®­ç»ƒè¿›åº¦
    "policy_loss": policy_loss,
    "value_loss": value_loss
})
```

---

## 5ï¸âƒ£ å®Œæ•´è®­ç»ƒæµç¨‹

```bash
# æ€»æ—¶é—´ï¼šçº¦7-10å¤©

# æ­¥éª¤1: æ•°æ®å‡†å¤‡ï¼ˆ1å¤©ï¼‰
python generate_training_data.py      # GPT-5ç”Ÿæˆæ”¹å†™ï¼ˆ30åˆ†é’Ÿï¼‰
python validate_with_rag.py          # RAGéªŒè¯ï¼ˆ20åˆ†é’Ÿï¼‰
python convert_to_training_format.py # æ ¼å¼è½¬æ¢ï¼ˆ1åˆ†é’Ÿï¼‰

# æ­¥éª¤2: SFTè®­ç»ƒï¼ˆ3-4å¤©ï¼‰
bash train_sft_qwen32b.sh

# æ­¥éª¤3: RLè®­ç»ƒï¼ˆ3-5å¤©ï¼‰
# ç»ˆç«¯1: å¯åŠ¨RAG
cd sales-rag && python startup.py -a

# ç»ˆç«¯2: å¯åŠ¨RLè®­ç»ƒ
python rl_train.py

# æ­¥éª¤4: è¯„ä¼°ä¸éƒ¨ç½²ï¼ˆ1å¤©ï¼‰
python evaluate_model.py
bash deploy_model.sh
```

---

## 6ï¸âƒ£ é¢„æœŸæ•ˆæœ

| æŒ‡æ ‡            | GPT-5 (Teacher) | Qwen-32B (SFTå) | Qwen-32B (RLå) |
| --------------- | --------------- | ---------------- | --------------- |
| æ”¹å†™è´¨é‡è¯„åˆ†    | 4.8/5           | 4.3/5            | 4.6/5           |
| æ£€ç´¢Top-1å‡†ç¡®ç‡ | 85%             | 78%              | 88%             |
| æ¨ç†å»¶è¿Ÿ        | 2000ms          | 850ms            | 900ms           |
| æˆæœ¬/1000æ¬¡     | $25 | $2.50     | $2.80            |                 |

**æ ¸å¿ƒç›®æ ‡**ï¼š

- âœ… æ”¹å†™è´¨é‡æ¥è¿‘GPT-5ï¼ˆ4.6 vs 4.8ï¼‰
- âœ… æ£€ç´¢æ•ˆæœè¶…è¶ŠGPT-5ï¼ˆ88% vs 85%ï¼‰
- âœ… æˆæœ¬é™ä½90%ï¼ˆ$2.80 vs $25ï¼‰
- âœ… å»¶è¿Ÿé™ä½55%ï¼ˆ900ms vs 2000msï¼‰

---

## 7ï¸âƒ£ å¸¸è§é—®é¢˜

**Q1: ä¸ºä»€ä¹ˆä¸ç”¨DeepSeek V3.1åšTeacherï¼Ÿ**

- A: GPT-5å’ŒDeepSeek V3.1éƒ½å¯ä»¥ï¼Œå»ºè®®å…ˆæµ‹è¯•æ”¹å†™è´¨é‡ï¼Œé€‰æ‹©æ›´å¥½çš„

**Q2: Qwen-32Bèƒ½è¶…è¶ŠGPT-5å—ï¼Ÿ**

- A: åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚ä¿å¥å“ï¼‰é€šè¿‡RLè®­ç»ƒï¼Œæœ‰å¯èƒ½åœ¨æ£€ç´¢æ•ˆæœä¸Šè¶…è¶ŠGPT-5

**Q3: 4å¼ A100å¤Ÿè®­ç»ƒå—ï¼Ÿ**

- A: LoRAè®­ç»ƒå¤Ÿç”¨ï¼Œå…¨é‡å¾®è°ƒéœ€è¦8å¼ A100

**Q4: RLè®­ç»ƒä¼šä¸ä¼šè¿‡æ‹Ÿåˆï¼Ÿ**

- A: é€šè¿‡éªŒè¯é›†ç›‘æ§ï¼Œè®¾ç½®early stoppingï¼Œé¿å…è¿‡æ‹Ÿåˆ

**Q5: GPT-5 APIæˆæœ¬å¤ªé«˜æ€ä¹ˆåŠï¼Ÿ**

- A: ä½¿ç”¨ç¼“å­˜ã€é‡‡æ ·ã€æˆ–åˆ‡æ¢åˆ°DeepSeek V3.1ï¼ˆæˆæœ¬æ›´ä½ï¼‰

---

## ğŸ“š å‚è€ƒèµ„æ–™

- **ms-swiftæ–‡æ¡£**: https://swift.readthedocs.io/zh-cn/latest/
- **Qwen2.5æœ€ä½³å®è·µ**: https://swift.readthedocs.io/zh-cn/latest/BestPractices/Qwen3æœ€ä½³å®è·µ.html
- **VERLæ¡†æ¶**: https://github.com/volcengine/verl
- **PPOç®—æ³•**: https://arxiv.org/abs/1707.06347
