# âš ï¸  å·²åºŸå¼ƒ - åŒæ¨¡å‹å¯¹æ¯”é…ç½®
# æ­¤é…ç½®æ–‡ä»¶åŸºäºé”™è¯¯çš„åŒæ¨¡å‹å¯¹æ¯”æ¶æ„ï¼Œå·²è¢«æ··åˆGRPOæ¶æ„æ›¿ä»£
# è¯·ä½¿ç”¨æ–°çš„æ··åˆGRPOé…ç½®ï¼šalgorithm.hybrid_grpo
# å¤‡ä»½æ—¶é—´: 20251113_181713
# æ›¿ä»£æ–‡ä»¶: verl_code/verl/trainer/config/ppo_trainer.yaml (algorithm.hybrid_grpoéƒ¨åˆ†)

# SalesRAG GRPOåŒæ¨¡å‹è®­ç»ƒé…ç½®æ–‡ä»¶ v2.0

# specify the default per-component configs
# æ‰€æœ‰defaultsç»§æ‰¿éƒ½å·²æ³¨é‡Šï¼Œä½¿ç”¨å®Œå…¨è‡ªå®šä¹‰é…ç½®
defaults:
  - _self_

# åŸæœ‰çš„defaultsç»§æ‰¿ï¼ˆå·²å…¨éƒ¨æ³¨é‡Šæ‰ï¼‰ï¼š
# - actor@actor_rollout_ref.actor: dp_actor
# - data@data: legacy_data
# - ref@actor_rollout_ref.ref: dp_ref
# - rollout@actor_rollout_ref.rollout: rollout
# - model@actor_rollout_ref.model: hf_model
# - critic@critic: dp_critic
# - reward_model@reward_model: dp_reward_model
# - algorithm@@algorithm.rollout_correction: rollout_correction

# config for actor, rollout and reference model - è¦†ç›–defaultsä¸­çš„é…ç½®
actor_rollout_ref:

  # Whether it's a hybrid engine, currently only supports hybrid engine
  hybrid_engine: true

  # Timeout for operations executed against the process group
  nccl_timeout: 1800

  # Modelé…ç½®
  model:
    _target_: verl.workers.config.HFModelConfig
    path: "/home/jovyan2/query_rl/output/qwen3-8b-lora-sft/v3-20251031-111238/checkpoint-159-merged"
    hf_config_path: null
    tokenizer_path: null
    use_shm: false
    trust_remote_code: false
    custom_chat_template: null
    external_lib: null
    override_config: {}
    enable_gradient_checkpointing: true
    enable_activation_offload: false
    use_remove_padding: false
    lora_rank: 16
    lora_alpha: 32
    target_modules: all-linear
    exclude_modules: null
    lora_adapter_path: null
    use_liger: false
    use_fused_kernels: false
    fused_kernel_options:
      impl_backend: torch
    
  # Actoré…ç½®
  actor:
    _target_: verl.workers.config.FSDPActorConfig
    strategy: fsdp
    ppo_mini_batch_size: 128
    ppo_micro_batch_size: null
    ppo_micro_batch_size_per_gpu: 16
    use_dynamic_bsz: false
    ppo_max_token_len_per_gpu: 16384
    ppo_epochs: 1
    clip_ratio: 0.2
    clip_ratio_low: 0.2
    clip_ratio_high: 0.2
    clip_ratio_c: 3.0
    loss_agg_mode: token-mean
    entropy_coeff: 0.01
    use_kl_loss: true
    kl_loss_coef: 0.001
    kl_loss_type: low_var_kl
    shuffle: false
    use_torch_compile: false
    freeze_vision_tower: false
    grad_clip: 1.0
    ulysses_sequence_parallel_size: 1
    entropy_from_logits_with_chunking: false
    entropy_checkpointing: false
    use_remove_padding: false
    optim:
      _target_: verl.workers.config.FSDPOptimizerConfig
      optimizer: AdamW
      optimizer_impl: torch.optim
      lr: 1e-6
      lr_warmup_steps_ratio: 0.0
      total_training_steps: -1
      weight_decay: 0.01
      lr_warmup_steps: -1
      betas: [0.9, 0.999]
      clip_grad: 1.0
      min_lr_ratio: 0.0
      num_cycles: 0.5
      lr_scheduler_type: constant
      warmup_style: null
      override_optimizer_config: null
    fsdp_config:
      _target_: verl.workers.config.FSDPEngineConfig
      wrap_policy:
        min_num_params: 0
      param_offload: false
      optimizer_offload: false
      offload_policy: false
      reshard_after_forward: true
      fsdp_size: -1
      forward_prefetch: false
      model_dtype: fp32
      use_orig_params: false
      ulysses_sequence_parallel_size: 1
      entropy_from_logits_with_chunking: false
      use_torch_compile: false
      entropy_checkpointing: false
      forward_only: false
      strategy: fsdp
    checkpoint:
      _target_: verl.trainer.config.CheckpointConfig
      save_contents: ['model', 'optimizer', 'extra']
      load_contents: ['model', 'optimizer', 'extra']
      async_save: false
    policy_loss:
      _target_: verl.workers.config.PolicyLossConfig
      loss_mode: "vanilla"
      clip_cov_ratio: 0.0002
      clip_cov_lb: 1.0
      clip_cov_ub: 5.0
      kl_cov_ratio: 0.0002
      ppo_kl_coef: 0.1
    profiler:
      _target_: verl.utils.profiler.ProfilerConfig
      tool: null
      enable: false
      all_ranks: false
      ranks: []
      save_path: null
      tool_config: null
  
  # Rollouté…ç½®
  rollout:
    _target_: verl.workers.config.RolloutConfig
    name: vllm
    mode: sync
    n: 5
    temperature: 0.7  # é™ä½temperatureï¼Œå‡å°‘éšæœºæ€§
    top_p: 0.9
    top_k: 50
    prompt_length: 1024
    response_length: 512
    dtype: bfloat16
    gpu_memory_utilization: 0.6  # ğŸ”¥ ä»0.45æ”¹ä¸º0.6ï¼Œæ›´å¤šå†…å­˜ç”¨äºKV cache
    ignore_eos: false
    enforce_eager: false
    cudagraph_capture_sizes: null
    free_cache_engine: true
    tensor_model_parallel_size: 1
    data_parallel_size: 1
    expert_parallel_size: 1
    pipeline_model_parallel_size: 1
    max_num_batched_tokens: 4096
    max_model_len: null
    max_num_seqs: 512
    enable_chunked_prefill: true
    enable_prefix_caching: true
    load_format: dummy
    log_prob_micro_batch_size: null
    log_prob_micro_batch_size_per_gpu: 32
    log_prob_use_dynamic_bsz: false
    log_prob_max_token_len_per_gpu: 16384
    disable_log_stats: true
    do_sample: true
    over_sample_rate: 0
    multi_stage_wake_up: false
    engine_kwargs:
      vllm: {}
      sglang: {}
    val_kwargs:
      _target_: verl.workers.config.SamplingConfig
      top_k: 50
      top_p: 0.9
      temperature: 0.7
      n: 3
      do_sample: true
    calculate_log_probs: false
    skip_rollout: false
    skip_tokenizer_init: true
    layered_summon: false
  
  # Referenceæ¨¡å‹é…ç½®
  ref:
    _target_: verl.workers.config.FSDPEngineConfig
    strategy: fsdp
    use_torch_compile: false
    log_prob_micro_batch_size: null
    log_prob_micro_batch_size_per_gpu: 32
    log_prob_use_dynamic_bsz: false
    log_prob_max_token_len_per_gpu: 16384
    fsdp_config:
      _target_: verl.workers.config.FSDPEngineConfig
      wrap_policy:
        min_num_params: 0
      param_offload: true
      optimizer_offload: false
      offload_policy: false
      reshard_after_forward: true
      fsdp_size: -1
      forward_prefetch: false
      model_dtype: fp32
      use_orig_params: false
      ulysses_sequence_parallel_size: 1
      entropy_from_logits_with_chunking: false
      use_torch_compile: false
      entropy_checkpointing: false
      forward_only: true
      strategy: fsdp
    model: null
    ulysses_sequence_parallel_size: 1
    entropy_from_logits_with_chunking: false
    entropy_checkpointing: false

# critic configuration - GRPOä¸éœ€è¦ä¼ ç»Ÿcritic
critic:
  _target_: verl.workers.config.CriticConfig
  enable: false
  rollout_n: 1
  strategy: fsdp
  optim:
    lr: 1e-5
    lr_warmup_steps_ratio: 0.0
    total_training_steps: -1
    weight_decay: 0.01
    lr_warmup_steps: -1
  model:
    path: ${actor_rollout_ref.model.path}
    tokenizer_path: ${actor_rollout_ref.model.path}
    override_config: {}
    external_lib: null
    trust_remote_code: false
  ppo_mini_batch_size: 256
  ppo_micro_batch_size: null
  ppo_micro_batch_size_per_gpu: null
  use_dynamic_bsz: false
  ppo_max_token_len_per_gpu: 32768
  forward_max_token_len_per_gpu: 32768
  ppo_epochs: 1
  shuffle: false
  cliprange_value: 0.5
  loss_agg_mode: token-mean
  checkpoint:
    _target_: verl.trainer.config.CheckpointConfig
    save_contents: ['model', 'optimizer', 'extra']
    load_contents: ['model', 'optimizer', 'extra']
    async_save: false
  profiler:
    _target_: verl.utils.profiler.ProfilerConfig
    tool: null
    enable: false
    all_ranks: false
    ranks: []
    save_path: null
    tool_config:
      nsys:
        _target_: verl.utils.profiler.config.NsightToolConfig
        discrete: false
      npu:
        _target_: verl.utils.profiler.config.NPUToolConfig
        contents: []
        level: "level1"
        analysis: true
        discrete: false
      torch:
        _target_: verl.utils.profiler.config.TorchProfilerToolConfig
        step_start: 0
        step_end: null
      torch_memory:
        _target_: verl.utils.profiler.config.TorchMemoryToolConfig
        trace_alloc_max_entries: 100000
        stack_depth: 32

# reward model configuration - ä½¿ç”¨è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ï¼Œä¸éœ€è¦å•ç‹¬çš„RM
reward_model:
  enable: false
  enable_resource_pool: false
  n_gpus_per_node: 0
  nnodes: 0
  strategy: fsdp
  model:
    input_tokenizer: null
    path: null
    external_lib: null
    trust_remote_code: false
  micro_batch_size: null
  micro_batch_size_per_gpu: null
  max_length: null
  use_dynamic_bsz: false
  forward_max_token_len_per_gpu: 16384
  reward_manager: naive
  launch_reward_fn_async: false
  sandbox_fusion:
    url: null
    max_concurrent: 64
    memory_limit_mb: 1024
  profiler:
    _target_: verl.utils.profiler.ProfilerConfig
    tool: null
    enable: false
    all_ranks: false
    ranks: []
    save_path: null
    tool_config: null

# custom reward function definition
custom_reward_function:

  # ä½¿ç”¨æˆ‘ä»¬çš„åŒæ¨¡å‹å¥–åŠ±è®¡ç®—å™¨
  path: verl/utils/reward_score/sales_rag.py
  name: compute_score

# config for the algorithm
algorithm:

  # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
  _target_: verl.trainer.config.AlgoConfig

  # Discount factor for future rewards
  gamma: 1.0

  # Trade-off between bias and variance in the GAE estimator
  lam: 1.0

  # ä½¿ç”¨GRPOä¼˜åŠ¿ä¼°è®¡å™¨ï¼ˆå®˜æ–¹æ¨èï¼‰
  adv_estimator: grpo

  # Whether to normalize advantages by std
  norm_adv: True

  # Whether to enable in-reward KL penalty
  use_kl_in_reward: False

  # How to estimate KL divergence: "kl", "abs", "mse", "low_var_kl", or "full"
  kl_penalty: kl

  # KL control configuration
  kl_ctrl:

    # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
    _target_: verl.trainer.config.KLControlConfig

    # KL control type: "fixed" or "adaptive"
    type: fixed

    # Initial coefficient for KL penalty
    kl_coef: 0.001

    # Horizon value for adaptive controller (if enabled)
    horizon: 10000

    # Target KL divergence (used for adaptive controller)
    target_kl: 0.1

  # Whether to enable preference feedback PPO
  use_pf_ppo: False

  # Preference feedback PPO settings
  pf_ppo:

    # Method for reweighting samples: "pow", "max_min", or "max_random"
    reweight_method: pow

    # Power used for weight scaling in "pow" method
    weight_pow: 2.0

  # Rollout correction configuration (GRPOä¸ä½¿ç”¨ï¼Œä½†ä¿ç•™é»˜è®¤å€¼)
  rollout_correction:
    rollout_is: null
    rollout_is_threshold: 2.0
    rollout_rs: null
    rollout_rs_threshold: null
    rollout_rs_threshold_lower: null
    rollout_token_veto_threshold: null
    bypass_old_logprob_for_rollout: false
    use_pure_rollout_correction: false

  # ç»„å†…æœ€ä¼˜é€‰æ‹©é…ç½®
  select_best_from_group: true

  # æ··åˆGRPOé…ç½®
  hybrid_grpo:
    enable: true
    gpt5_weight: 0.85
    grpo_weight: 0.15
    scoring_model: "GPT-5"
    scoring_dimensions:
      - "è´¨é‡æå‡åº¦"
      - "ç›¸å…³æ€§å‡†ç¡®æ€§"
      - "ä¿¡æ¯å®Œæ•´æ€§"
      - "æ£€ç´¢æœ‰æ•ˆæ€§"
    enable_dynamic_weight: false
    weight_decay_rate: 0.4
    min_auxiliary_weight: 0.1
    group_size: 3  # ğŸ”¥ ä»5æ”¹ä¸º3ï¼Œä¸rollout.nä¿æŒä¸€è‡´

# config for the trainer
trainer:

  # Whether to balance batch sizes across distributed workers
  balance_batch: True

  # Number of epochs in training
  total_epochs: 7

  # Total training steps (can be set explicitly or derived from epochs)
  total_training_steps: null

  # Project name for experiment tracking
  project_name: 'sales_rag_grpo_dual'

  # Experiment name for run identification in tracking tools
  experiment_name: 'qwen8b_grpo_dual_v1'

  # Logging backends to use
  logger: ["console"]

  # Number of generations to log during validation
  log_val_generations: 10

  # Directory for logging rollout data
  rollout_data_dir: logs/rollout_data

  # Directory for logging validation data
  validation_data_dir: logs/validation_data

  # Number of nodes used in the training
  nnodes: 1

  # Number of GPUs per node
  n_gpus_per_node: 1

  # Save frequency (by iteration) for model checkpoints
  save_freq: 100

  # Validation frequency (in training iterations)
  test_freq: 2

  # Number of iterations to warm up the critic before updating policy
  critic_warmup: 0

  # Default path to distributed filesystem for saving checkpoints
  default_hdfs_dir: null

  # Whether to delete local checkpoints after loading
  del_local_ckpt_after_load: False

  # Default local directory for saving checkpoints
  default_local_dir: checkpoints/SalesRAG_GRPO/query_rewrite_dual_model

  # Maximum number of actor checkpoints to keep
  max_actor_ckpt_to_keep: 5

  # Maximum number of critic checkpoints to keep
  max_critic_ckpt_to_keep: 5

  # Timeout (in seconds) for Ray worker to wait for registration
  ray_wait_register_center_timeout: 600

  # Device to run training on
  device: cuda

  # whether to use legacy worker implementation
  use_legacy_worker_impl: auto

  # Resume mode: "auto", "disable", or "must"
  resume_mode: auto

  # Path to resume from (if resume_mode is not "disable")
  resume_from_path: null

  # Whether to run validation only (no training)
  val_only: false

  # Whether to run validation before training starts
  val_before_train: false

  # ESI redundant time
  esi_redundant_time: 0

# æ•°æ®é…ç½®
data:
  tokenizer: null
  use_shm: false
  train_files: ["../data/sales_rag/train.parquet"]
  val_files: ["../data/sales_rag/val.parquet"]
  train_max_samples: -1
  val_max_samples: -1
  prompt_key: prompt
  reward_fn_key: data_source
  data_source: sales_rag_hybrid  # æ•°æ®æºæ ‡è¯†
  max_prompt_length: 1024
  max_response_length: 512
  train_batch_size: 128  # ğŸ”¥ ä»256æ”¹ä¸º128ï¼ŒåŠ å¿«é¦–æ¬¡è¿­ä»£
  val_batch_size: null
  tool_config_path: null
  return_raw_input_ids: false
  return_raw_chat: false
  return_full_prompt: false
  shuffle: true
  seed: null
  dataloader_num_workers: 16  # ğŸ”¥ ä»8æ”¹ä¸º16ï¼ŒåŠ å¿«æ•°æ®åŠ è½½
  image_patch_size: 14
  validation_shuffle: false
  filter_overlong_prompts: false
  filter_overlong_prompts_workers: 1
  truncation: error
  image_key: images
  video_key: videos
  trust_remote_code: false
  custom_cls:
    path: null
    name: null
  return_multi_modal_inputs: true
  sampler:
    class_path: null
    class_name: null
  datagen:
    path: null
    name: null
  apply_chat_template_kwargs: 
    enable_thinking: false  # ç¦ç”¨thinkingæ¨¡å¼ï¼Œç›´æ¥ç”ŸæˆJSON

# profiler configs
global_profiler:

  # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
  _target_: verl.utils.profiler.ProfilerConfig

  # Profiling tool: choose between nsys, npu, torch, torch_memory
  tool: null

  # profile steps
  steps: null

  # Whether to combine continuous steps into one database
  profile_continuous_steps: False

  # Path to save profiling contents
  save_path: "outputs/profile"

# configs for TransferQueue
transfer_queue:

  # Whether to enable transfer queue
  enable: False

# configs related to ray
ray_kwargs:

  # configs related to ray initialization
  ray_init:

    # Number of CPUs for Ray. Use a fixed number instead of null when using SLURM.
    num_cpus: 8  # å•GPUæ¨¡å¼è®¾ç½®å›ºå®šCPUæ•°

  # Path to save Ray timeline JSON for performance profiling
  timeline_json_file: null

# RAGé…ç½®
rag:
  endpoints:
    chat_8b: "/rag/chat_8b"
    chat: "/rag/chat"
  
  connection:
    timeout: 30
    max_retries: 3
    retry_delay: 1.0
  
  retrieval:
    top_k: 5
    score_threshold: 0.6
    max_document_length: 2000

# è¯„åˆ†é…ç½®
scoring:
  gpt5:
    api_timeout: 60
    max_retries: 2
    temperature: 0.3
    max_tokens: 2000
  
  cache:
    enable: true
    cache_dir: "cache/scoring"
    max_cache_size: 10000
    ttl_seconds: 3600

# æ€§èƒ½ä¼˜åŒ–é…ç½®
performance:
  memory:
    gradient_checkpointing: true
    mixed_precision: true
  
  compute:
    flash_attention: true
  
  io:
    prefetch_batches: 2
    async_data_loading: true

# è°ƒè¯•é…ç½®
debug:
  enable: false
  log_level: "INFO"
  verbose_logging: {
    "model_output": false,
    "rag_calls": false,
    "scoring": false,
    "reward_calculation": false
  }