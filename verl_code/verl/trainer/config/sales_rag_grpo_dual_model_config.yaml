# ⚠️  已废弃 - 双模型对比配置
# 此配置文件基于错误的双模型对比架构，已被混合GRPO架构替代
# 请使用新的混合GRPO配置：algorithm.hybrid_grpo
# 备份时间: 20251113_181713
# 替代文件: verl_code/verl/trainer/config/ppo_trainer.yaml (algorithm.hybrid_grpo部分)

# SalesRAG GRPO双模型训练配置文件 v2.0

# specify the default per-component configs
defaults:

  # Actor model config
  - actor@actor_rollout_ref.actor: dp_actor

  # Data config
  - data@data: legacy_data

  # Reference model config
  - ref@actor_rollout_ref.ref: dp_ref

  # Rollout model config
  - rollout@actor_rollout_ref.rollout: rollout

  # Model config
  - model@actor_rollout_ref.model: hf_model

  # Critic model config
  - critic@critic: dp_critic

  # Reward model config
  - reward_model@reward_model: dp_reward_model

  # Rollout correction config
  - algorithm@@algorithm.rollout_correction: rollout_correction

  # load the reference default config, then apply the fields in the current yaml
  - _self_

# config for actor, rollout and reference model - 覆盖defaults中的配置
actor_rollout_ref:

  # Whether it's a hybrid engine, currently only supports hybrid engine
  hybrid_engine: true

  # Timeout for operations executed against the process group
  nccl_timeout: 1800

  # Actor模型配置
  model:
    path: "/home/jovyan2/query_rl/model/Qwen3-8B"  # 本地8B模型路径
    enable_gradient_checkpointing: true
    use_flash_attention: true
    
  actor:
    strategy: fsdp
    fsdp_config:
      mixed_precision: bf16
      fsdp_size: 1  # 单GPU模式
      forward_only: false
      use_torch_compile: false  # 单GPU时禁用torch_compile避免问题
    ppo_mini_batch_size: 128
    ppo_micro_batch_size: 64
    grad_clip: 1.0
    clip_ratio: 0.2
    
    # GRPO核心配置
    use_kl_loss: true
    kl_loss_coef: 0.001
    kl_loss_type: "low_var_kl"
    
    optim:
      lr: 1e-6
      weight_decay: 0.01
  
  rollout:
    name: vllm
    n: 5  # 每个prompt生成5个输出
    temperature: 0.8
    top_p: 0.9
    max_new_tokens: 512
    log_prob_micro_batch_size_per_gpu: 32
    tensor_model_parallel_size: 2
    gpu_memory_utilization: 0.6
  
  # 参考模型配置 - 按照官方example调整
  ref:
    model:
      path: "/home/jovyan2/query_rl/model/Qwen3-8B"  # 使用本地模型作为参考
      enable_gradient_checkpointing: true
      use_remove_padding: true
    log_prob_micro_batch_size: 32
    fsdp_config:
      param_offload: true

# critic configuration - GRPO不需要传统critic
critic:
  enable: false

# custom reward function definition
custom_reward_function:

  # 使用我们的双模型奖励计算器
  path: verl/utils/reward_score/sales_rag.py
  name: compute_score

# config for the algorithm
algorithm:

  # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
  _target_: verl.trainer.config.AlgoConfig

  # Discount factor for future rewards
  gamma: 1.0

  # Trade-off between bias and variance in the GAE estimator
  lam: 1.0

  # 使用GRPO优势估计器（官方推荐）
  adv_estimator: grpo

  # Whether to normalize advantages by std
  norm_adv: True

  # Whether to enable in-reward KL penalty
  use_kl_in_reward: False

  # How to estimate KL divergence: "kl", "abs", "mse", "low_var_kl", or "full"
  kl_penalty: kl

  # KL control configuration
  kl_ctrl:

    # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
    _target_: verl.trainer.config.KLControlConfig

    # KL control type: "fixed" or "adaptive"
    type: fixed

    # Initial coefficient for KL penalty
    kl_coef: 0.001

    # Horizon value for adaptive controller (if enabled)
    horizon: 10000

    # Target KL divergence (used for adaptive controller)
    target_kl: 0.1

  # Whether to enable preference feedback PPO
  use_pf_ppo: False

  # Preference feedback PPO settings
  pf_ppo:

    # Method for reweighting samples: "pow", "max_min", or "max_random"
    reweight_method: pow

    # Power used for weight scaling in "pow" method
    weight_pow: 2.0

# config for the trainer
trainer:

  # Whether to balance batch sizes across distributed workers
  balance_batch: True

  # Number of epochs in training
  total_epochs: 7

  # Total training steps (can be set explicitly or derived from epochs)
  total_training_steps: null

  # Project name for experiment tracking
  project_name: 'sales_rag_grpo_dual'

  # Experiment name for run identification in tracking tools
  experiment_name: 'qwen8b_grpo_dual_v1'

  # Logging backends to use
  logger: ["console", "wandb"]

  # Number of generations to log during validation
  log_val_generations: 10

  # Directory for logging rollout data
  rollout_data_dir: logs/rollout_data

  # Directory for logging validation data
  validation_data_dir: logs/validation_data

  # Number of nodes used in the training
  nnodes: 1

  # Number of GPUs per node
  n_gpus_per_node: 1

  # Save frequency (by iteration) for model checkpoints
  save_freq: 100

  # Validation frequency (in training iterations)
  test_freq: 2

  # Number of iterations to warm up the critic before updating policy
  critic_warmup: 0

  # Default path to distributed filesystem for saving checkpoints
  default_hdfs_dir: null

  # Whether to delete local checkpoints after loading
  del_local_ckpt_after_load: False

  # Default local directory for saving checkpoints
  default_local_dir: checkpoints/SalesRAG_GRPO/query_rewrite_dual_model

  # Maximum number of actor checkpoints to keep
  max_actor_ckpt_to_keep: 5

  # Maximum number of critic checkpoints to keep
  max_critic_ckpt_to_keep: 5

  # Timeout (in seconds) for Ray worker to wait for registration
  ray_wait_register_center_timeout: 600

  # Device to run training on
  device: cuda

  # whether to use legacy worker implementation
  use_legacy_worker_impl: auto

# 数据配置
data:
  train_files: ["/home/jovyan2/query_rl/data/sales_rag/train_dual_model.parquet"]
  train_batch_size: 256
  max_prompt_length: 1024
  max_response_length: 512
  val_files: ["/home/jovyan2/query_rl/data/sales_rag/train_dual_model.parquet"]
  
  # 数据源配置
  data_source:
    excel_path: "data/sales_rag/RL_tranning_data/橙啦-query_RL_训练集.xlsx"
    prompt_template_path: "src/prompts/query_rewrite_prompt.txt"
    processing_version: "v2_dual_rag"

# profiler configs
global_profiler:

  # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
  _target_: verl.utils.profiler.ProfilerConfig

  # Profiling tool: choose between nsys, npu, torch, torch_memory
  tool: null

  # profile steps
  steps: null

  # Whether to combine continuous steps into one database
  profile_continuous_steps: False

  # Path to save profiling contents
  save_path: "outputs/profile"

# configs for TransferQueue
transfer_queue:

  # Whether to enable transfer queue
  enable: False

# configs related to ray
ray_kwargs:

  # configs related to ray initialization
  ray_init:

    # Number of CPUs for Ray. Use a fixed number instead of null when using SLURM.
    num_cpus: 8  # 单GPU模式设置固定CPU数

  # Path to save Ray timeline JSON for performance profiling
  timeline_json_file: null

# RAG配置
rag:
  endpoints:
    chat_8b: "/rag/chat_8b"
    chat: "/rag/chat"
  
  connection:
    timeout: 30
    max_retries: 3
    retry_delay: 1.0
  
  retrieval:
    top_k: 5
    score_threshold: 0.6
    max_document_length: 2000

# 评分配置
scoring:
  gpt5:
    api_timeout: 60
    max_retries: 2
    temperature: 0.3
    max_tokens: 2000
  
  cache:
    enable: true
    cache_dir: "cache/scoring"
    max_cache_size: 10000
    ttl_seconds: 3600

# 性能优化配置
performance:
  memory:
    gradient_checkpointing: true
    mixed_precision: true
  
  compute:
    flash_attention: true
  
  io:
    prefetch_batches: 2
    async_data_loading: true

# 调试配置
debug:
  enable: false
  log_level: "INFO"
  verbose_logging: {
    "model_output": false,
    "rag_calls": false,
    "scoring": false,
    "reward_calculation": false
  }