

"""
Actoræ¨¡å‹ï¼ˆQwen-8Bï¼‰å¤„ç†å™¨ v2.0
ç”¨äºå¤„ç†SalesRAG Queryæ”¹å†™çš„GRPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒ
"""

import asyncio
import json
import logging
from typing import Dict, Any, Optional, Tuple
import time
from dataclasses import dataclass

from src.core.rag_chater import RagChater
from src.utils.log import logger

logger = logging.getLogger(__name__)


@dataclass
class ModelOutput:
    """æ¨¡å‹è¾“å‡ºæ•°æ®ç»“æ„"""
    user_profile: str = ""
    rewritten_query: str = ""
    history_summary: str = ""
    rag_recall: list = None
    rag_status: str = ""
    processing_time: float = 0.0
    success: bool = True
    error_message: str = ""


class ActorModelProcessor:
    """Actoræ¨¡å‹ï¼ˆQwen-8Bï¼‰å¤„ç†å™¨"""
    
    def __init__(self, model_name: str = "Qwen3-8B-Instruct"):
        """
        åˆå§‹åŒ–Actoræ¨¡å‹å¤„ç†å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.rag_client = None
        self._init_rag_client()
        
    def _init_rag_client(self):
        """åˆå§‹åŒ–RAGå®¢æˆ·ç«¯"""
        try:
            self.rag_client = RagChater(
                tenant_id="chengla",
                contact_id="chengla_query_rl_contact",
                account_id="chengla_query_rl_account",
                message_id="chengla_query_rl_message_id"
            )
            logger.info("RAGå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"RAGå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def process_sample(self, prompt: str, sample_data: dict) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªè®­ç»ƒæ ·æœ¬
        
        Args:
            prompt: å®Œæ•´çš„prompt
            sample_data: æ ·æœ¬æ•°æ®å­—å…¸
            
        Returns:
            dict: å¤„ç†ç»“æœ
        """
        try:
            start_time = time.time()
            
            # 1. æ¨¡å‹ç”Ÿæˆç»“æ„åŒ–è¾“å‡º
            logger.debug(f"å¼€å§‹å¤„ç†æ ·æœ¬: {sample_data.get('prompt_id', 'unknown')}")
            model_output = await self._generate_model_output(prompt)
            
            # 2. è§£æJSONè¾“å‡º
            parsed_output = self._parse_model_output(model_output)
            
            # 3. ğŸ”¥ è°ƒç”¨RAG /chat_8bæ¥å£
            rag_result = await self._call_rag_chat_8b(
                user_profile=parsed_output["user_profile"],
                rewritten_query=parsed_output["rewritten_query"],
                history_summary=parsed_output["history_summary"]
            )
            
            # 4. é‡æ–°ç»„åˆå®Œæ•´è¾“å‡ºæ ¼å¼
            complete_output = self._combine_complete_output(
                parsed_output, rag_result, sample_data, start_time
            )
            
            logger.info(f"æ ·æœ¬å¤„ç†æˆåŠŸ: {sample_data.get('prompt_id', 'unknown')}")
            return complete_output
            
        except Exception as e:
            logger.error(f"Actoræ¨¡å‹å¤„ç†å¤±è´¥: {e}")
            return self._get_error_output(sample_data, str(e))
    
    async def _generate_model_output(self, prompt: str) -> str:
        """
        ç”Ÿæˆæ¨¡å‹è¾“å‡º
        
        Args:
            prompt: è¾“å…¥prompt
            
        Returns:
            str: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
        """
        try:
            logger.debug("ç”Ÿæˆæ¨¡å‹è¾“å‡º")
            
            # ä½¿ç”¨æœ¬åœ°Qwen-8Bæ¨¡å‹ç”Ÿæˆè¾“å‡º
            import sys
            import os
            
            # æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(os.path.dirname(current_dir))  # å›åˆ°verl_codeç›®å½•
            src_path = os.path.join(project_root, '..', 'src')  # æŒ‡å‘ /home/jovyan2/query_rl/src
            
            if src_path not in sys.path:
                sys.path.insert(0, src_path)
            
            try:
                from src.utils.llm import get_chat_llm
            except ImportError as e:
                print(f"âš ï¸  å¯¼å…¥srcæ¨¡å—å¤±è´¥: {e}")
                # åˆ›å»ºæ¨¡æ‹Ÿå‡½æ•°ä»¥é¿å…å´©æºƒ
                def get_chat_llm(llm_name):
                    return lambda x: f"æ¨¡æ‹Ÿ{llm_name}å“åº”"
            
            llm = get_chat_llm("qwen3-8b")
            response = await llm.ainvoke(prompt)
            
            # éªŒè¯è¾“å‡ºæ ¼å¼ï¼Œç¡®ä¿æ˜¯æœ‰æ•ˆçš„JSON
            model_output = response.content
            
            # å°è¯•è§£æJSONï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
            try:
                parsed_output = json.loads(model_output)
                # ç¡®ä¿åŒ…å«å¿…éœ€å­—æ®µ
                required_fields = ["user_profile", "rewritten_query", "history_summary"]
                for field in required_fields:
                    if field not in parsed_output:
                        logger.warning(f"æ¨¡å‹è¾“å‡ºç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                        parsed_output[field] = ""
                
                # é‡æ–°åºåˆ—åŒ–ä¸ºJSONå­—ç¬¦ä¸²
                return json.dumps(parsed_output, ensure_ascii=False)
                
            except json.JSONDecodeError as json_error:
                logger.error(f"æ¨¡å‹è¾“å‡ºä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {json_error}")
                # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œè¿”å›ç»“æ„åŒ–çš„é»˜è®¤å€¼
                default_output = {
                    "user_profile": "",
                    "rewritten_query": "",
                    "history_summary": ""
                }
                return json.dumps(default_output, ensure_ascii=False)
            
        except Exception as e:
            logger.error(f"æ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤çš„JSONç»“æ„è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            default_output = {
                "user_profile": "",
                "rewritten_query": "",
                "history_summary": ""
            }
            return json.dumps(default_output, ensure_ascii=False)
    
    def _parse_model_output(self, model_output: str) -> Dict[str, str]:
        """
        è§£ææ¨¡å‹è¾“å‡ºçš„JSON
        
        Args:
            model_output: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
            
        Returns:
            dict: è§£æåçš„ç»“æ„åŒ–æ•°æ®
        """
        try:
            # å°è¯•è§£æJSON
            parsed = json.loads(model_output.strip())
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            required_fields = ["user_profile", "rewritten_query", "history_summary"]
            for field in required_fields:
                if field not in parsed:
                    parsed[field] = ""
            
            logger.debug(f"æ¨¡å‹è¾“å‡ºè§£ææˆåŠŸ: {list(parsed.keys())}")
            return parsed
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å€¼
            return {
                "user_profile": "",
                "rewritten_query": "",
                "history_summary": ""
            }
        except Exception as e:
            logger.error(f"æ¨¡å‹è¾“å‡ºè§£æå¤±è´¥: {e}")
            raise
    
    async def _call_rag_chat_8b(
        self, 
        user_profile: str, 
        rewritten_query: str, 
        history_summary: str
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨RAG /chat_8bæ¥å£
        
        Args:
            user_profile: ç”¨æˆ·ç”»åƒ
            rewritten_query: é‡å†™æŸ¥è¯¢
            history_summary: å†å²æ‘˜è¦
            
        Returns:
            dict: RAGè°ƒç”¨ç»“æœ
        """
        try:
            logger.debug("è°ƒç”¨RAG /chat_8bæ¥å£")
            
            # ä½¿ç”¨rag_chater.pyä¸­çš„chat_8bæ–¹æ³•
            rag_result = await self.rag_client.chat_8b(
                user_profile=user_profile,
                rewritten_query=rewritten_query,
                history_summary=history_summary,
                score_threshold=0.95
            )
            
            response_data, status, request_body, cost_time = rag_result
            
            result = {
                "response_data": response_data,
                "status": status,
                "request_body": request_body,
                "cost_time": cost_time,
                "success": True
            }
            
            logger.debug(f"RAG /chat_8bè°ƒç”¨æˆåŠŸï¼Œè€—æ—¶: {cost_time}s")
            return result
            
        except Exception as e:
            logger.error(f"RAG /chat_8bè°ƒç”¨å¤±è´¥: {e}")
            return self._get_error_rag_result(str(e))
    
    def _combine_complete_output(
        self, 
        parsed_output: Dict[str, str], 
        rag_result: Dict[str, Any], 
        sample_data: dict,
        start_time: float
    ) -> Dict[str, Any]:
        """
        ç»„åˆå®Œæ•´è¾“å‡ºæ ¼å¼
        
        Args:
            parsed_output: è§£æåçš„æ¨¡å‹è¾“å‡º
            rag_result: RAGè°ƒç”¨ç»“æœ
            sample_data: åŸå§‹æ ·æœ¬æ•°æ®
            start_time: å¼€å§‹æ—¶é—´
            
        Returns:
            dict: å®Œæ•´çš„è¾“å‡ºç»“æœ
        """
        try:
            processing_time = time.time() - start_time
            
            # æ„å»ºå®Œæ•´å“åº”
            complete_response = {
                "user_profile": parsed_output["user_profile"],
                "rewritten_query": parsed_output["rewritten_query"],
                "history_summary": parsed_output["history_summary"],
                "rag_recall": rag_result["response_data"] if rag_result["success"] else [],
                "rag_status": rag_result["status"],
                "processing_metadata": {
                    "model_name": self.model_name,
                    "endpoint": "/chat_8b",
                    "processing_time": processing_time,
                    "rag_cost_time": rag_result.get("cost_time", 0.0),
                    "success": rag_result["success"]
                }
            }
            
            result = {
                "prompt_id": sample_data.get("prompt_id", "unknown"),
                "complete_response": complete_response,
                "original_data": sample_data.get("parsed_data", {}),
                "model_output": parsed_output,
                "rag_result": rag_result,
                "processing_success": True,
                "total_processing_time": processing_time
            }
            
            logger.debug(f"å®Œæ•´è¾“å‡ºç»„åˆæˆåŠŸï¼Œæ€»è€—æ—¶: {processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"å®Œæ•´è¾“å‡ºç»„åˆå¤±è´¥: {e}")
            raise
    
    def _get_error_output(self, sample_data: dict, error_message: str) -> Dict[str, Any]:
        """è·å–é”™è¯¯è¾“å‡º"""
        return {
            "prompt_id": sample_data.get("prompt_id", "unknown"),
            "complete_response": {
                "user_profile": "",
                "rewritten_query": "",
                "history_summary": "",
                "rag_recall": [],
                "rag_status": "error",
                "processing_metadata": {
                    "model_name": self.model_name,
                    "endpoint": "/chat_8b",
                    "processing_time": 0.0,
                    "rag_cost_time": 0.0,
                    "success": False,
                    "error_message": error_message
                }
            },
            "original_data": sample_data.get("parsed_data", {}),
            "model_output": {},
            "rag_result": {"success": False, "error_message": error_message},
            "processing_success": False,
            "total_processing_time": 0.0,
            "error_message": error_message
        }
    
    def _get_error_rag_result(self, error_message: str) -> Dict[str, Any]:
        """è·å–RAGé”™è¯¯ç»“æœ"""
        return {
            "response_data": [],
            "status": "error",
            "request_body": {},
            "cost_time": 0.0,
            "success": False,
            "error_message": error_message
        }
    
    async def batch_process(self, samples: list, max_concurrency: int = 5) -> list:
        """
        æ‰¹é‡å¤„ç†æ ·æœ¬
        
        Args:
            samples: æ ·æœ¬åˆ—è¡¨
            max_concurrency: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            list: å¤„ç†ç»“æœåˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç†{len(samples)}ä¸ªæ ·æœ¬ï¼Œæœ€å¤§å¹¶å‘æ•°: {max_concurrency}")
            
            # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
            semaphore = asyncio.Semaphore(max_concurrency)
            
            async def process_with_semaphore(sample):
                async with semaphore:
                    return await self.process_sample(sample["prompt"], sample)
            
            # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
            tasks = [process_with_semaphore(sample) for sample in samples]
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†å¼‚å¸¸ç»“æœ
            processed_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"æ ·æœ¬{i}å¤„ç†å¼‚å¸¸: {result}")
                    processed_results.append(self._get_error_output(samples[i], str(result)))
                else:
                    processed_results.append(result)
            
            success_count = sum(1 for r in processed_results if r["processing_success"])
            logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸ: {success_count}/{len(processed_results)}")
            
            return processed_results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
            return []


class ActorModelManager:
    """Actoræ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self, model_name: str = "Qwen3-8B-Instruct"):
        """
        åˆå§‹åŒ–Actoræ¨¡å‹ç®¡ç†å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.processor = None
        
    def get_processor(self) -> ActorModelProcessor:
        """è·å–Actoræ¨¡å‹å¤„ç†å™¨å®ä¾‹"""
        if self.processor is None:
            self.processor = ActorModelProcessor(self.model_name)
        return self.processor
    
    async def process_training_batch(self, training_samples: list) -> list:
        """
        å¤„ç†è®­ç»ƒæ‰¹æ¬¡
        
        Args:
            training_samples: è®­ç»ƒæ ·æœ¬åˆ—è¡¨
            
        Returns:
            list: å¤„ç†ç»“æœåˆ—è¡¨
        """
        processor = self.get_processor()
        return await processor.batch_process(training_samples)


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    async def test_actor_processor():
        processor = ActorModelProcessor()
        
        # æ¨¡æ‹Ÿæ ·æœ¬æ•°æ®
        sample_data = {
            "prompt_id": "test_001",
            "prompt": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•prompt",
            "parsed_data": {
                "history_chat": "[é”€å”®][2024-12-09 16:01:58]:å“ˆå–½ï¼Œä½ å¥½ï¼",
                "query": "[å®¢æˆ·][2024-12-09 16:39:41]:å›½è€ƒå’Œçœè€ƒæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
            }
        }
        
        result = await processor.process_sample(sample_data["prompt"], sample_data)
        print("å¤„ç†ç»“æœ:")
        print(json.dumps(result, ensure_ascii=False, indent=2))
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_actor_processor())