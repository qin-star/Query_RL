"""
å‚è€ƒæ¨¡å‹ï¼ˆQwen-32Bï¼‰å¤„ç†å™¨ v2.0
ç”¨äºå¤„ç†SalesRAG Queryæ”¹å†™çš„GRPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒ
"""

import asyncio
import json
import logging
from typing import Dict, Any, Optional, Tuple
import time
from dataclasses import dataclass

from src.core.rag_chater import RagChater
from src.utils.log import logger

logger = logging.getLogger(__name__)


@dataclass
class ReferenceModelOutput:
    """å‚è€ƒæ¨¡å‹è¾“å‡ºæ•°æ®ç»“æ„"""
    user_profile: str = ""
    rewritten_query: str = ""
    history_summary: str = ""
    rag_recall: list = None
    rag_status: str = ""
    processing_time: float = 0.0
    success: bool = True
    error_message: str = ""
    query_analysis: dict = None
    intent_recognition: dict = None


class ReferenceModelProcessor:
    """å‚è€ƒæ¨¡å‹ï¼ˆQwen-32Bï¼‰å¤„ç†å™¨"""
    
    def __init__(self, model_name: str = "Qwen3-32B-Instruct"):
        """
        åˆå§‹åŒ–å‚è€ƒæ¨¡å‹å¤„ç†å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.rag_client = None
        self._init_rag_client()
        
    def _init_rag_client(self):
        """åˆå§‹åŒ–RAGå®¢æˆ·ç«¯"""
        try:
            self.rag_client = RagChater(
                tenant_id="chengla",
                contact_id="chengla_query_rl_contact",
                account_id="chengla_query_rl_account",
                message_id="chengla_query_rl_message_id"
            )
            logger.info("å‚è€ƒæ¨¡å‹RAGå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"å‚è€ƒæ¨¡å‹RAGå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def process_sample(self, prompt: str, sample_data: dict) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ ·æœ¬
        
        Args:
            prompt: å®Œæ•´çš„prompt
            sample_data: æ ·æœ¬æ•°æ®å­—å…¸
            
        Returns:
            dict: å¤„ç†ç»“æœ
        """
        try:
            start_time = time.time()
            
            # ğŸ”¥ ç›´æ¥è°ƒç”¨RAG /chatæ¥å£
            logger.debug(f"å¼€å§‹å¤„ç†å‚è€ƒæ¨¡å‹æ ·æœ¬: {sample_data.get('prompt_id', 'unknown')}")
            rag_result = await self._call_rag_chat(prompt)
            
            # è§£æRAGè¾“å‡º
            parsed_rag_output = self._parse_rag_output(rag_result)
            
            # ç»„åˆå®Œæ•´è¾“å‡ºæ ¼å¼
            complete_output = self._combine_complete_output(
                parsed_rag_output, rag_result, sample_data, start_time
            )
            
            logger.info(f"å‚è€ƒæ¨¡å‹æ ·æœ¬å¤„ç†æˆåŠŸ: {sample_data.get('prompt_id', 'unknown')}")
            return complete_output
            
        except Exception as e:
            logger.error(f"å‚è€ƒæ¨¡å‹å¤„ç†å¤±è´¥: {e}")
            return self._get_error_output(sample_data, str(e))
    
    async def _call_rag_chat(self, prompt: str) -> Dict[str, Any]:
        """
        è°ƒç”¨RAG /chatæ¥å£
        
        Args:
            prompt: å®Œæ•´çš„promptä½œä¸ºcontext
            
        Returns:
            dict: RAGè°ƒç”¨ç»“æœ
        """
        try:
            logger.debug("è°ƒç”¨RAG /chatæ¥å£")
            
            # ä½¿ç”¨rag_chater.pyä¸­çš„chatæ–¹æ³•
            rag_result = await self.rag_client.chat(
                context=prompt,  # ğŸ”¥ ä¼ å…¥å®Œæ•´promptä½œä¸ºcontext
                score_threshold=0.95
            )
            
            response_data, status, request_body, cost_time = rag_result
            
            result = {
                "response_data": response_data,
                "status": status,
                "request_body": request_body,
                "cost_time": cost_time,
                "success": True
            }
            
            logger.debug(f"RAG /chatè°ƒç”¨æˆåŠŸï¼Œè€—æ—¶: {cost_time}s")
            return result
            
        except Exception as e:
            logger.error(f"RAG /chatè°ƒç”¨å¤±è´¥: {e}")
            return self._get_error_rag_result(str(e))
    
    def _parse_rag_output(self, rag_result: dict) -> ReferenceModelOutput:
        """
        è§£æRAGè¾“å‡º
        
        Args:
            rag_result: RAGè°ƒç”¨ç»“æœ
            
        Returns:
            ReferenceModelOutput: è§£æåçš„è¾“å‡º
        """
        try:
            response_data = rag_result["response_data"]
            status = rag_result["status"]
            
            # /chat è¿”å›å®Œæ•´çš„æ•°æ®ç»“æ„
            if response_data and len(response_data) > 0:
                model_data = response_data[0].get("data", {})
                
                return ReferenceModelOutput(
                    user_profile=model_data.get("user_profile", ""),
                    rewritten_query=model_data.get("rewritten_query", ""),
                    history_summary=model_data.get("history_summary", ""),
                    rag_recall=model_data.get("recall", []),
                    rag_status=status,
                    processing_time=rag_result.get("cost_time", 0.0),
                    success=True,
                    query_analysis=model_data.get("query_analysis", {}),
                    intent_recognition=model_data.get("intent_recognition", {})
                )
            else:
                return ReferenceModelOutput(
                    rag_status=status,
                    success=False,
                    error_message="RAGè¿”å›ç©ºæ•°æ®"
                )
                
        except Exception as e:
            logger.error(f"å‚è€ƒæ¨¡å‹RAGè¾“å‡ºè§£æå¤±è´¥: {e}")
            return ReferenceModelOutput(
                success=False,
                error_message=str(e)
            )
    
    def _combine_complete_output(
        self, 
        parsed_output: ReferenceModelOutput, 
        rag_result: Dict[str, Any], 
        sample_data: dict,
        start_time: float
    ) -> Dict[str, Any]:
        """
        ç»„åˆå®Œæ•´è¾“å‡ºæ ¼å¼
        
        Args:
            parsed_output: è§£æåçš„RAGè¾“å‡º
            rag_result: RAGè°ƒç”¨ç»“æœ
            sample_data: åŸå§‹æ ·æœ¬æ•°æ®
            start_time: å¼€å§‹æ—¶é—´
            
        Returns:
            dict: å®Œæ•´çš„è¾“å‡ºç»“æœ
        """
        try:
            processing_time = time.time() - start_time
            
            # æ„å»ºå®Œæ•´å“åº”
            complete_response = {
                "user_profile": parsed_output.user_profile,
                "rewritten_query": parsed_output.rewritten_query,
                "history_summary": parsed_output.history_summary,
                "rag_recall": parsed_output.rag_recall,
                "rag_status": parsed_output.rag_status,
                "additional_info": {
                    "query_analysis": parsed_output.query_analysis,
                    "intent_recognition": parsed_output.intent_recognition,
                    "internal_processing": True
                },
                "processing_metadata": {
                    "model_name": self.model_name,
                    "endpoint": "/chat",
                    "processing_time": processing_time,
                    "rag_cost_time": parsed_output.processing_time,
                    "success": parsed_output.success
                }
            }
            
            result = {
                "prompt_id": sample_data.get("prompt_id", "unknown"),
                "complete_response": complete_response,
                "original_data": sample_data.get("parsed_data", {}),
                "rag_result": rag_result,
                "processing_success": parsed_output.success,
                "total_processing_time": processing_time
            }
            
            logger.debug(f"å‚è€ƒæ¨¡å‹å®Œæ•´è¾“å‡ºç»„åˆæˆåŠŸï¼Œæ€»è€—æ—¶: {processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"å‚è€ƒæ¨¡å‹å®Œæ•´è¾“å‡ºç»„åˆå¤±è´¥: {e}")
            raise
    
    def _get_error_output(self, sample_data: dict, error_message: str) -> Dict[str, Any]:
        """è·å–é”™è¯¯è¾“å‡º"""
        return {
            "prompt_id": sample_data.get("prompt_id", "unknown"),
            "complete_response": {
                "user_profile": "",
                "rewritten_query": "",
                "history_summary": "",
                "rag_recall": [],
                "rag_status": "error",
                "additional_info": {
                    "query_analysis": {},
                    "intent_recognition": {},
                    "internal_processing": False
                },
                "processing_metadata": {
                    "model_name": self.model_name,
                    "endpoint": "/chat",
                    "processing_time": 0.0,
                    "rag_cost_time": 0.0,
                    "success": False,
                    "error_message": error_message
                }
            },
            "original_data": sample_data.get("parsed_data", {}),
            "rag_result": {"success": False, "error_message": error_message},
            "processing_success": False,
            "total_processing_time": 0.0,
            "error_message": error_message
        }
    
    def _get_error_rag_result(self, error_message: str) -> Dict[str, Any]:
        """è·å–RAGé”™è¯¯ç»“æœ"""
        return {
            "response_data": [],
            "status": "error",
            "request_body": {},
            "cost_time": 0.0,
            "success": False,
            "error_message": error_message
        }
    
    async def batch_process(self, samples: list, max_concurrency: int = 5) -> list:
        """
        æ‰¹é‡å¤„ç†æ ·æœ¬
        
        Args:
            samples: æ ·æœ¬åˆ—è¡¨
            max_concurrency: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            list: å¤„ç†ç»“æœåˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç†{len(samples)}ä¸ªå‚è€ƒæ¨¡å‹æ ·æœ¬ï¼Œæœ€å¤§å¹¶å‘æ•°: {max_concurrency}")
            
            # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
            semaphore = asyncio.Semaphore(max_concurrency)
            
            async def process_with_semaphore(sample):
                async with semaphore:
                    return await self.process_sample(sample["prompt"], sample)
            
            # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
            tasks = [process_with_semaphore(sample) for sample in samples]
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†å¼‚å¸¸ç»“æœ
            processed_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"å‚è€ƒæ¨¡å‹æ ·æœ¬{i}å¤„ç†å¼‚å¸¸: {result}")
                    processed_results.append(self._get_error_output(samples[i], str(result)))
                else:
                    processed_results.append(result)
            
            success_count = sum(1 for r in processed_results if r["processing_success"])
            logger.info(f"å‚è€ƒæ¨¡å‹æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸ: {success_count}/{len(processed_results)}")
            
            return processed_results
            
        except Exception as e:
            logger.error(f"å‚è€ƒæ¨¡å‹æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
            return []


class ReferenceModelManager:
    """å‚è€ƒæ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self, model_name: str = "Qwen3-32B-Instruct"):
        """
        åˆå§‹åŒ–å‚è€ƒæ¨¡å‹ç®¡ç†å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.processor = None
        
    def get_processor(self) -> ReferenceModelProcessor:
        """è·å–å‚è€ƒæ¨¡å‹å¤„ç†å™¨å®ä¾‹"""
        if self.processor is None:
            self.processor = ReferenceModelProcessor(self.model_name)
        return self.processor
    
    async def process_training_batch(self, training_samples: list) -> list:
        """
        å¤„ç†è®­ç»ƒæ‰¹æ¬¡
        
        Args:
            training_samples: è®­ç»ƒæ ·æœ¬åˆ—è¡¨
            
        Returns:
            list: å¤„ç†ç»“æœåˆ—è¡¨
        """
        processor = self.get_processor()
        return await processor.batch_process(training_samples)


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    async def test_reference_processor():
        processor = ReferenceModelProcessor()
        
        # æ¨¡æ‹Ÿæ ·æœ¬æ•°æ®
        sample_data = {
            "prompt_id": "test_001",
            "prompt": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•prompt",
            "parsed_data": {
                "history_chat": "[é”€å”®][2024-12-09 16:01:58]:å“ˆå–½ï¼Œä½ å¥½ï¼",
                "query": "[å®¢æˆ·][2024-12-09 16:39:41]:å›½è€ƒå’Œçœè€ƒæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
            }
        }
        
        result = await processor.process_sample(sample_data["prompt"], sample_data)
        print("å‚è€ƒæ¨¡å‹å¤„ç†ç»“æœ:")
        print(json.dumps(result, ensure_ascii=False, indent=2))
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_reference_processor())