# <think> æ ‡è®°ä½¿ç”¨è¯´æ˜

## ğŸ“‹ åŠŸèƒ½è¯´æ˜

å·²åœ¨SFTè®­ç»ƒæ•°æ®ä¸­æ·»åŠ `<think></think>`æ ‡è®°ï¼Œç”¨äºä¿æŒæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

### ä¸ºä»€ä¹ˆéœ€è¦<think>æ ‡è®°ï¼Ÿ

åœ¨å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­ï¼Œå¦‚æœç›´æ¥è®©æ¨¡å‹è¾“å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œå¯èƒ½ä¼šï¼š
- âŒ ä¸§å¤±æ¨ç†è¿‡ç¨‹
- âŒ é™ä½å¤æ‚é—®é¢˜çš„è§£å†³èƒ½åŠ›
- âŒ æ— æ³•å±•ç¤ºæ€ç»´é“¾ï¼ˆChain of Thoughtï¼‰

é€šè¿‡æ·»åŠ `<think></think>`æ ‡è®°ï¼š
- âœ… ä¸ºæ¨¡å‹é¢„ç•™"æ€è€ƒç©ºé—´"
- âœ… ä¿æŒæ¨ç†èƒ½åŠ›
- âœ… è®­ç»ƒåå¯ä»¥çœ‹åˆ°æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹

---

## ğŸ¯ ä¿®æ”¹å†…å®¹

### ä¿®æ”¹å‰çš„è¾“å‡ºæ ¼å¼

```json
{
  "user_profile": "ç”¨æˆ·ä¸ºå¤‡è€ƒå…¬åŠ¡å‘˜çš„åº”å±Šç”Ÿ",
  "history_summary": "ç”¨æˆ·æ­£åœ¨äº†è§£è¯¾ç¨‹ä¿¡æ¯",
  "rewritten_query": "å…¬åŠ¡å‘˜è€ƒè¯•åŸ¹è®­è¯¾ç¨‹çš„ä»·æ ¼æ˜¯å¤šå°‘ï¼Ÿ"
}
```

### ä¿®æ”¹åçš„è¾“å‡ºæ ¼å¼

```
<think>

</think>

{
  "user_profile": "ç”¨æˆ·ä¸ºå¤‡è€ƒå…¬åŠ¡å‘˜çš„åº”å±Šç”Ÿ",
  "history_summary": "ç”¨æˆ·æ­£åœ¨äº†è§£è¯¾ç¨‹ä¿¡æ¯",
  "rewritten_query": "å…¬åŠ¡å‘˜è€ƒè¯•åŸ¹è®­è¯¾ç¨‹çš„ä»·æ ¼æ˜¯å¤šå°‘ï¼Ÿ"
}
```

---

## ğŸ“ å®Œæ•´è®­ç»ƒæ ·æœ¬ç¤ºä¾‹

```json
{
  "messages": [
    {
      "role": "system",
      "content": "## èƒŒæ™¯\nä½ æ˜¯æ•™åŸ¹è¡Œä¸šçš„å¯¹è¯ç†è§£ä¸ç”¨æˆ·æ´å¯Ÿä¸“å®¶..."
    },
    {
      "role": "user",
      "content": "è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œä¾æ¬¡å®Œæˆä¸‰ä¸ªä»»åŠ¡ï¼š\n\n## è¾“å…¥ä¿¡æ¯\n\n- å†å²å¯¹è¯å†…å®¹ï¼š\n[é”€å”®][...]...\n\n- ç”¨æˆ·å½“å‰è¾“å…¥ï¼š\næˆ‘æƒ³äº†è§£è¯¾ç¨‹ä»·æ ¼\n\nè¯·æŒ‰ç…§è¦æ±‚è¾“å‡ºJSONæ ¼å¼çš„åˆ†æç»“æœã€‚"
    },
    {
      "role": "assistant",
      "content": "<think>\n\n</think>\n\n{\n  \"user_profile\": \"ç”¨æˆ·ä¸ºå¤‡è€ƒå…¬åŠ¡å‘˜çš„åº”å±Šç”Ÿ\",\n  \"history_summary\": \"ç”¨æˆ·æ­£åœ¨äº†è§£è¯¾ç¨‹ä¿¡æ¯\",\n  \"rewritten_query\": \"å…¬åŠ¡å‘˜è€ƒè¯•åŸ¹è®­è¯¾ç¨‹çš„ä»·æ ¼æ˜¯å¤šå°‘ï¼Ÿ\"\n}"
    }
  ]
}
```

---

## ğŸ”§ ä»£ç ä¿®æ”¹

### ä¿®æ”¹çš„å‡½æ•°ï¼š`convert_to_messages_format`

```python
# åŠ©æ‰‹è¾“å‡ºï¼šJSONæ ¼å¼
assistant_output = {
    "user_profile": user_profile.strip() if pd.notna(user_profile) else "",
    "history_summary": history_summary.strip() if pd.notna(history_summary) else "",
    "rewritten_query": rewritten_query.strip() if pd.notna(rewritten_query) else ""
}

# è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ï¼ˆæ ¼å¼åŒ–è¾“å‡ºï¼‰
json_output = json.dumps(assistant_output, ensure_ascii=False, indent=2)

# â­ åœ¨è¾“å‡ºå‰æ·»åŠ <think>æ ‡è®°ï¼Œä¿æŒæ¨¡å‹æ¨ç†èƒ½åŠ›
assistant_content = f"<think>\n\n</think>\n\n{json_output}"
```

### ä¿®æ”¹çš„å‡½æ•°ï¼š`generate_stats_report`

```python
# è§£æassistantçš„JSONè¾“å‡ºï¼ˆéœ€è¦å»æ‰<think>æ ‡è®°ï¼‰
assistant_content = sample['messages'][2]['content']

# â­ æå–JSONéƒ¨åˆ†ï¼ˆå»æ‰<think>\n\n</think>\n\nå‰ç¼€ï¼‰
if assistant_content.startswith('<think>\n\n</think>\n\n'):
    json_str = assistant_content.replace('<think>\n\n</think>\n\n', '', 1)
else:
    json_str = assistant_content

output_json = json.loads(json_str)
```

---

## ğŸš€ è®­ç»ƒæ•ˆæœé¢„æœŸ

### è®­ç»ƒå‰ï¼ˆæ— <think>æ ‡è®°ï¼‰

```
è¾“å…¥ï¼šè¯·åˆ†æå¯¹è¯...
è¾“å‡ºï¼š{"user_profile": "...", ...}
```
âŒ æ¨¡å‹ç›´æ¥è¾“å‡ºç­”æ¡ˆï¼Œæ— æ¨ç†è¿‡ç¨‹

### è®­ç»ƒåï¼ˆæœ‰<think>æ ‡è®°ï¼‰

```
è¾“å…¥ï¼šè¯·åˆ†æå¯¹è¯...
è¾“å‡ºï¼š<think>
ç”¨æˆ·æœ€åæé—®æ˜¯å…³äºè¯¾ç¨‹ä»·æ ¼ï¼Œä¹‹å‰è®¨è®ºäº†æŠ¥åäº‹å®œ...
</think>

{"user_profile": "...", ...}
```
âœ… æ¨¡å‹å…ˆæ€è€ƒå†è¾“å‡ºï¼Œä¿æŒæ¨ç†èƒ½åŠ›

---

## ğŸ“ æ¨ç†æ—¶çš„ä½¿ç”¨

### æ ‡å‡†æ¨ç†æ¨¡å¼

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import json

model = AutoModelForCausalLM.from_pretrained("output/chengla_v2/final")
tokenizer = AutoTokenizer.from_pretrained("output/chengla_v2/final")

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_input}
]

inputs = tokenizer.apply_chat_template(messages, return_tensors="pt")
outputs = model.generate(inputs, max_new_tokens=512)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

# è§£æå“åº”
if '<think>' in response:
    # æå–æ€è€ƒè¿‡ç¨‹
    think_start = response.find('<think>')
    think_end = response.find('</think>')
    thinking_process = response[think_start+7:think_end].strip()
    
    # æå–JSONç»“æœ
    json_start = think_end + 9  # '</think>\n\n' çš„é•¿åº¦
    json_str = response[json_start:].strip()
    
    result = json.loads(json_str)
    
    print("ğŸ§  æ€è€ƒè¿‡ç¨‹ï¼š")
    print(thinking_process)
    print("\nğŸ“Š åˆ†æç»“æœï¼š")
    print(json.dumps(result, ensure_ascii=False, indent=2))
else:
    # å¦‚æœæ²¡æœ‰<think>æ ‡è®°ï¼ˆfallbackï¼‰
    result = json.loads(response)
```

### å¿«é€Ÿæ¨¡å¼ï¼ˆå¿½ç•¥æ€è€ƒè¿‡ç¨‹ï¼‰

```python
# å¦‚æœåªéœ€è¦æœ€ç»ˆç»“æœ
response = model.generate(...)
response_text = tokenizer.decode(...)

# æå–JSONéƒ¨åˆ†
if response_text.startswith('<think>\n\n</think>\n\n'):
    json_str = response_text.replace('<think>\n\n</think>\n\n', '', 1)
else:
    json_str = response_text

result = json.loads(json_str)
```

---

## âš™ï¸ è®­ç»ƒé…ç½®å»ºè®®

### ms-swiftè®­ç»ƒå‘½ä»¤ï¼ˆæ›´æ–°ï¼‰

```bash
CUDA_VISIBLE_DEVICES=0 \
swift sft \
    --model Qwen/Qwen3-8B-Instruct \
    --train_type lora \
    --dataset data/sft/chengla_v2/train_latest.jsonl \
    --val_dataset data/sft/chengla_v2/val_latest.jsonl \
    --torch_dtype bfloat16 \
    --num_train_epochs 3 \
    --per_device_train_batch_size 2 \
    --learning_rate 1e-4 \
    --lora_rank 16 \
    --lora_alpha 32 \
    --target_modules all-linear \
    --gradient_accumulation_steps 8 \
    --eval_steps 100 \
    --save_steps 100 \
    --max_length 2048 \
    --output_dir output/chengla_v2 \
    --warmup_ratio 0.1
```

**å…³é”®å‚æ•°**ï¼š
- `max_length=2048`ï¼šéœ€è¦è¶³å¤Ÿé•¿ä»¥å®¹çº³`<think>`æ ‡è®°çš„å†…å®¹
- å»ºè®®åœ¨éªŒè¯é›†ä¸Šè§‚å¯Ÿæ¨¡å‹æ˜¯å¦å­¦ä¼šä½¿ç”¨`<think>`æ ‡è®°

---

## ğŸ” éªŒè¯æµ‹è¯•

### æµ‹è¯•è„šæœ¬å·²åˆ›å»º

```bash
python test_think_tag.py
```

**æµ‹è¯•å†…å®¹**ï¼š
1. âœ… éªŒè¯`<think>`æ ‡è®°æ˜¯å¦æ­£ç¡®æ·»åŠ 
2. âœ… éªŒè¯JSONæ ¼å¼æ˜¯å¦æ­£ç¡®
3. âœ… éªŒè¯è§£æé€»è¾‘æ˜¯å¦æ­£å¸¸
4. âœ… å±•ç¤ºå®Œæ•´çš„messagesç»“æ„

---

## ğŸ“Š å¯¹æ¯”ï¼šæœ‰æ— <think>æ ‡è®°çš„åŒºåˆ«

| ç‰¹æ€§ | æ— <think>æ ‡è®° | æœ‰<think>æ ‡è®° |
|------|--------------|--------------|
| **è¾“å‡ºé•¿åº¦** | è¾ƒçŸ­ | ç¨é•¿ |
| **æ¨ç†èƒ½åŠ›** | å¯èƒ½é€€åŒ– | ä¿æŒè‰¯å¥½ |
| **å¤æ‚é—®é¢˜** | è¡¨ç°ä¸€èˆ¬ | è¡¨ç°æ›´å¥½ |
| **å¯è§£é‡Šæ€§** | é»‘ç›’ | å¯è§æ€è€ƒè¿‡ç¨‹ |
| **è®­ç»ƒæˆæœ¬** | ç•¥ä½ | ç•¥é«˜ï¼ˆtokenå¢åŠ ï¼‰ |
| **æ¨ç†æˆæœ¬** | ç•¥ä½ | ç•¥é«˜ï¼ˆç”Ÿæˆæ›´é•¿ï¼‰ |

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. Tokenæ¶ˆè€—

æ·»åŠ `<think>`æ ‡è®°åï¼Œæ¯ä¸ªæ ·æœ¬ä¼šå¢åŠ çº¦20ä¸ªtokenï¼š
- è®­ç»ƒæ•°æ®ï¼š474æ¡ Ã— 20 tokens = ~10,000 extra tokens
- å¯¹æ€»ä½“è®­ç»ƒæˆæœ¬å½±å“è¾ƒå°

### 2. æ¨ç†æ—¶çš„å¤„ç†

æ¨ç†æ—¶éœ€è¦æ­£ç¡®è§£æå¸¦`<think>`æ ‡è®°çš„è¾“å‡ºï¼š
```python
# é”™è¯¯ç¤ºä¾‹
result = json.loads(response)  # âŒ ä¼šæŠ¥é”™ï¼Œå› ä¸ºåŒ…å«<think>

# æ­£ç¡®ç¤ºä¾‹
json_str = response.replace('<think>\n\n</think>\n\n', '', 1)
result = json.loads(json_str)  # âœ… æ­£ç¡®
```

### 3. ç©ºçš„<think>æ ‡è®°

å½“å‰å®ç°ä¸­ï¼Œ`<think></think>`ä¹‹é—´æ˜¯ç©ºçš„ï¼Œè¿™æ˜¯æ­£å¸¸çš„ï¼š
- è®­ç»ƒæ—¶ï¼šæ¨¡å‹å­¦ä¹ åœ¨è¿™é‡Œ"æ€è€ƒ"
- æ¨ç†æ—¶ï¼šæ¨¡å‹ä¼šåœ¨è¿™é‡Œç”Ÿæˆæ€è€ƒå†…å®¹

å¦‚æœå¸Œæœ›åœ¨è®­ç»ƒæ•°æ®ä¸­ä¹ŸåŒ…å«æ€è€ƒå†…å®¹ï¼Œéœ€è¦ï¼š
1. äººå·¥æ ‡æ³¨æˆ–ä½¿ç”¨GPTç”Ÿæˆæ€è€ƒè¿‡ç¨‹
2. ä¿®æ”¹æ•°æ®ç”Ÿæˆè„šæœ¬æ·»åŠ æ€è€ƒå†…å®¹

---

## ğŸ¯ æœ€ä½³å®è·µå»ºè®®

### è®­ç»ƒé˜¶æ®µ

1. **ä»ç©º<think>å¼€å§‹**ï¼šå…ˆç”¨ç©ºçš„`<think></think>`è®­ç»ƒ
2. **è§‚å¯Ÿæ•ˆæœ**ï¼šéªŒè¯æ¨¡å‹æ˜¯å¦å­¦ä¼šåœ¨è¿™é‡Œç”Ÿæˆå†…å®¹
3. **è¿­ä»£ä¼˜åŒ–**ï¼šå¦‚æœæ•ˆæœä¸ä½³ï¼Œè€ƒè™‘æ·»åŠ ç¤ºä¾‹æ€è€ƒå†…å®¹

### æ¨ç†é˜¶æ®µ

1. **ä¿ç•™æ€è€ƒè¿‡ç¨‹**ï¼šä¾¿äºè°ƒè¯•å’Œç†è§£
2. **å¯é€‰ä¼˜åŒ–**ï¼šç”Ÿäº§ç¯å¢ƒå¯ä»¥è®¾ç½®`stop_tokens=['</think>']`æå‰åœæ­¢ï¼ŒèŠ‚çœæˆæœ¬
3. **ç›‘æ§è´¨é‡**ï¼šå®šæœŸæ£€æŸ¥æ€è€ƒè¿‡ç¨‹æ˜¯å¦æœ‰æ„ä¹‰

---

## âœ… ä¿®æ”¹å®Œæˆæ¸…å•

- âœ… ä¿®æ”¹`convert_to_messages_format`å‡½æ•°ï¼Œæ·»åŠ `<think>`æ ‡è®°
- âœ… ä¿®æ”¹`generate_stats_report`å‡½æ•°ï¼Œæ­£ç¡®è§£æå¸¦æ ‡è®°çš„è¾“å‡º
- âœ… åˆ›å»ºæµ‹è¯•è„šæœ¬`test_think_tag.py`
- âœ… åˆ›å»ºè¯´æ˜æ–‡æ¡£ï¼ˆæœ¬æ–‡æ¡£ï¼‰

---

## ğŸš€ ä¸‹ä¸€æ­¥

ç°åœ¨å¯ä»¥è¿è¡Œè½¬æ¢è„šæœ¬ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼š

```bash
python convert_to_sft_format_v2.py
```

ç”Ÿæˆçš„è®­ç»ƒæ•°æ®å°†åŒ…å«`<think></think>`æ ‡è®°ï¼Œå¯ä»¥ç›´æ¥ç”¨äºæ¨¡å‹è®­ç»ƒï¼

