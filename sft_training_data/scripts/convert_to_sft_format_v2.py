"""
å°†æ©™å•¦è®­ç»ƒæ•°æ®é›†è½¬æ¢ä¸ºSFTè®­ç»ƒæ ¼å¼ï¼ˆå¤šä»»åŠ¡è¾“å‡ºç‰ˆæœ¬ï¼‰

ä»»åŠ¡ï¼šä»å¯¹è¯å†å²ä¸­åŒæ—¶ç”Ÿæˆ user_profile + history_summary + rewritten_query

è¾“å…¥: æ©™å•¦-query_RL_è®­ç»ƒé›†.xlsx
è¾“å‡º: JSONæ ¼å¼çš„è®­ç»ƒæ•°æ®ï¼ˆå•ä¸€æ•°æ®é›†ï¼Œä¸åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ï¼‰
"""

import pandas as pd
import json
import random
from pathlib import Path
from typing import List, Dict

class SFTDataConverterV2:
    """SFTè®­ç»ƒæ•°æ®è½¬æ¢å™¨ - å¤šä»»åŠ¡è¾“å‡ºç‰ˆæœ¬"""
    
    def __init__(self, tenant_id: str = "chengla"):
        self.tenant_id = tenant_id
        
        # ç³»ç»Ÿprompt - åªåŒ…å«è§’è‰²å®šä½å’Œé€šç”¨æŒ‡ä»¤ï¼Œä¸åŒ…å«æ•°æ®å ä½ç¬¦
        self.system_prompt = """ä½ æ˜¯æ•™åŸ¹è¡Œä¸šçš„å¯¹è¯ç†è§£ä¸ç”¨æˆ·æ´å¯Ÿä¸“å®¶ï¼Œå…·å¤‡å¤šå¹´å…¬è€ƒæ•™åŸ¹é”€å”®ç»éªŒï¼Œæ“…é•¿ç”¨æˆ·ç”»åƒæå–ã€ä¸Šä¸‹æ–‡ç†è§£ä¸é—®é¢˜æ”¹å†™ã€‚
ä½ èƒ½å¤Ÿå‡†ç¡®è¯†åˆ«ç”¨æˆ·å¯¹è¯ä¸­çš„çœŸå®æ„å›¾ï¼Œå°¤å…¶æ“…é•¿å¤„ç†æ¨¡ç³Šè¡¨è¾¾ã€åé—®å¥ã€é”™åˆ«å­—ã€æƒ…ç»ªæ€§è¡¨è¾¾ç­‰ï¼Œ
å¹¶å°†å…¶è½¬åŒ–ä¸ºè¯­ä¹‰å®Œæ•´ã€æ£€ç´¢ç›®æ ‡æ˜ç¡®ã€çŸ¥è¯†åº“èƒ½å‘½ä¸­çš„æ¸…æ™°é—®é¢˜è¡¨è¾¾ã€‚

ä½ çš„ä»»åŠ¡æ˜¯åŸºäºç”¨æˆ·æä¾›çš„å¯¹è¯å†å²ï¼Œä¾æ¬¡å®Œæˆä¸‰ä¸ªä»»åŠ¡ï¼š
1. æå–ç”¨æˆ·ç”»åƒï¼ˆuser_profileï¼‰
2. æç‚¼å†å²ä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆhistory_summaryï¼‰
3. å¯¹å½“å‰ç”¨æˆ·è¾“å…¥è¿›è¡Œ query æ”¹å†™ï¼ˆrewritten_queryï¼‰

è¯·ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·æŒ‡ä»¤ä¸­çš„è¦æ±‚å’Œè§„åˆ™è¿›è¡Œåˆ†æå’Œè¾“å‡ºã€‚"""
        
        # ç”¨æˆ·æŒ‡ä»¤æ¨¡æ¿ - åŒ…å«å®Œæ•´çš„ä»»åŠ¡è¯´æ˜ã€è§„åˆ™å’Œè¾“å‡ºæ ¼å¼
        self.user_instruction_template = """è¯·åŸºäºä»¥ä¸‹å¯¹è¯ä¿¡æ¯ï¼Œä¾æ¬¡å®Œæˆä¸‰ä¸ªä»»åŠ¡ï¼š

## è¾“å…¥ä¿¡æ¯

### å†å²å¯¹è¯å†…å®¹ï¼š
{history_chat}

### ç”¨æˆ·å½“å‰è¾“å…¥ï¼š
{current_query}

## ä»»åŠ¡è¦æ±‚

### ä»»åŠ¡1ï¼šæå–ç”¨æˆ·ç”»åƒï¼ˆuser_profileï¼‰
æ€»ç»“ç”¨æˆ·èº«ä»½èƒŒæ™¯ã€è€ƒè¯•ç›®æ ‡ã€å½“å‰å¤‡è€ƒé˜¶æ®µä¸å…³æ³¨é‡ç‚¹ï¼Œå¯å‚è€ƒä»¥ä¸‹ç»´åº¦ï¼š
- å¹´é¾„æ®µæˆ–èº«ä»½ï¼ˆå¦‚åº”å±Šç”Ÿã€åœ¨èŒç­‰ï¼‰
- ç›®æ ‡è€ƒè¯•ç±»å‹ï¼ˆå¦‚å…¬åŠ¡å‘˜ã€äº‹ä¸šç¼–ç­‰ï¼‰
- å½“å‰å¤‡è€ƒé˜¶æ®µ
- æ˜¯å¦æœ‰åŸ¹è®­æˆ–å¤ä¹ ç»éªŒ
- å½“å‰å…³æ³¨é‡ç‚¹ï¼ˆå¦‚æŠ¥åæ—¶é—´ã€è¯¾ç¨‹å†…å®¹ã€é¢è¯•å‡†å¤‡ç­‰ï¼‰

> å¦‚ä¿¡æ¯ä¸è¶³æ—¶ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡åˆç†æ¨ç†ï¼›å¦‚ä»æ— æ³•åˆ¤æ–­ï¼Œå¯ç•¥å†™æˆ–ç•™ç©ºã€‚

### ä»»åŠ¡2ï¼šæç‚¼å†å²ä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆhistory_summaryï¼‰
è¯·æ ¹æ®å†å²å¯¹è¯ï¼Œæå–å‡ºå¯¹å½“å‰è½®å¯¹è¯æœ€æœ‰å¸®åŠ©çš„æ ¸å¿ƒä¿¡æ¯ï¼Œå†…å®¹åŒ…æ‹¬ä½†ä¸é™äºï¼š
- ç›®æ ‡è€ƒè¯•ç±»å‹ï¼ˆå¦‚äº‹ä¸šç¼–ã€å…¬åŠ¡å‘˜ç­‰ï¼‰
- å½“å‰å¤‡è€ƒçŠ¶æ€æˆ–ç”¨æˆ·ç–‘é—®
- ç”¨æˆ·å…´è¶£æ–¹å‘ï¼ˆå¦‚è¯¾ç¨‹ã€é¢è¯•æŠ€å·§ç­‰ï¼‰
- é”€å”®è€å¸ˆå¼•å¯¼ç‚¹æˆ–è¯¾ç¨‹æ¨èè®°å½•
- ç”¨æˆ·å…³æ³¨çš„é—®é¢˜è¶‹åŠ¿æˆ–åå¤æåŠå†…å®¹

> å¦‚ä¿¡æ¯ä¸è¶³ï¼Œå¯ä¸å¼ºè¡Œæ€»ç»“ï¼Œä½†è¯·å°½å¯èƒ½å‹ç¼©å†å²å¯¹è¯ä¸ºæœ‰æ•ˆæ‘˜è¦ã€‚

### ä»»åŠ¡3ï¼šQueryæ”¹å†™ï¼ˆrewritten_queryï¼‰
è¯·åŸºäºç”¨æˆ·å½“å‰è¾“å…¥ï¼Œç»“åˆä¸Šè¿°ç”¨æˆ·ç”»åƒå’Œå†å²æ‘˜è¦è¿›è¡Œæ”¹å†™ã€‚

#### æ”¹å†™è§„åˆ™

**è¡¨è¾¾è¡¥å…¨ä¸ä¿®æ”¹ï¼š**
1. æ˜ç¡®ç”¨æˆ·æé—®ä¸­æœªæŒ‡æ˜çš„èƒŒæ™¯ä¿¡æ¯ï¼ˆå¦‚è€ƒè¯•ç±»å‹ã€å…¬åŠ¡å‘˜ç¬”è¯•æˆ–é¢è¯•ã€æŠ¥åæµç¨‹ç­‰ï¼‰ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ä¸»åŠ¨è¡¥å…¨
2. è‹¥ç”¨æˆ·å½“å‰è½®å¯¹è¯å†…å®¹å­˜åœ¨è¯­ç—…ã€é”™åˆ«å­—æˆ–è¡¨è¾¾ä¸æ¸…ï¼Œå¯åŸºäºä¸Šä¸‹æ–‡åˆç†ä¿®æ”¹
3. å»æ‰è¯­æ°”è¯ï¼Œä¾‹å¦‚"å“¦å“¦"ã€"å—¯å—¯"ç­‰ï¼Œä½†ä¸å¾—æ”¹å˜åŸå§‹è¯­ä¹‰
4. ä¿ç•™ç”¨æˆ·åŸå§‹æ„å›¾ï¼Œä¸åšè¯­ä¹‰æ‰­æ›²æˆ–ä¸»è§‚åˆ¤æ–­
5. ä¸¥ç¦å‡­ç©ºæ·»åŠ å¹´ä»½ã€æœˆä»½æˆ–å…·ä½“æ—¶é—´ä¿¡æ¯ï¼Œé™¤éå†å²å¯¹è¯ä¸­å·²æ˜ç¡®æåˆ°å…·ä½“æ—¶é—´

**æ„å›¾è¿˜åŸä¸é‡æ„ï¼š**
6. å¯¹æ¨¡ç³Šè¡¨è¾¾ç»“åˆä¸Šä¸‹æ–‡è¡¥å…¨æˆæ¸…æ™°æ„å›¾
7. å¯¹äºè¯¾ç¨‹/è€ƒè¯•ç±»é—®é¢˜ï¼Œé‡æ„ä¸ºæ˜ç¡®çš„ç›®æ ‡æ€§é—®é¢˜ï¼Œå¦‚"è¯¥è¯¾ç¨‹æ˜¯å¦é€‚ç”¨äºè¯¥è€ƒè¯•"æˆ–"è¯¾ç¨‹å†…å®¹æ˜¯å¦è¦†ç›–è€ƒè¯•æ ¸å¿ƒçŸ¥è¯†ç‚¹"
8. æƒ…ç»ªæ€§æˆ–ç¢ç‰‡è¡¨è¾¾ï¼Œåº”è½¬åŒ–ä¸ºå…·æœ‰æ£€ç´¢ä»·å€¼çš„é—®é¢˜
9. è‹¥ç”¨æˆ·è¡¨éœ²å‡ºå¯¹æ¨¡å—æŒæ¡ã€åšé¢˜æ—¶é—´ã€è€ƒè¯•å‹åŠ›ç­‰å›°æ‰°ï¼Œè¯·é‡å†™ä¸ºç­–ç•¥æ€§å»ºè®®æˆ–æŠ€å·§æ€§é—®é¢˜ï¼Œå¦‚"å¦‚ä½•å®‰æ’ç§‘å­¦çš„ç­”é¢˜é¡ºåº""èµ„æ–™åˆ†ææ¨¡å—æœ‰å“ªäº›è§£é¢˜æŠ€å·§"
10. è‹¥åŒ…å«å¤šä¸ªé—®é¢˜ï¼Œæ‹†åˆ†ä¸ºä¸è¶…è¿‡ä¸‰æ¡ç‹¬ç«‹é—®å¥ï¼ŒæŒ‰é‡è¦æ€§æ’åº
11. è‹¥å†å²å¯¹è¯ä¿¡æ¯ä¸­æ²¡æœ‰å……è¶³ä¸Šä¸‹æ–‡ä¿¡æ¯æˆ–ä¸Šä¸‹æ–‡æ„å›¾ä¸æ˜æ˜¾ï¼Œåˆ™ä¸è¦è¿›è¡Œæ”¹å†™ï¼Œä¸å…è®¸è‡ªå·±å‘æŒ¥
12. è‹¥é”€å”®é—®å®¢æˆ·æ‰‹æœºå·å’Œè€ƒè¯•ç›®æ ‡ï¼Œè¯·ä¸è¦æ”¹å†™
13. è‹¥å®¢æˆ·å•çº¯å›å¤"å¥½çš„""å—¯å—¯""æ”¶åˆ°"ç­‰ï¼Œè¯·ä¸è¦æ”¹å†™

**è¡¨è¾¾é£æ ¼æ§åˆ¶ï¼š**
14. ä¿æŒè¯­è¨€è‡ªç„¶æµç•…ï¼Œè´´è¿‘æ•™åŸ¹è¡Œä¸šç”¨æˆ·è¡¨è¾¾ä¹ æƒ¯
15. æ”¹å†™å†…å®¹é¡»ä¸ä¸Šä¸‹æ–‡ä¿æŒé€»è¾‘è¿è´¯ï¼Œé¿å…ä¿¡æ¯è·³è·ƒ
16. ä¸å¾—åŠ å…¥ç”¨æˆ·æœªè¡¨è¾¾çš„å†…å®¹ã€ä¸å¾—åˆ¶é€ æ„å›¾æˆ–è™šæ„ä¿¡æ¯

## è¾“å‡ºæ ¼å¼

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼ˆä¸‰ä¸ªå­—æ®µç¼ºä¸€ä¸å¯ï¼‰ï¼š

```json
{{
  "user_profile": "ç”¨ä¸€å¥è¯æ¦‚æ‹¬ç”¨æˆ·çš„ç”»åƒä¿¡æ¯",
  "history_summary": "ç”¨ä¸€å¥è¯æ€»ç»“å†å²å¯¹è¯ä¸­å¯¹å½“å‰é—®é¢˜æœ€æœ‰å¸®åŠ©çš„ä¿¡æ¯",
  "rewritten_query": "ç”¨ä¸€å¥è¯è¡¨è¾¾ç”¨æˆ·å½“å‰è¾“å…¥çš„æ¸…æ™°æ£€ç´¢é—®é¢˜ï¼Œè¯­è¨€è‡ªç„¶ã€è¯­ä¹‰å®Œæ•´"
}}
```"""
    
    def convert_to_messages_format(
        self, 
        context: str, 
        user_profile: str,
        history_summary: str,
        rewritten_query: str
    ) -> Dict:
        """è½¬æ¢ä¸ºmessagesæ ¼å¼
        
        è¾“å…¥ï¼šå¯¹è¯å†å²
        è¾“å‡ºï¼šJSONæ ¼å¼çš„ä¸‰ä¸ªå­—æ®µ
        """
        
        # æ‹†åˆ†å¯¹è¯å†å²ï¼šæå–æœ€åä¸€æ¡å®¢æˆ·æ¶ˆæ¯ä½œä¸ºå½“å‰query
        lines = context.strip().split('\n')
        
        # ä»åå¾€å‰æ‰¾æœ€åä¸€æ¡[å®¢æˆ·]æ¶ˆæ¯
        current_query = ""
        history_chat = ""
        
        for i in range(len(lines) - 1, -1, -1):
            line = lines[i].strip()
            if line.startswith('[å®¢æˆ·]'):
                # æ‰¾åˆ°æœ€åä¸€æ¡å®¢æˆ·æ¶ˆæ¯
                if 'ï¼š' in line:
                    current_query = line.split('ï¼š', 1)[1].strip()
                else:
                    current_query = line
                
                # å…¶ä½™éƒ¨åˆ†ä½œä¸ºå†å²å¯¹è¯
                history_chat = '\n'.join(lines[:i]) if i > 0 else ""
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°[å®¢æˆ·]æ¶ˆæ¯ï¼Œå°†æœ€åä¸€è¡Œä½œä¸ºqueryï¼Œå‰é¢çš„ä½œä¸ºå†å²
        if not current_query and lines:
            current_query = lines[-1]
            history_chat = '\n'.join(lines[:-1]) if len(lines) > 1 else ""
        
        # ä½¿ç”¨æ¨¡æ¿å¡«å……å®é™…æ•°æ®ï¼Œæ„å»ºå®Œæ•´çš„ç”¨æˆ·æŒ‡ä»¤
        user_content = self.user_instruction_template.format(
            history_chat=history_chat if history_chat else "ï¼ˆæ— å†å²å¯¹è¯ï¼‰",
            current_query=current_query
        )
        
        # åŠ©æ‰‹è¾“å‡ºï¼šJSONæ ¼å¼
        assistant_output = {
            "user_profile": user_profile.strip() if pd.notna(user_profile) else "",
            "history_summary": history_summary.strip() if pd.notna(history_summary) else "",
            "rewritten_query": rewritten_query.strip() if pd.notna(rewritten_query) else ""
        }
        
        # è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ï¼ˆæ ¼å¼åŒ–è¾“å‡ºï¼‰
        json_output = json.dumps(assistant_output, ensure_ascii=False, indent=2)
        
        # åœ¨è¾“å‡ºå‰æ·»åŠ <think>æ ‡è®°ï¼Œä¿æŒæ¨¡å‹æ¨ç†èƒ½åŠ›
        assistant_content = f"<think>\n\n</think>\n\n{json_output}"
        
        # æ„å»ºmessages
        messages = [
            {
                "role": "system",
                "content": self.system_prompt
            },
            {
                "role": "user",
                "content": user_content
            },
            {
                "role": "assistant",
                "content": assistant_content
            }
        ]
        
        return messages
    
    def extract_last_customer_query(self, context: str) -> str:
        """ä»å¯¹è¯ä¸Šä¸‹æ–‡ä¸­æå–æœ€åä¸€æ¡å®¢æˆ·æ¶ˆæ¯ï¼ˆç”¨äºmetadataï¼‰"""
        lines = context.strip().split('\n')
        
        # ä»åå¾€å‰æ‰¾æœ€åä¸€æ¡[å®¢æˆ·]æ¶ˆæ¯
        for line in reversed(lines):
            line = line.strip()
            if line.startswith('[å®¢æˆ·]'):
                # æå–æ—¶é—´æˆ³åçš„å†…å®¹
                if 'ï¼š' in line:
                    query = line.split('ï¼š', 1)[1].strip()
                    return query
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›æœ€åä¸€è¡Œï¼ˆå»é™¤å‰ç¼€ï¼‰
        last_line = lines[-1].strip() if lines else ""
        if 'ï¼š' in last_line:
            return last_line.split('ï¼š', 1)[1].strip()
        return last_line
    
    def convert_excel_to_json(
        self,
        excel_path: str,
        output_dir: str = "data/sft/chengla_v2",
        quality_filter: bool = True
    ):
        """å°†Excelè½¬æ¢ä¸ºJSONè®­ç»ƒæ ¼å¼
        
        Args:
            excel_path: Excelæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            quality_filter: æ˜¯å¦è¿›è¡Œè´¨é‡è¿‡æ»¤
        """
        
        print("=" * 80)
        print("å¼€å§‹è½¬æ¢SFTè®­ç»ƒæ•°æ®ï¼ˆå¤šä»»åŠ¡è¾“å‡ºç‰ˆæœ¬ï¼‰")
        print("=" * 80)
        print("\nğŸ“‹ ä»»åŠ¡è¯´æ˜ï¼š")
        print("  è¾“å…¥ï¼šå¯¹è¯å†å²")
        print("  è¾“å‡ºï¼š{user_profile, history_summary, rewritten_query}")
        
        # è¯»å–Excel
        df = pd.read_excel(excel_path)
        print(f"\nğŸ“š è¯»å–æ•°æ®é›†: {len(df)} æ¡")
        
        # æ•°æ®æ¸…æ´—å’Œè´¨é‡è¿‡æ»¤
        original_count = len(df)
        
        # 1. ç§»é™¤ç©ºå€¼ - ä¸‰ä¸ªç›®æ ‡å­—æ®µéƒ½ä¸èƒ½ä¸ºç©º
        required_cols = ['rewritten_query', 'user_profile', 'history_summary']
        for col in required_cols:
            df = df.dropna(subset=[col])
            df = df[df[col].str.strip() != '']
        
        print(f"âœ… ç§»é™¤ç©ºå€¼æ ·æœ¬: {original_count - len(df)} æ¡ï¼Œå‰©ä½™ {len(df)} æ¡")
        
        if quality_filter:
            # 2. é•¿åº¦æ£€æŸ¥
            initial_len = len(df)
            
            # rewritten_query: 5-200å­—ç¬¦
            df = df[df['rewritten_query'].str.len() >= 5]
            df = df[df['rewritten_query'].str.len() <= 200]
            
            # user_profile: 10-300å­—ç¬¦
            df = df[df['user_profile'].str.len() >= 10]
            df = df[df['user_profile'].str.len() <= 300]
            
            # history_summary: 10-300å­—ç¬¦
            df = df[df['history_summary'].str.len() >= 10]
            df = df[df['history_summary'].str.len() <= 300]
            
            print(f"âœ… é•¿åº¦è¿‡æ»¤åå‰©ä½™: {len(df)} æ¡")
            
            # 3. ç¡®ä¿å¯¹è¯å†å²ä¸ä¸ºç©º
            df = df[df['æœ€ç»ˆä¼ å‚ä¸Šä¸‹æ–‡'].str.len() >= 20]
            print(f"âœ… å¯¹è¯å†å²è¿‡æ»¤åå‰©ä½™: {len(df)} æ¡")
        
        # è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
        training_samples = []
        
        for idx, row in df.iterrows():
            try:
                # æå–åŸå§‹queryï¼ˆç”¨äºmetadataï¼‰
                original_query = self.extract_last_customer_query(row['æœ€ç»ˆä¼ å‚ä¸Šä¸‹æ–‡'])
                
                # è½¬æ¢ä¸ºmessagesæ ¼å¼
                messages = self.convert_to_messages_format(
                    context=row['æœ€ç»ˆä¼ å‚ä¸Šä¸‹æ–‡'],
                    user_profile=row['user_profile'],
                    history_summary=row['history_summary'],
                    rewritten_query=row['rewritten_query']
                )
                
                sample = {
                    "messages": messages,
                    "metadata": {
                        "source": "chengla_rl_dataset",
                        "tenant_id": self.tenant_id,
                        "sample_id": f"chengla_v2_{idx}",
                        "original_query": original_query,
                        "task_type": "multi_output"  # æ ‡è®°ä¸ºå¤šä»»åŠ¡è¾“å‡º
                    }
                }
                
                training_samples.append(sample)
                
            except Exception as e:
                print(f"âš ï¸  å¤„ç†ç¬¬{idx}è¡Œæ—¶å‡ºé”™: {e}")
                continue
        
        print(f"\nâœ… æˆåŠŸè½¬æ¢: {len(training_samples)} æ¡æ ·æœ¬")
        
        # ä¿å­˜ä¸ºå•ä¸ªJSONæ–‡ä»¶
        self.save_as_json(training_samples, output_dir)
        
        # ä¿å­˜æ ·æœ¬ç¤ºä¾‹
        self.save_sample_examples(training_samples[:3], output_dir)
        
        return training_samples
    
    def save_as_json(
        self,
        samples: List[Dict],
        output_dir: str
    ):
        """ä¿å­˜ä¸ºå•ä¸ªJSONæ–‡ä»¶"""
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        print("\n" + "=" * 80)
        print("ä¿å­˜æ•°æ®é›†")
        print("=" * 80)
        print(f"æ€»æ ·æœ¬æ•°: {len(samples)} æ¡")
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼
        output_file = output_path / "dataset_latest.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(samples, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ å·²ä¿å­˜: {output_file}")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_stats_report({"dataset": samples}, output_path)
    
    def save_sample_examples(self, samples: List[Dict], output_path: Path):
        """ä¿å­˜æ ·æœ¬ç¤ºä¾‹ä¾›æŸ¥çœ‹"""
        
        example_file = Path(output_path) / "sample_examples.json"
        
        with open(example_file, 'w', encoding='utf-8') as f:
            json.dump(samples, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ æ ·æœ¬ç¤ºä¾‹å·²ä¿å­˜: {example_file}")
        
        # åœ¨æ§åˆ¶å°æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬
        if samples:
            print("\n" + "=" * 80)
            print("æ ·æœ¬ç¤ºä¾‹ï¼ˆç¬¬1æ¡ï¼‰")
            print("=" * 80)
            
            sample = samples[0]
            
            print("\nã€ç³»ç»ŸPromptã€‘")
            print("-" * 80)
            print(sample['messages'][0]['content'][:300] + "...")
            
            print("\nã€ç”¨æˆ·è¾“å…¥ã€‘")
            print("-" * 80)
            user_msg = sample['messages'][1]['content']
            # åªæ˜¾ç¤ºå‰500å­—ç¬¦
            if len(user_msg) > 500:
                print(user_msg[:500] + "...")
            else:
                print(user_msg)
            
            print("\nã€æ¨¡å‹è¾“å‡ºã€‘")
            print("-" * 80)
            print(sample['messages'][2]['content'])
    
    def generate_stats_report(self, splits: Dict, output_path: Path):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        
        report = {
            "tenant_id": self.tenant_id,
            "dataset_name": "chengla_query_rewrite_sft_v2",
            "task_type": "multi_output",
            "output_fields": ["user_profile", "history_summary", "rewritten_query"],
            "total_samples": sum(len(split) for split in splits.values()),
            "splits": {}
        }
        
        for split_name, split_data in splits.items():
            # è§£æJSONè¾“å‡ºç»Ÿè®¡é•¿åº¦
            user_profile_lengths = []
            history_summary_lengths = []
            rewritten_query_lengths = []
            
            for sample in split_data:
                try:
                    # è§£æassistantçš„JSONè¾“å‡ºï¼ˆéœ€è¦å»æ‰<think>æ ‡è®°ï¼‰
                    assistant_content = sample['messages'][2]['content']
                    
                    # æå–JSONéƒ¨åˆ†ï¼ˆå»æ‰<think>\n\n</think>\n\nå‰ç¼€ï¼‰
                    if assistant_content.startswith('<think>\n\n</think>\n\n'):
                        json_str = assistant_content.replace('<think>\n\n</think>\n\n', '', 1)
                    else:
                        json_str = assistant_content
                    
                    output_json = json.loads(json_str)
                    
                    user_profile_lengths.append(len(output_json.get('user_profile', '')))
                    history_summary_lengths.append(len(output_json.get('history_summary', '')))
                    rewritten_query_lengths.append(len(output_json.get('rewritten_query', '')))
                except:
                    continue
            
            report["splits"][split_name] = {
                "total_samples": len(split_data),
                "avg_user_profile_length": sum(user_profile_lengths) / len(user_profile_lengths) if user_profile_lengths else 0,
                "avg_history_summary_length": sum(history_summary_lengths) / len(history_summary_lengths) if history_summary_lengths else 0,
                "avg_rewritten_query_length": sum(rewritten_query_lengths) / len(rewritten_query_lengths) if rewritten_query_lengths else 0
            }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = output_path / "stats_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        print("\n" + "=" * 80)
        print("ç»Ÿè®¡æ‘˜è¦")
        print("=" * 80)
        print(json.dumps(report, ensure_ascii=False, indent=2))


def main():
    """ä¸»å‡½æ•°"""
    
    converter = SFTDataConverterV2(tenant_id="chengla")
    
    # è½¬æ¢æ•°æ®
    samples = converter.convert_excel_to_json(
        excel_path=r"/home/jovyan2/query_rl/sft_training_data/data/sft/chengla_v2/æ©™å•¦-query_RL_è®­ç»ƒé›†.xlsx",
        output_dir="/home/jovyan2/query_rl/sft_training_data/data/sft/chengla_v2",
        quality_filter=True
    )
    
    print("\n" + "=" * 80)
    print("âœ¨ SFTæ•°æ®å‡†å¤‡å®Œæˆï¼ï¼ˆå¤šä»»åŠ¡è¾“å‡ºç‰ˆæœ¬ï¼‰")
    print("=" * 80)
    print("\nğŸ“‚ è¾“å‡ºæ–‡ä»¶ï¼š")
    print("  - data/sft/chengla_v2/dataset_latest.json")
    print("  - data/sft/chengla_v2/stats_report.json")
    print("  - data/sft/chengla_v2/sample_examples.json")
    
    print("\nğŸ¯ è®­ç»ƒä»»åŠ¡ï¼š")
    print("   è¾“å…¥ï¼šå¯¹è¯å†å²")
    print("   è¾“å‡ºï¼šJSONæ ¼å¼ {user_profile, history_summary, rewritten_query}")
    
    print("\nğŸš€ ä¸‹ä¸€æ­¥ï¼šä½¿ç”¨è¿™äº›æ•°æ®å¼€å§‹SFTè®­ç»ƒ")
    print("   è®­ç»ƒæ—¶éœ€è¦æ³¨æ„JSONæ ¼å¼è¾“å‡ºçš„è§£æ")


if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­ä¿è¯å¯å¤ç°
    random.seed(42)
    
    main()


