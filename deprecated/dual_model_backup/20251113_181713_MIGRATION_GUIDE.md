# 双模型对比架构迁移指南

## 🚫 已废弃的架构
基于双模型对比的GRPO实现已被废弃，原因：
- ❌ 误解了GRPO核心原理（组内相对优化 vs 跨模型对比）
- ❌ 试图用绝对质量评估替代相对优势计算
- ❌ 破坏了GRPO的零均值特性

## ✅ 新的混合GRPO架构
已迁移到修正版混合GRPO架构：

### 核心改进
1. **保持GRPO核心**：仍然使用官方组内相对优化算法
2. **辅助信号增强**：GPT-5提供质量信号，但不破坏相对性
3. **零均值保持**：所有奖励都经过组内中心化
4. **官方兼容性**：完全兼容verl官方GRPO实现

### 关键文件变更
```
废弃文件（已备份到 /home/jovyan2/query_rl/deprecated/dual_model_backup）：
- dual_model_reward_calculator.py → 混合奖励计算器
- gpt5_dual_model_rater.py → 组内中心化评估

新增文件：
- hybrid_grpo_reward_calculator.py → 修正版混合奖励计算
- grpo_group_generator.py → GRPO组生成器
- actor_model_processor_v2.py → 组内多样本生成
```

### 配置变更
```yaml
# 旧配置（已废弃）
algorithm.hybrid_training:
  enable: true
  auxiliary_weight: 0.3
  # ...其他双模型对比参数

# 新配置（推荐使用）
algorithm.hybrid_grpo:
  enable: true
  grpo_weight: 0.7           # GRPO主权重
  auxiliary_weight: 0.3      # GPT-5辅助权重
  auxiliary_centralization: true  # 关键：组内中心化
  auxiliary_normalization: std    # 标准化方式
```

### 训练脚本变更
```bash
# 旧脚本（已废弃）
scripts/run_grpo_query_RL.sh

# 新脚本（推荐使用）
scripts/run_hybrid_grpo_official_format.sh
```

## 🔧 迁移步骤
1. 备份现有配置和代码
2. 使用新的混合GRPO训练脚本
3. 更新配置文件为algorithm.hybrid_grpo格式
4. 验证训练流程正确性

## 📞 技术支持
如遇到问题，请参考：
- 修正版架构文档：GRPO_RAG_Query_Rewrite_Architecture_v3.md
- 官方GRPO实现：verl_code/verl/trainer/ppo/core_algos.py
- 混合奖励计算器：hybrid_grpo_reward_calculator.py

备份时间: 20251113_181713
